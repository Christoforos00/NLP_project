{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christoforos00/NLP_project/blob/main/nlp_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD4Xay0Hm9F8"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,  Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRj3YEpGrVSo",
        "outputId": "e7283867-eb6e-4351-f673-c19d98601056"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hos9XfTpuP_9",
        "outputId": "2481d63b-8f06-472f-facf-9c98d20efa55"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-16 15:22:04--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  10.8MB/s    in 6m 35s  \n",
            "\n",
            "2021-08-16 15:28:41 (10.9 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGTlueDumK9z",
        "outputId": "e1739903-fec8-439c-93d9-8b5171532296"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gzip -d cc.en.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-16 18:56:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  18.4MB/s    in 66s     \n",
            "\n",
            "2021-08-16 18:58:05 (19.2 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFR2FkDgYcR0"
      },
      "source": [
        "from gensim.models.wrappers import FastText\n",
        "\n",
        "fasttext = FastText.load_fasttext_format('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "a-3sLBhejgfu",
        "outputId": "a8570b67-00d1-4bc3-9b11-feb386de1c7a"
      },
      "source": [
        "i=0\n",
        "vocabulary = {}\n",
        "\n",
        "with open(\"cc.en.300.vec\", 'r', encoding=\"utf-8\", newline='\\n', errors='ignore') as f:\n",
        "    for l in f:\n",
        "        line = l.rstrip().split(' ')\n",
        "        print(i)\n",
        "        if i==0:\n",
        "            vocabulary_size = int(line[0])+2\n",
        "            dim = int(line[1])\n",
        "            vecs = np.zeros(vocabulary_size*dim).reshape(vocabulary_size,dim)\n",
        "            vocabulary[\"__PADDING__\"] = 0\n",
        "            vocabulary[\"__UNK__\"] = 1\n",
        "            i = 2\n",
        "        else:\n",
        "            vocabulary[line[0]] = i\n",
        "            embedding = np.array(line[1:]).astype(np.float)\n",
        "            if (embedding.shape[0]==dim):\n",
        "                vecs[i,:] = embedding\n",
        "                i+=1\n",
        "            else:\n",
        "                print(\"Aaaaaaaaaaaaaaaaa\")\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8e07ae9a7261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cc.en.300.vec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cc.en.300.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He4itYTyW_m3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "960534d6-6718-4a69-c6fd-18c411860bcf"
      },
      "source": [
        "\n",
        "# Serialize vocab & embeddings\n",
        "pickle.dump(vocabulary, open(\"/content/drive/My Drive/fasttext_voc.pkl\" ,'wb'))\n",
        "np.save(\"/content/drive/My Drive/fasttext.npy\", vecs)\n",
        "\n",
        "# Free ram\n",
        "vecs = None\n",
        "vocabulary = None\n",
        "embedding = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d8dd8b18852d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Serialize vocab & embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/fasttext_voc.pkl\"\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/fasttext.npy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96CNevkqyjLm"
      },
      "source": [
        "df = pd.read_json('/content/drive/My Drive/News_Category_Dataset_v2.json', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "nYVaQIFSzCVr",
        "outputId": "5e9ff0eb-c45c-4681-ac15-9e76d9b9a706"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200848</th>\n",
              "      <td>TECH</td>\n",
              "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
              "      <td>Reuters, Reuters</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
              "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200849</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
              "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200850</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
              "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200851</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
              "      <td>CORRECTION: An earlier version of this story i...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200852</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
              "      <td>The five-time all-star center tore into his te...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200853 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             category  ...       date\n",
              "0               CRIME  ... 2018-05-26\n",
              "1       ENTERTAINMENT  ... 2018-05-26\n",
              "2       ENTERTAINMENT  ... 2018-05-26\n",
              "3       ENTERTAINMENT  ... 2018-05-26\n",
              "4       ENTERTAINMENT  ... 2018-05-26\n",
              "...               ...  ...        ...\n",
              "200848           TECH  ... 2012-01-28\n",
              "200849         SPORTS  ... 2012-01-28\n",
              "200850         SPORTS  ... 2012-01-28\n",
              "200851         SPORTS  ... 2012-01-28\n",
              "200852         SPORTS  ... 2012-01-28\n",
              "\n",
              "[200853 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heZEqGvB6_48"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df[:100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3i_QoR8zeNg"
      },
      "source": [
        "df['text'] = df['headline'] + \" \" + df['short_description']\n",
        "texts = df['text'].tolist() \n",
        "labels = df['category'].tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDvcLmdE5jib"
      },
      "source": [
        "del df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVcr82UrjpQn"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm',disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer')) \n",
        "\n",
        "def tokenize_samples(samples):\n",
        "  \n",
        "    tokenized_samples = []\n",
        "    for i in range(len(samples)):  # For each sample\n",
        "        doc = nlp(samples[i])  # Tokenize the sample into sentences\n",
        "        tokens = []\n",
        "        for sent in doc.sents:  # For each sentence\n",
        "            for tok in sent:  # Iterate through each token \n",
        "                # Preprocessing: Filter stopwords\n",
        "                if '\\n' in tok.text or \"\\t\" in tok.text or \"--\" in tok.text or \"*\" in tok.text or tok.text.lower() in STOP_WORDS:\n",
        "                    continue\n",
        "                if tok.text.strip():  \n",
        "                    tokens.append(tok.text.replace('\"',\"'\").strip())\n",
        "        tokenized_samples.append(tokens)\n",
        "\n",
        "    return tokenized_samples\n",
        "\n",
        "texts_tokenized = tokenize_samples(texts)\n",
        "text_edited = texts_tokenized\n",
        "texts_tokenized = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbnZEtQym9GC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_edited, labels, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzvdqYaLVqpv"
      },
      "source": [
        "# Load/deserialize\n",
        "fasttext_embed = np.load(\"/content/drive/My Drive/fasttext.npy\")\n",
        "fasttext_word_to_index = pickle.load(open(\"/content/drive/My Drive/fasttext_voc.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxW6PFUh3ZPL"
      },
      "source": [
        "del text_edited\n",
        "del labels\n",
        "\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av4SQAO9CAwu"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "y_train = lb.fit_transform(y_train)\n",
        "# y_dev = lb.transform(y_dev)\n",
        "y_test = lb.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfonF1FShBcb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 250 \n",
        "EMBEDDING_DIM = fasttext_embed.shape[1]\n",
        "\n",
        "# Init tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='__UNK__')\n",
        "# num_words: the maximum number of words to keep, based on word frequency.\n",
        "# oov_token: will be used to replace OOV WORDS\n",
        "\n",
        "# Fit tokenizer\n",
        "tokenizer.fit_on_texts([\" \".join(x) for x in X_train])\n",
        "\n",
        "# Converts text to sequences of IDs\n",
        "train_seqs = tokenizer.texts_to_sequences([\" \".join(x) for x in X_train])\n",
        "test_seqs = tokenizer.texts_to_sequences([\" \".join(x) for x in X_test])\n",
        "\n",
        "# Pads sequences to a fixed value\n",
        "X_train = pad_sequences(sequences=train_seqs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "X_test = pad_sequences(sequences=test_seqs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHfMy9UW3gsM"
      },
      "source": [
        "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=101)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us-xmi81l9Qu",
        "outputId": "9559e606-8982-47bb-ffeb-ca7366c2e376"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens.'.format(len(word_index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 59533 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oowuMDcOmBce"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQVm2YZomDte",
        "outputId": "3a02b4ac-baca-423b-8090-c57818783cec"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens.'.format(len(word_index)))\n",
        "\n",
        "embedding_matrix = np.zeros((MAX_WORDS+2, EMBEDDING_DIM))  # +2 (pad, unknown)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_WORDS:\n",
        "            continue\n",
        "    try:\n",
        "        embedding_vector = fasttext_embed[fasttext_word_to_index[word],:]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 59533 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG-cust7BPnQ"
      },
      "source": [
        "y_train_nonbinary = lb.inverse_transform(y_train)\n",
        "y_dev_nonbinary = lb.inverse_transform(y_dev)\n",
        "y_test_nonbinary = lb.inverse_transform(y_test )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLIcOri4-m51"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGp_EO7Am9GF",
        "outputId": "7f345234-fa08-44fa-a60a-ee654d81d5a6"
      },
      "source": [
        "print(\"Results of the majority classifier\")\n",
        "\n",
        "baseline = DummyClassifier(strategy='most_frequent')\n",
        "baseline.fit(X_train, y_train_nonbinary)\n",
        "\n",
        "print(\"Classification report on the training data:\")\n",
        "predictions_train = baseline.predict(X_train)\n",
        "print(classification_report(y_train_nonbinary, predictions_train))\n",
        "\n",
        "print(\"Classification report on the development data:\")\n",
        "predictions_dev = baseline.predict(X_dev)\n",
        "print(classification_report(y_dev_nonbinary, predictions_dev))\n",
        "\n",
        "print(\"Classification report on the test data:\")\n",
        "predictions_test = baseline.predict(X_test)\n",
        "print(classification_report(y_test_nonbinary, predictions_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of the majority classifier\n",
            "Classification report on the training data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.00      0.00      0.00       479\n",
            "ARTS & CULTURE       0.00      0.00      0.00       387\n",
            "  BLACK VOICES       0.00      0.00      0.00      1419\n",
            "      BUSINESS       0.00      0.00      0.00      1791\n",
            "       COLLEGE       0.00      0.00      0.00       344\n",
            "        COMEDY       0.00      0.00      0.00      1488\n",
            "         CRIME       0.00      0.00      0.00      1054\n",
            "CULTURE & ARTS       0.00      0.00      0.00       297\n",
            "       DIVORCE       0.00      0.00      0.00      1025\n",
            "     EDUCATION       0.00      0.00      0.00       285\n",
            " ENTERTAINMENT       0.00      0.00      0.00      4864\n",
            "   ENVIRONMENT       0.00      0.00      0.00       408\n",
            "         FIFTY       0.00      0.00      0.00       420\n",
            "  FOOD & DRINK       0.00      0.00      0.00      1827\n",
            "     GOOD NEWS       0.00      0.00      0.00       381\n",
            "         GREEN       0.00      0.00      0.00       767\n",
            "HEALTHY LIVING       0.00      0.00      0.00      1972\n",
            " HOME & LIVING       0.00      0.00      0.00      1253\n",
            "        IMPACT       0.00      0.00      0.00      1010\n",
            " LATINO VOICES       0.00      0.00      0.00       336\n",
            "         MEDIA       0.00      0.00      0.00       845\n",
            "         MONEY       0.00      0.00      0.00       492\n",
            "     PARENTING       0.00      0.00      0.00      2605\n",
            "       PARENTS       0.00      0.00      0.00      1186\n",
            "      POLITICS       0.16      1.00      0.28      9743\n",
            "  QUEER VOICES       0.00      0.00      0.00      1899\n",
            "      RELIGION       0.00      0.00      0.00       792\n",
            "       SCIENCE       0.00      0.00      0.00       649\n",
            "        SPORTS       0.00      0.00      0.00      1457\n",
            "         STYLE       0.00      0.00      0.00       680\n",
            "STYLE & BEAUTY       0.00      0.00      0.00      2887\n",
            "         TASTE       0.00      0.00      0.00       622\n",
            "          TECH       0.00      0.00      0.00       642\n",
            " THE WORLDPOST       0.00      0.00      0.00      1095\n",
            "        TRAVEL       0.00      0.00      0.00      2932\n",
            "      WEDDINGS       0.00      0.00      0.00      1070\n",
            "    WEIRD NEWS       0.00      0.00      0.00       830\n",
            "      WELLNESS       0.00      0.00      0.00      5342\n",
            "         WOMEN       0.00      0.00      0.00      1006\n",
            "    WORLD NEWS       0.00      0.00      0.00       661\n",
            "     WORLDPOST       0.00      0.00      0.00       758\n",
            "\n",
            "      accuracy                           0.16     60000\n",
            "     macro avg       0.00      0.02      0.01     60000\n",
            "  weighted avg       0.03      0.16      0.05     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.00      0.00      0.00       167\n",
            "ARTS & CULTURE       0.00      0.00      0.00       133\n",
            "  BLACK VOICES       0.00      0.00      0.00       445\n",
            "      BUSINESS       0.00      0.00      0.00       542\n",
            "       COLLEGE       0.00      0.00      0.00       102\n",
            "        COMEDY       0.00      0.00      0.00       565\n",
            "         CRIME       0.00      0.00      0.00       319\n",
            "CULTURE & ARTS       0.00      0.00      0.00       112\n",
            "       DIVORCE       0.00      0.00      0.00       318\n",
            "     EDUCATION       0.00      0.00      0.00       102\n",
            " ENTERTAINMENT       0.00      0.00      0.00      1606\n",
            "   ENVIRONMENT       0.00      0.00      0.00       107\n",
            "         FIFTY       0.00      0.00      0.00       119\n",
            "  FOOD & DRINK       0.00      0.00      0.00       641\n",
            "     GOOD NEWS       0.00      0.00      0.00       136\n",
            "         GREEN       0.00      0.00      0.00       235\n",
            "HEALTHY LIVING       0.00      0.00      0.00       691\n",
            " HOME & LIVING       0.00      0.00      0.00       441\n",
            "        IMPACT       0.00      0.00      0.00       337\n",
            " LATINO VOICES       0.00      0.00      0.00       105\n",
            "         MEDIA       0.00      0.00      0.00       304\n",
            "         MONEY       0.00      0.00      0.00       152\n",
            "     PARENTING       0.00      0.00      0.00       843\n",
            "       PARENTS       0.00      0.00      0.00       406\n",
            "      POLITICS       0.16      1.00      0.28      3288\n",
            "  QUEER VOICES       0.00      0.00      0.00       605\n",
            "      RELIGION       0.00      0.00      0.00       254\n",
            "       SCIENCE       0.00      0.00      0.00       229\n",
            "        SPORTS       0.00      0.00      0.00       503\n",
            "         STYLE       0.00      0.00      0.00       209\n",
            "STYLE & BEAUTY       0.00      0.00      0.00       933\n",
            "         TASTE       0.00      0.00      0.00       247\n",
            "          TECH       0.00      0.00      0.00       237\n",
            " THE WORLDPOST       0.00      0.00      0.00       394\n",
            "        TRAVEL       0.00      0.00      0.00       998\n",
            "      WEDDINGS       0.00      0.00      0.00       340\n",
            "    WEIRD NEWS       0.00      0.00      0.00       247\n",
            "      WELLNESS       0.00      0.00      0.00      1758\n",
            "         WOMEN       0.00      0.00      0.00       337\n",
            "    WORLD NEWS       0.00      0.00      0.00       216\n",
            "     WORLDPOST       0.00      0.00      0.00       277\n",
            "\n",
            "      accuracy                           0.16     20000\n",
            "     macro avg       0.00      0.02      0.01     20000\n",
            "  weighted avg       0.03      0.16      0.05     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.00      0.00      0.00       145\n",
            "ARTS & CULTURE       0.00      0.00      0.00       145\n",
            "  BLACK VOICES       0.00      0.00      0.00       448\n",
            "      BUSINESS       0.00      0.00      0.00       625\n",
            "       COLLEGE       0.00      0.00      0.00       112\n",
            "        COMEDY       0.00      0.00      0.00       508\n",
            "         CRIME       0.00      0.00      0.00       306\n",
            "CULTURE & ARTS       0.00      0.00      0.00        99\n",
            "       DIVORCE       0.00      0.00      0.00       347\n",
            "     EDUCATION       0.00      0.00      0.00       111\n",
            " ENTERTAINMENT       0.00      0.00      0.00      1647\n",
            "   ENVIRONMENT       0.00      0.00      0.00       139\n",
            "         FIFTY       0.00      0.00      0.00       155\n",
            "  FOOD & DRINK       0.00      0.00      0.00       592\n",
            "     GOOD NEWS       0.00      0.00      0.00       136\n",
            "         GREEN       0.00      0.00      0.00       266\n",
            "HEALTHY LIVING       0.00      0.00      0.00       665\n",
            " HOME & LIVING       0.00      0.00      0.00       411\n",
            "        IMPACT       0.00      0.00      0.00       369\n",
            " LATINO VOICES       0.00      0.00      0.00       105\n",
            "         MEDIA       0.00      0.00      0.00       254\n",
            "         MONEY       0.00      0.00      0.00       161\n",
            "     PARENTING       0.00      0.00      0.00       911\n",
            "       PARENTS       0.00      0.00      0.00       448\n",
            "      POLITICS       0.16      1.00      0.28      3258\n",
            "  QUEER VOICES       0.00      0.00      0.00       610\n",
            "      RELIGION       0.00      0.00      0.00       258\n",
            "       SCIENCE       0.00      0.00      0.00       210\n",
            "        SPORTS       0.00      0.00      0.00       482\n",
            "         STYLE       0.00      0.00      0.00       208\n",
            "STYLE & BEAUTY       0.00      0.00      0.00       914\n",
            "         TASTE       0.00      0.00      0.00       205\n",
            "          TECH       0.00      0.00      0.00       200\n",
            " THE WORLDPOST       0.00      0.00      0.00       338\n",
            "        TRAVEL       0.00      0.00      0.00      1004\n",
            "      WEDDINGS       0.00      0.00      0.00       369\n",
            "    WEIRD NEWS       0.00      0.00      0.00       242\n",
            "      WELLNESS       0.00      0.00      0.00      1741\n",
            "         WOMEN       0.00      0.00      0.00       393\n",
            "    WORLD NEWS       0.00      0.00      0.00       213\n",
            "     WORLDPOST       0.00      0.00      0.00       250\n",
            "\n",
            "      accuracy                           0.16     20000\n",
            "     macro avg       0.00      0.02      0.01     20000\n",
            "  weighted avg       0.03      0.16      0.05     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmkcV16zm9GF",
        "outputId": "5bae1415-54c7-456a-e77d-c2786815459c"
      },
      "source": [
        "print(\"Results of logistic regression\")\n",
        "\n",
        "logReg = LogisticRegression(solver=\"liblinear\", C = 10)\n",
        "logReg.fit(X_train, y_train_nonbinary)\n",
        "\n",
        "print(\"Classification report on the training data:\")\n",
        "predictions_train = logReg.predict(X_train)\n",
        "print(classification_report(y_train_nonbinary, predictions_train))\n",
        "\n",
        "print(\"Classification report on the development data:\")\n",
        "predictions_dev = logReg.predict(X_dev)\n",
        "print(classification_report(y_dev_nonbinary, predictions_dev))\n",
        "\n",
        "print(\"Classification report on the test data:\")\n",
        "predictions_test = logReg.predict(X_test)\n",
        "print(classification_report(y_test_nonbinary, predictions_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of logistic regression\n",
            "Classification report on the training data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.47      0.18      0.26       457\n",
            "ARTS & CULTURE       0.46      0.16      0.23       411\n",
            "  BLACK VOICES       0.49      0.27      0.35      1375\n",
            "      BUSINESS       0.47      0.42      0.45      1756\n",
            "       COLLEGE       0.49      0.33      0.39       368\n",
            "        COMEDY       0.53      0.30      0.38      1576\n",
            "         CRIME       0.56      0.55      0.55      1010\n",
            "CULTURE & ARTS       0.63      0.24      0.35       331\n",
            "       DIVORCE       0.78      0.64      0.70      1003\n",
            "     EDUCATION       0.53      0.33      0.41       302\n",
            " ENTERTAINMENT       0.46      0.70      0.55      4868\n",
            "   ENVIRONMENT       0.72      0.18      0.28       410\n",
            "         FIFTY       0.39      0.07      0.11       399\n",
            "  FOOD & DRINK       0.59      0.71      0.64      1916\n",
            "     GOOD NEWS       0.49      0.18      0.26       439\n",
            "         GREEN       0.45      0.31      0.37       796\n",
            "HEALTHY LIVING       0.36      0.14      0.20      1969\n",
            " HOME & LIVING       0.67      0.65      0.66      1277\n",
            "        IMPACT       0.41      0.20      0.27      1001\n",
            " LATINO VOICES       0.62      0.05      0.10       346\n",
            "         MEDIA       0.51      0.27      0.35       829\n",
            "         MONEY       0.54      0.29      0.38       502\n",
            "     PARENTING       0.50      0.62      0.55      2547\n",
            "       PARENTS       0.50      0.21      0.30      1155\n",
            "      POLITICS       0.62      0.85      0.71      9772\n",
            "  QUEER VOICES       0.71      0.61      0.66      1903\n",
            "      RELIGION       0.61      0.35      0.44       775\n",
            "       SCIENCE       0.58      0.36      0.45       650\n",
            "        SPORTS       0.58      0.52      0.55      1466\n",
            "         STYLE       0.59      0.19      0.29       675\n",
            "STYLE & BEAUTY       0.68      0.77      0.72      2821\n",
            "         TASTE       0.58      0.10      0.17       609\n",
            "          TECH       0.56      0.39      0.46       624\n",
            " THE WORLDPOST       0.50      0.41      0.45      1081\n",
            "        TRAVEL       0.60      0.74      0.66      2942\n",
            "      WEDDINGS       0.79      0.75      0.77      1059\n",
            "    WEIRD NEWS       0.42      0.18      0.26       789\n",
            "      WELLNESS       0.50      0.79      0.61      5377\n",
            "         WOMEN       0.41      0.28      0.33      1008\n",
            "    WORLD NEWS       0.42      0.07      0.12       615\n",
            "     WORLDPOST       0.46      0.19      0.27       791\n",
            "\n",
            "      accuracy                           0.56     60000\n",
            "     macro avg       0.54      0.38      0.42     60000\n",
            "  weighted avg       0.55      0.56      0.53     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.29      0.09      0.14       147\n",
            "ARTS & CULTURE       0.31      0.11      0.16       132\n",
            "  BLACK VOICES       0.53      0.26      0.34       477\n",
            "      BUSINESS       0.44      0.41      0.42       566\n",
            "       COLLEGE       0.33      0.25      0.29       108\n",
            "        COMEDY       0.45      0.24      0.31       502\n",
            "         CRIME       0.53      0.48      0.50       369\n",
            "CULTURE & ARTS       0.35      0.15      0.21       100\n",
            "       DIVORCE       0.76      0.61      0.67       340\n",
            "     EDUCATION       0.30      0.21      0.24        87\n",
            " ENTERTAINMENT       0.46      0.70      0.56      1620\n",
            "   ENVIRONMENT       0.65      0.19      0.29       139\n",
            "         FIFTY       0.14      0.02      0.03       116\n",
            "  FOOD & DRINK       0.55      0.67      0.61       597\n",
            "     GOOD NEWS       0.26      0.09      0.13       152\n",
            "         GREEN       0.39      0.25      0.31       271\n",
            "HEALTHY LIVING       0.29      0.11      0.16       693\n",
            " HOME & LIVING       0.61      0.59      0.60       406\n",
            "        IMPACT       0.35      0.14      0.20       340\n",
            " LATINO VOICES       0.50      0.03      0.06       101\n",
            "         MEDIA       0.46      0.27      0.34       263\n",
            "         MONEY       0.49      0.23      0.32       162\n",
            "     PARENTING       0.48      0.55      0.51       901\n",
            "       PARENTS       0.34      0.16      0.22       380\n",
            "      POLITICS       0.60      0.84      0.70      3264\n",
            "  QUEER VOICES       0.66      0.59      0.62       629\n",
            "      RELIGION       0.53      0.32      0.40       247\n",
            "       SCIENCE       0.48      0.26      0.34       213\n",
            "        SPORTS       0.58      0.51      0.54       490\n",
            "         STYLE       0.48      0.13      0.20       238\n",
            "STYLE & BEAUTY       0.67      0.76      0.71       943\n",
            "         TASTE       0.28      0.04      0.07       210\n",
            "          TECH       0.51      0.29      0.37       224\n",
            " THE WORLDPOST       0.44      0.35      0.39       387\n",
            "        TRAVEL       0.59      0.72      0.65       985\n",
            "      WEDDINGS       0.73      0.71      0.72       350\n",
            "    WEIRD NEWS       0.33      0.13      0.19       262\n",
            "      WELLNESS       0.47      0.77      0.58      1778\n",
            "         WOMEN       0.35      0.24      0.29       341\n",
            "    WORLD NEWS       0.49      0.08      0.13       236\n",
            "     WORLDPOST       0.30      0.14      0.19       234\n",
            "\n",
            "      accuracy                           0.53     20000\n",
            "     macro avg       0.46      0.33      0.36     20000\n",
            "  weighted avg       0.51      0.53      0.49     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.30      0.09      0.14       169\n",
            "ARTS & CULTURE       0.35      0.10      0.16       138\n",
            "  BLACK VOICES       0.43      0.24      0.30       431\n",
            "      BUSINESS       0.43      0.35      0.39       610\n",
            "       COLLEGE       0.46      0.33      0.38       103\n",
            "        COMEDY       0.44      0.26      0.33       476\n",
            "         CRIME       0.49      0.46      0.47       312\n",
            "CULTURE & ARTS       0.50      0.14      0.22       107\n",
            "       DIVORCE       0.78      0.63      0.70       360\n",
            "     EDUCATION       0.45      0.31      0.37        96\n",
            " ENTERTAINMENT       0.41      0.67      0.51      1601\n",
            "   ENVIRONMENT       0.58      0.14      0.23       152\n",
            "         FIFTY       0.30      0.04      0.08       137\n",
            "  FOOD & DRINK       0.52      0.69      0.59       576\n",
            "     GOOD NEWS       0.24      0.06      0.10       127\n",
            "         GREEN       0.37      0.22      0.27       291\n",
            "HEALTHY LIVING       0.26      0.09      0.13       639\n",
            " HOME & LIVING       0.63      0.60      0.61       424\n",
            "        IMPACT       0.41      0.18      0.25       354\n",
            " LATINO VOICES       0.50      0.03      0.06       100\n",
            "         MEDIA       0.52      0.28      0.36       278\n",
            "         MONEY       0.51      0.24      0.32       159\n",
            "     PARENTING       0.46      0.56      0.50       837\n",
            "       PARENTS       0.35      0.17      0.23       376\n",
            "      POLITICS       0.59      0.84      0.69      3168\n",
            "  QUEER VOICES       0.66      0.57      0.61       620\n",
            "      RELIGION       0.56      0.31      0.40       261\n",
            "       SCIENCE       0.51      0.27      0.36       217\n",
            "        SPORTS       0.55      0.48      0.51       505\n",
            "         STYLE       0.51      0.14      0.22       251\n",
            "STYLE & BEAUTY       0.67      0.74      0.70       972\n",
            "         TASTE       0.25      0.02      0.04       250\n",
            "          TECH       0.46      0.29      0.35       203\n",
            " THE WORLDPOST       0.43      0.31      0.36       391\n",
            "        TRAVEL       0.57      0.71      0.64      1030\n",
            "      WEDDINGS       0.75      0.66      0.70       361\n",
            "    WEIRD NEWS       0.32      0.13      0.18       286\n",
            "      WELLNESS       0.48      0.78      0.59      1752\n",
            "         WOMEN       0.39      0.28      0.33       361\n",
            "    WORLD NEWS       0.35      0.05      0.09       240\n",
            "     WORLDPOST       0.40      0.15      0.22       279\n",
            "\n",
            "      accuracy                           0.52     20000\n",
            "     macro avg       0.47      0.33      0.36     20000\n",
            "  weighted avg       0.50      0.52      0.48     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxxuV20Pm9GG"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "class Metrics(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, valid_data):\n",
        "        super(Metrics, self).__init__()\n",
        "        self.validation_data = valid_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)\n",
        "        val_targ = self.validation_data[1]\n",
        "        \n",
        "        if len(val_targ.shape) == 2 and val_targ.shape[1] != 1:\n",
        "            val_targ = np.argmax(val_targ, -1)\n",
        "        val_targ = tf.cast(val_targ,dtype=tf.float32)\n",
        "        \n",
        "        _val_f1 = f1_score(val_targ, val_predict,average=\"weighted\")\n",
        "        _val_recall = recall_score(val_targ, val_predict,average=\"weighted\")\n",
        "        _val_precision = precision_score(val_targ, val_predict,average=\"weighted\")\n",
        "\n",
        "        logs['val_f1'] = _val_f1\n",
        "        logs['val_recall'] = _val_recall\n",
        "        logs['val_precision'] = _val_precision\n",
        "        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" % (_val_f1, _val_precision, _val_recall))\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQjHir8Am9GG",
        "outputId": "37b8137d-51c8-4bd7-83e6-34820c44b33e"
      },
      "source": [
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "\n",
        "with tf.device('/device:GPU:0'):  \n",
        "    model = Sequential()\n",
        "    model.add(Dense(448, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=[\"categorical_crossentropy\"]\n",
        "    )\n",
        "    \n",
        "    if not os.path.exists('./checkpoints'):\n",
        "        os.makedirs('./checkpoints')\n",
        "        \n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'checkpoints/weights.hdf5',\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        verbose=2,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "    \n",
        "    history= model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data= (X_dev, y_dev),\n",
        "        batch_size=256,\n",
        "        epochs=30,\n",
        "        shuffle=True,\n",
        "        callbacks=[Metrics(valid_data=(X_dev, y_dev)), checkpoint]\n",
        "    )\n",
        "\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int)    \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 448)               224448    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 448)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 320)               143680    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 320)               102720    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 41)                13161     \n",
            "=================================================================\n",
            "Total params: 484,009\n",
            "Trainable params: 484,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "235/235 [==============================] - 7s 26ms/step - loss: 2.8981 - categorical_crossentropy: 2.8981 - val_loss: 2.2267 - val_categorical_crossentropy: 2.2267\n",
            " — val_f1: 0.316848 — val_precision: 0.301038 — val_recall: 0.432450\n",
            "\n",
            "Epoch 00001: categorical_crossentropy improved from -inf to 2.89812, saving model to checkpoints/weights.hdf5\n",
            "Epoch 2/30\n",
            "  7/235 [..............................] - ETA: 5s - loss: 2.3960 - categorical_crossentropy: 2.3960"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 6s 26ms/step - loss: 2.2261 - categorical_crossentropy: 2.2261 - val_loss: 1.9780 - val_categorical_crossentropy: 1.9780\n",
            " — val_f1: 0.403841 — val_precision: 0.390308 — val_recall: 0.487300\n",
            "\n",
            "Epoch 00002: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 3/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 2.0492 - categorical_crossentropy: 2.0492 - val_loss: 1.8868 - val_categorical_crossentropy: 1.8868\n",
            " — val_f1: 0.432061 — val_precision: 0.464064 — val_recall: 0.503450\n",
            "\n",
            "Epoch 00003: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 4/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.9530 - categorical_crossentropy: 1.9530 - val_loss: 1.8345 - val_categorical_crossentropy: 1.8345\n",
            " — val_f1: 0.444264 — val_precision: 0.446962 — val_recall: 0.511900\n",
            "\n",
            "Epoch 00004: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 5/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.8857 - categorical_crossentropy: 1.8857 - val_loss: 1.8020 - val_categorical_crossentropy: 1.8020\n",
            " — val_f1: 0.461226 — val_precision: 0.518611 — val_recall: 0.521350\n",
            "\n",
            "Epoch 00005: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 6/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.8321 - categorical_crossentropy: 1.8321 - val_loss: 1.7811 - val_categorical_crossentropy: 1.7811\n",
            " — val_f1: 0.471610 — val_precision: 0.497578 — val_recall: 0.525350\n",
            "\n",
            "Epoch 00006: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 7/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.7843 - categorical_crossentropy: 1.7843 - val_loss: 1.7695 - val_categorical_crossentropy: 1.7695\n",
            " — val_f1: 0.474848 — val_precision: 0.495663 — val_recall: 0.528350\n",
            "\n",
            "Epoch 00007: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 8/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.7445 - categorical_crossentropy: 1.7445 - val_loss: 1.7589 - val_categorical_crossentropy: 1.7589\n",
            " — val_f1: 0.480776 — val_precision: 0.496418 — val_recall: 0.530150\n",
            "\n",
            "Epoch 00008: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 9/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.7092 - categorical_crossentropy: 1.7092 - val_loss: 1.7579 - val_categorical_crossentropy: 1.7579\n",
            " — val_f1: 0.481451 — val_precision: 0.492042 — val_recall: 0.528050\n",
            "\n",
            "Epoch 00009: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 10/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.6774 - categorical_crossentropy: 1.6774 - val_loss: 1.7495 - val_categorical_crossentropy: 1.7495\n",
            " — val_f1: 0.486362 — val_precision: 0.499937 — val_recall: 0.534250\n",
            "\n",
            "Epoch 00010: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 11/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.6436 - categorical_crossentropy: 1.6436 - val_loss: 1.7473 - val_categorical_crossentropy: 1.7473\n",
            " — val_f1: 0.489114 — val_precision: 0.505246 — val_recall: 0.534750\n",
            "\n",
            "Epoch 00011: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 12/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.6126 - categorical_crossentropy: 1.6126 - val_loss: 1.7444 - val_categorical_crossentropy: 1.7444\n",
            " — val_f1: 0.494281 — val_precision: 0.500170 — val_recall: 0.535550\n",
            "\n",
            "Epoch 00012: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 13/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.5835 - categorical_crossentropy: 1.5835 - val_loss: 1.7413 - val_categorical_crossentropy: 1.7413\n",
            " — val_f1: 0.497995 — val_precision: 0.506703 — val_recall: 0.537250\n",
            "\n",
            "Epoch 00013: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 14/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.5566 - categorical_crossentropy: 1.5566 - val_loss: 1.7472 - val_categorical_crossentropy: 1.7472\n",
            " — val_f1: 0.494221 — val_precision: 0.504813 — val_recall: 0.535500\n",
            "\n",
            "Epoch 00014: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 15/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.5280 - categorical_crossentropy: 1.5280 - val_loss: 1.7518 - val_categorical_crossentropy: 1.7518\n",
            " — val_f1: 0.495234 — val_precision: 0.499405 — val_recall: 0.534850\n",
            "\n",
            "Epoch 00015: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 16/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.5034 - categorical_crossentropy: 1.5034 - val_loss: 1.7610 - val_categorical_crossentropy: 1.7610\n",
            " — val_f1: 0.501537 — val_precision: 0.503001 — val_recall: 0.535850\n",
            "\n",
            "Epoch 00016: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 17/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4830 - categorical_crossentropy: 1.4830 - val_loss: 1.7612 - val_categorical_crossentropy: 1.7612\n",
            " — val_f1: 0.497916 — val_precision: 0.500654 — val_recall: 0.535750\n",
            "\n",
            "Epoch 00017: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 18/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4593 - categorical_crossentropy: 1.4593 - val_loss: 1.7627 - val_categorical_crossentropy: 1.7627\n",
            " — val_f1: 0.501351 — val_precision: 0.501631 — val_recall: 0.535650\n",
            "\n",
            "Epoch 00018: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 19/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4410 - categorical_crossentropy: 1.4410 - val_loss: 1.7811 - val_categorical_crossentropy: 1.7811\n",
            " — val_f1: 0.498499 — val_precision: 0.498208 — val_recall: 0.533450\n",
            "\n",
            "Epoch 00019: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 20/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4250 - categorical_crossentropy: 1.4250 - val_loss: 1.7762 - val_categorical_crossentropy: 1.7762\n",
            " — val_f1: 0.500474 — val_precision: 0.506055 — val_recall: 0.533300\n",
            "\n",
            "Epoch 00020: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 21/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4049 - categorical_crossentropy: 1.4049 - val_loss: 1.7770 - val_categorical_crossentropy: 1.7770\n",
            " — val_f1: 0.503085 — val_precision: 0.501992 — val_recall: 0.535500\n",
            "\n",
            "Epoch 00021: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 22/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.3919 - categorical_crossentropy: 1.3919 - val_loss: 1.7873 - val_categorical_crossentropy: 1.7873\n",
            " — val_f1: 0.501999 — val_precision: 0.499180 — val_recall: 0.535550\n",
            "\n",
            "Epoch 00022: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 23/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3686 - categorical_crossentropy: 1.3686 - val_loss: 1.7889 - val_categorical_crossentropy: 1.7889\n",
            " — val_f1: 0.502478 — val_precision: 0.504906 — val_recall: 0.536000\n",
            "\n",
            "Epoch 00023: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 24/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.3575 - categorical_crossentropy: 1.3575 - val_loss: 1.7978 - val_categorical_crossentropy: 1.7978\n",
            " — val_f1: 0.503681 — val_precision: 0.505884 — val_recall: 0.536150\n",
            "\n",
            "Epoch 00024: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 25/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3495 - categorical_crossentropy: 1.3495 - val_loss: 1.7949 - val_categorical_crossentropy: 1.7949\n",
            " — val_f1: 0.504575 — val_precision: 0.502363 — val_recall: 0.536150\n",
            "\n",
            "Epoch 00025: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 26/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3350 - categorical_crossentropy: 1.3350 - val_loss: 1.8055 - val_categorical_crossentropy: 1.8055\n",
            " — val_f1: 0.506619 — val_precision: 0.514358 — val_recall: 0.536500\n",
            "\n",
            "Epoch 00026: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 27/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3179 - categorical_crossentropy: 1.3179 - val_loss: 1.8161 - val_categorical_crossentropy: 1.8161\n",
            " — val_f1: 0.504297 — val_precision: 0.502617 — val_recall: 0.533850\n",
            "\n",
            "Epoch 00027: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 28/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3073 - categorical_crossentropy: 1.3073 - val_loss: 1.8247 - val_categorical_crossentropy: 1.8247\n",
            " — val_f1: 0.503157 — val_precision: 0.501831 — val_recall: 0.533350\n",
            "\n",
            "Epoch 00028: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 29/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.2985 - categorical_crossentropy: 1.2985 - val_loss: 1.8359 - val_categorical_crossentropy: 1.8359\n",
            " — val_f1: 0.505831 — val_precision: 0.504994 — val_recall: 0.535350\n",
            "\n",
            "Epoch 00029: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 30/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.2834 - categorical_crossentropy: 1.2834 - val_loss: 1.8353 - val_categorical_crossentropy: 1.8353\n",
            " — val_f1: 0.506289 — val_precision: 0.503671 — val_recall: 0.534400\n",
            "\n",
            "Epoch 00030: categorical_crossentropy did not improve from 2.89812\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.75      0.02      0.04       147\n",
            "ARTS & CULTURE       0.50      0.03      0.06       132\n",
            "  BLACK VOICES       0.72      0.17      0.27       477\n",
            "      BUSINESS       0.59      0.28      0.38       566\n",
            "       COLLEGE       0.45      0.16      0.23       108\n",
            "        COMEDY       0.64      0.20      0.31       502\n",
            "         CRIME       0.58      0.35      0.44       369\n",
            "CULTURE & ARTS       0.45      0.05      0.09       100\n",
            "       DIVORCE       0.83      0.57      0.68       340\n",
            "     EDUCATION       0.41      0.14      0.21        87\n",
            " ENTERTAINMENT       0.67      0.50      0.57      1620\n",
            "   ENVIRONMENT       0.69      0.14      0.24       139\n",
            "         FIFTY       0.00      0.00      0.00       116\n",
            "  FOOD & DRINK       0.63      0.60      0.61       597\n",
            "     GOOD NEWS       0.00      0.00      0.00       152\n",
            "         GREEN       0.45      0.10      0.17       271\n",
            "HEALTHY LIVING       0.56      0.07      0.13       693\n",
            " HOME & LIVING       0.69      0.56      0.62       406\n",
            "        IMPACT       0.57      0.06      0.11       340\n",
            " LATINO VOICES       0.00      0.00      0.00       101\n",
            "         MEDIA       0.54      0.19      0.28       263\n",
            "         MONEY       0.56      0.21      0.30       162\n",
            "     PARENTING       0.56      0.50      0.53       901\n",
            "       PARENTS       0.52      0.09      0.15       380\n",
            "      POLITICS       0.75      0.73      0.74      3264\n",
            "  QUEER VOICES       0.77      0.55      0.64       629\n",
            "      RELIGION       0.64      0.24      0.35       247\n",
            "       SCIENCE       0.54      0.18      0.27       213\n",
            "        SPORTS       0.60      0.47      0.52       490\n",
            "         STYLE       0.63      0.17      0.27       238\n",
            "STYLE & BEAUTY       0.79      0.72      0.75       943\n",
            "         TASTE       0.00      0.00      0.00       210\n",
            "          TECH       0.63      0.25      0.36       224\n",
            " THE WORLDPOST       0.52      0.25      0.34       387\n",
            "        TRAVEL       0.73      0.64      0.68       985\n",
            "      WEDDINGS       0.75      0.69      0.72       350\n",
            "    WEIRD NEWS       0.59      0.04      0.07       262\n",
            "      WELLNESS       0.62      0.61      0.62      1778\n",
            "         WOMEN       0.43      0.13      0.20       341\n",
            "    WORLD NEWS       0.00      0.00      0.00       236\n",
            "     WORLDPOST       0.17      0.00      0.01       234\n",
            "\n",
            "     micro avg       0.68      0.44      0.53     20000\n",
            "     macro avg       0.52      0.26      0.32     20000\n",
            "  weighted avg       0.62      0.44      0.48     20000\n",
            "   samples avg       0.44      0.44      0.44     20000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "NdzCa2RpbCt7",
        "outputId": "0c50fe0f-aaac-4671-a0f0-82071ed987b5"
      },
      "source": [
        "import warnings\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(448, input_dim=X_train.shape[1], activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    # Load weights from the pre-trained model\n",
        "    model.load_weights(\"checkpoints/weights.hdf5\")\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy'])\n",
        "    )\n",
        "\n",
        "    print(\"Classification report on the training data:\")\n",
        "    predictions_train = model.predict(X_train)\n",
        "    predictions_train = (predictions_train > 0.5).astype(int)   \n",
        "    print(classification_report(y_train, predictions_train, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the development data:\")\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int) \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the test data:\")\n",
        "    predictions_test = model.predict(X_test)\n",
        "    predictions_test = (predictions_test > 0.5).astype(int)    \n",
        "    print(classification_report(y_test, predictions_test, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-e012c688c25e>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB3DZqOEd7Kp"
      },
      "source": [
        "class DeepSelfAttention(layers.Layer):\n",
        "\n",
        "    def __init__(self, input_dim, dense1_dim, dense2_dim,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.dense1_dim = dense1_dim\n",
        "        self.dense2_dim = dense2_dim\n",
        "        self.dense1 = Dense(dense1_dim)\n",
        "        self.dense2 = Dense(dense2_dim)\n",
        "        self.dense3 = Dense(1)\n",
        "\n",
        "    def call(self, input):\n",
        "        x = self.dense1(input)\n",
        "        x = self.dense2(x)\n",
        "        x = self.dense3(x)\n",
        "        output = tf.nn.softmax(x, axis=-1)\n",
        "        return tf.math.reduce_sum( tf.math.multiply(input,output) , axis=1 )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_dim\" : self.input_dim,\n",
        "            \"dense1_dim\" : self.dense1_dim,\n",
        "            \"dense2_dim\" : self.dense2_dim\n",
        "        })\n",
        "        return config    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVf6le9PNmyS",
        "outputId": "0667732d-d360-4b87-b275-44e3047aa833"
      },
      "source": [
        "pip install keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 22.8 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 92 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.4 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVi0jyJUNxr6"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P5zzcF2VQ9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "588a40d6-5340-4970-bc75-f5a00d596eb3"
      },
      "source": [
        "#hyperparameter tuning\n",
        "\n",
        "def build_model(hp):\n",
        "    GRU_SIZE = 64\n",
        "    DENSE = hp.Int('dense' , min_value=32, max_value=416, step=128)\n",
        "    DROPOUT = hp.Float('dropout' , min_value=0.2, max_value=0.5, step=0.15)\n",
        "    ATTENTION_DENSE1 = hp.Int('att_dense1' , min_value=32, max_value=416, step=128)\n",
        "    ATTENTION_DENSE2 = hp.Int('att_dense2' , min_value=32, max_value=416, step=128)\n",
        "    STACKED_GRUs = hp.Int('GRUs' , min_value=1, max_value=4, step=1)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "                    ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
        "    model.add(Dropout(DROPOUT))\n",
        "\n",
        "    for i in range(STACKED_GRUs):\n",
        "        model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
        "        model.add(Dropout(DROPOUT))\n",
        "\n",
        "    model.add(DeepSelfAttention(MAX_SEQUENCE_LENGTH, ATTENTION_DENSE1, ATTENTION_DENSE2))\n",
        "    model.add(Dropout(DROPOUT))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True )\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 6\n",
            "dense (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 416, 'step': 128, 'sampling': None}\n",
            "dropout (Float)\n",
            "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.5, 'step': 0.15, 'sampling': None}\n",
            "att_dense1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 416, 'step': 128, 'sampling': None}\n",
            "att_dense2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 416, 'step': 128, 'sampling': None}\n",
            "GRUs (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 4, 'step': 1, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSA7kk8dNi6b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu3iV4tJHEIo",
        "outputId": "7c034274-ba55-40e5-b4b5-2772c364a1de"
      },
      "source": [
        "with tf.device('/device:GPU:0'):  \n",
        "    tuner.search(X_train, y_train,\n",
        "                epochs=8,\n",
        "                batch_size = 256,\n",
        "                validation_data=(X_dev, y_dev))\n",
        "    \n",
        "    best_model = tuner.get_best_models()[0]\n",
        "\n",
        "    best_model.save('/savedModels/bestGRUatt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 23m 40s]\n",
            "accuracy: 0.5806083381175995\n",
            "\n",
            "Best accuracy So Far: 0.6093166768550873\n",
            "Total elapsed time: 03h 12m 47s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dense_layer_call_and_return_conditional_losses, dense_layer_call_fn, dense_1_layer_call_and_return_conditional_losses, dense_1_layer_call_fn, dense_2_layer_call_and_return_conditional_losses while saving (showing 5 of 45). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /savedModels/bestGRUatt/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /savedModels/bestGRUatt/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmFXM-JSX38g"
      },
      "source": [
        "best_model = tuner.get_best_models()[0]\n",
        "\n",
        "best_model.save('/savedModels/bestGRUatt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrYt46cCqUcA"
      },
      "source": [
        "new_model = tf.keras.models.load_model('/savedModels/bestGRUatt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bd_Jnxye39k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTJXB2A6xchb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b01547-7677-4914-d44d-210e82b28e03"
      },
      "source": [
        "\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    GRU_SIZE = 64\n",
        "    DENSE = 32\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "                    ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(DeepSelfAttention(MAX_SEQUENCE_LENGTH, 100))\n",
        "    model.add(Dropout(0.33))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    if not os.path.exists('/content/gdrive/My Drive/checkpoints'):\n",
        "        os.makedirs('/content/gdrive/My Drive/checkpoints')\n",
        "\n",
        "    checkpoint = ModelCheckpoint('/content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5',\n",
        "                            monitor='val_accuracy', \n",
        "                            mode='max', verbose=2,\n",
        "                            save_best_only=True,\n",
        "                            save_weights_only=True)\n",
        "\n",
        "    history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data = (X_dev,y_dev),\n",
        "                    batch_size=256,\n",
        "                    epochs=50,\n",
        "                    shuffle=True,\n",
        "                    callbacks= [Metrics(valid_data=(X_dev,y_dev)), checkpoint] )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 250, 300)          30000600  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 250, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 250, 128)          140544    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 250, 128)          0         \n",
            "_________________________________________________________________\n",
            "deep_self_attention (DeepSel (None, 128)               13001     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 41)                5289      \n",
            "=================================================================\n",
            "Total params: 30,159,434\n",
            "Trainable params: 158,834\n",
            "Non-trainable params: 30,000,600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "235/235 [==============================] - 46s 139ms/step - loss: 2.4090 - accuracy: 0.4021 - val_loss: 1.8803 - val_accuracy: 0.5217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " — val_f1: 0.465835 — val_precision: 0.488547 — val_recall: 0.521750\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.52175, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 2/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.9453 - accuracy: 0.4937 - val_loss: 1.7536 - val_accuracy: 0.5402\n",
            " — val_f1: 0.492048 — val_precision: 0.514563 — val_recall: 0.540200\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.52175 to 0.54020, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 3/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.8374 - accuracy: 0.5149 - val_loss: 1.6903 - val_accuracy: 0.5531\n",
            " — val_f1: 0.508575 — val_precision: 0.528597 — val_recall: 0.553150\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.54020 to 0.55315, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 4/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.7802 - accuracy: 0.5279 - val_loss: 1.6561 - val_accuracy: 0.5595\n",
            " — val_f1: 0.518680 — val_precision: 0.538523 — val_recall: 0.559550\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.55315 to 0.55955, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 5/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.7363 - accuracy: 0.5370 - val_loss: 1.6316 - val_accuracy: 0.5623\n",
            " — val_f1: 0.521611 — val_precision: 0.544110 — val_recall: 0.562250\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.55955 to 0.56225, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 6/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.7082 - accuracy: 0.5431 - val_loss: 1.6094 - val_accuracy: 0.5719\n",
            " — val_f1: 0.531748 — val_precision: 0.556157 — val_recall: 0.571850\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.56225 to 0.57185, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 7/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.6840 - accuracy: 0.5504 - val_loss: 1.5879 - val_accuracy: 0.5743\n",
            " — val_f1: 0.538785 — val_precision: 0.557700 — val_recall: 0.574350\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.57185 to 0.57435, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 8/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.6639 - accuracy: 0.5531 - val_loss: 1.5728 - val_accuracy: 0.5783\n",
            " — val_f1: 0.544023 — val_precision: 0.563255 — val_recall: 0.578300\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.57435 to 0.57830, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 9/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.6390 - accuracy: 0.5574 - val_loss: 1.5540 - val_accuracy: 0.5815\n",
            " — val_f1: 0.550343 — val_precision: 0.562454 — val_recall: 0.581500\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.57830 to 0.58150, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 10/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.6247 - accuracy: 0.5608 - val_loss: 1.5475 - val_accuracy: 0.5844\n",
            " — val_f1: 0.553667 — val_precision: 0.566246 — val_recall: 0.584400\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.58150 to 0.58440, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 11/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.6078 - accuracy: 0.5634 - val_loss: 1.5320 - val_accuracy: 0.5865\n",
            " — val_f1: 0.559057 — val_precision: 0.566901 — val_recall: 0.586550\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.58440 to 0.58655, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 12/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5937 - accuracy: 0.5670 - val_loss: 1.5261 - val_accuracy: 0.5860\n",
            " — val_f1: 0.553460 — val_precision: 0.573902 — val_recall: 0.586050\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.58655\n",
            "Epoch 13/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.5738 - accuracy: 0.5707 - val_loss: 1.5180 - val_accuracy: 0.5873\n",
            " — val_f1: 0.555313 — val_precision: 0.576857 — val_recall: 0.587300\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.58655 to 0.58730, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 14/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.5651 - accuracy: 0.5741 - val_loss: 1.5031 - val_accuracy: 0.5917\n",
            " — val_f1: 0.562413 — val_precision: 0.575403 — val_recall: 0.591700\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.58730 to 0.59170, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 15/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.5489 - accuracy: 0.5746 - val_loss: 1.4915 - val_accuracy: 0.5930\n",
            " — val_f1: 0.565796 — val_precision: 0.575438 — val_recall: 0.593000\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.59170 to 0.59300, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 16/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5345 - accuracy: 0.5787 - val_loss: 1.4866 - val_accuracy: 0.5946\n",
            " — val_f1: 0.570094 — val_precision: 0.581390 — val_recall: 0.594600\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.59300 to 0.59460, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 17/30\n",
            "235/235 [==============================] - 29s 122ms/step - loss: 1.5253 - accuracy: 0.5811 - val_loss: 1.4760 - val_accuracy: 0.5960\n",
            " — val_f1: 0.568001 — val_precision: 0.582355 — val_recall: 0.595950\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.59460 to 0.59595, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 18/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5139 - accuracy: 0.5841 - val_loss: 1.4704 - val_accuracy: 0.5984\n",
            " — val_f1: 0.569593 — val_precision: 0.583549 — val_recall: 0.598400\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.59595 to 0.59840, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 19/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5024 - accuracy: 0.5841 - val_loss: 1.4623 - val_accuracy: 0.5992\n",
            " — val_f1: 0.573875 — val_precision: 0.586309 — val_recall: 0.599200\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.59840 to 0.59920, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 20/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4928 - accuracy: 0.5878 - val_loss: 1.4506 - val_accuracy: 0.6034\n",
            " — val_f1: 0.579036 — val_precision: 0.589338 — val_recall: 0.603400\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.59920 to 0.60340, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 21/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4865 - accuracy: 0.5906 - val_loss: 1.4459 - val_accuracy: 0.6066\n",
            " — val_f1: 0.582243 — val_precision: 0.594324 — val_recall: 0.606650\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.60340 to 0.60665, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 22/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4719 - accuracy: 0.5900 - val_loss: 1.4396 - val_accuracy: 0.6037\n",
            " — val_f1: 0.580051 — val_precision: 0.588135 — val_recall: 0.603700\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.60665\n",
            "Epoch 23/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.4628 - accuracy: 0.5954 - val_loss: 1.4475 - val_accuracy: 0.5986\n",
            " — val_f1: 0.569546 — val_precision: 0.592334 — val_recall: 0.598550\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.60665\n",
            "Epoch 24/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4549 - accuracy: 0.5952 - val_loss: 1.4328 - val_accuracy: 0.6044\n",
            " — val_f1: 0.579258 — val_precision: 0.593380 — val_recall: 0.604450\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.60665\n",
            "Epoch 25/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4488 - accuracy: 0.5947 - val_loss: 1.4240 - val_accuracy: 0.6079\n",
            " — val_f1: 0.584673 — val_precision: 0.594804 — val_recall: 0.607900\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.60665 to 0.60790, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 26/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4382 - accuracy: 0.5983 - val_loss: 1.4250 - val_accuracy: 0.6080\n",
            " — val_f1: 0.586376 — val_precision: 0.595832 — val_recall: 0.608050\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.60790 to 0.60805, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 27/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4301 - accuracy: 0.6011 - val_loss: 1.4164 - val_accuracy: 0.6114\n",
            " — val_f1: 0.589511 — val_precision: 0.594883 — val_recall: 0.611400\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.60805 to 0.61140, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 28/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.4216 - accuracy: 0.6026 - val_loss: 1.4226 - val_accuracy: 0.6075\n",
            " — val_f1: 0.588596 — val_precision: 0.599549 — val_recall: 0.607450\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.61140\n",
            "Epoch 29/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4122 - accuracy: 0.6031 - val_loss: 1.4153 - val_accuracy: 0.6102\n",
            " — val_f1: 0.585160 — val_precision: 0.600632 — val_recall: 0.610250\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.61140\n",
            "Epoch 30/30\n",
            "235/235 [==============================] - 28s 115ms/step - loss: 1.4081 - accuracy: 0.6064 - val_loss: 1.4082 - val_accuracy: 0.6119\n",
            " — val_f1: 0.590154 — val_precision: 0.596906 — val_recall: 0.611950\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.61140 to 0.61195, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yVvBZJyW7nLZ",
        "outputId": "aee4e964-7776-48fb-f32b-864549f83c47"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7431996550>]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK2kDnT6Gnc5",
        "outputId": "89a01ece-c520-4aa4-b53e-605c508a5458"
      },
      "source": [
        "import warnings\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    GRU_SIZE = 64\n",
        "    DENSE = 32\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "                    ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(DeepSelfAttention(MAX_SEQUENCE_LENGTH, 100))\n",
        "    model.add(Dropout(0.33))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    # Load weights from the pre-trained model\n",
        "    model.load_weights('/content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5')\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=[\"categorical_crossentropy\"]\n",
        "    )\n",
        "\n",
        "    print(\"Classification report on the training data:\")\n",
        "    predictions_train = model.predict(X_train)\n",
        "    predictions_train = (predictions_train > 0.5).astype(int)   \n",
        "    print(classification_report(y_train, predictions_train, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the development data:\")\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int) \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the test data:\")\n",
        "    predictions_test = model.predict(X_test)\n",
        "    predictions_test = (predictions_test > 0.5).astype(int)    \n",
        "    print(classification_report(y_test, predictions_test, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 250, 300)          30000600  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 250, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 250, 128)          140544    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 250, 128)          0         \n",
            "_________________________________________________________________\n",
            "deep_self_attention_1 (DeepS (None, 128)               13001     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 41)                5289      \n",
            "=================================================================\n",
            "Total params: 30,159,434\n",
            "Trainable params: 158,834\n",
            "Non-trainable params: 30,000,600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Classification report on the training data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.77      0.15      0.25       458\n",
            "ARTS & CULTURE       0.69      0.12      0.21       380\n",
            "  BLACK VOICES       0.68      0.32      0.44      1420\n",
            "      BUSINESS       0.70      0.32      0.44      1711\n",
            "       COLLEGE       0.57      0.29      0.39       329\n",
            "        COMEDY       0.78      0.33      0.46      1549\n",
            "         CRIME       0.71      0.41      0.52      1016\n",
            "CULTURE & ARTS       0.86      0.33      0.48       323\n",
            "       DIVORCE       0.93      0.64      0.76      1040\n",
            "     EDUCATION       0.64      0.15      0.24       305\n",
            " ENTERTAINMENT       0.77      0.63      0.69      4719\n",
            "   ENVIRONMENT       0.72      0.35      0.47       419\n",
            "         FIFTY       0.73      0.15      0.24       427\n",
            "  FOOD & DRINK       0.74      0.73      0.74      1846\n",
            "     GOOD NEWS       0.75      0.08      0.14       423\n",
            "         GREEN       0.67      0.14      0.24       767\n",
            "HEALTHY LIVING       0.72      0.11      0.19      2022\n",
            " HOME & LIVING       0.89      0.67      0.77      1295\n",
            "        IMPACT       0.72      0.14      0.24      1000\n",
            " LATINO VOICES       0.69      0.21      0.32       340\n",
            "         MEDIA       0.76      0.23      0.35       831\n",
            "         MONEY       0.71      0.35      0.47       544\n",
            "     PARENTING       0.66      0.58      0.62      2556\n",
            "       PARENTS       0.81      0.04      0.08      1168\n",
            "      POLITICS       0.86      0.72      0.78      9739\n",
            "  QUEER VOICES       0.86      0.61      0.71      1903\n",
            "      RELIGION       0.74      0.36      0.49       760\n",
            "       SCIENCE       0.83      0.37      0.52       655\n",
            "        SPORTS       0.83      0.59      0.69      1483\n",
            "         STYLE       0.87      0.16      0.27       650\n",
            "STYLE & BEAUTY       0.89      0.82      0.85      2840\n",
            "         TASTE       0.89      0.03      0.05       612\n",
            "          TECH       0.75      0.37      0.50       617\n",
            " THE WORLDPOST       0.68      0.49      0.57      1072\n",
            "        TRAVEL       0.87      0.72      0.79      2970\n",
            "      WEDDINGS       0.88      0.77      0.82      1094\n",
            "    WEIRD NEWS       0.84      0.13      0.22       794\n",
            "      WELLNESS       0.77      0.65      0.71      5475\n",
            "         WOMEN       0.54      0.12      0.19      1005\n",
            "    WORLD NEWS       0.85      0.04      0.07       656\n",
            "     WORLDPOST       0.67      0.27      0.38       787\n",
            "\n",
            "     micro avg       0.80      0.51      0.62     60000\n",
            "     macro avg       0.76      0.36      0.45     60000\n",
            "  weighted avg       0.79      0.51      0.59     60000\n",
            "   samples avg       0.51      0.51      0.51     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.62      0.10      0.18       145\n",
            "ARTS & CULTURE       0.35      0.05      0.08       145\n",
            "  BLACK VOICES       0.59      0.26      0.36       446\n",
            "      BUSINESS       0.66      0.29      0.40       630\n",
            "       COLLEGE       0.45      0.19      0.27       118\n",
            "        COMEDY       0.72      0.28      0.40       536\n",
            "         CRIME       0.64      0.32      0.42       329\n",
            "CULTURE & ARTS       0.69      0.27      0.39       100\n",
            "       DIVORCE       0.89      0.61      0.73       331\n",
            "     EDUCATION       0.38      0.14      0.21        83\n",
            " ENTERTAINMENT       0.71      0.60      0.65      1629\n",
            "   ENVIRONMENT       0.73      0.36      0.48       135\n",
            "         FIFTY       0.54      0.13      0.21       118\n",
            "  FOOD & DRINK       0.70      0.66      0.68       634\n",
            "     GOOD NEWS       0.71      0.08      0.14       133\n",
            "         GREEN       0.43      0.09      0.14       266\n",
            "HEALTHY LIVING       0.49      0.07      0.12       647\n",
            " HOME & LIVING       0.83      0.64      0.72       406\n",
            "        IMPACT       0.56      0.11      0.19       332\n",
            " LATINO VOICES       0.69      0.15      0.25       117\n",
            "         MEDIA       0.58      0.17      0.27       249\n",
            "         MONEY       0.61      0.34      0.44       172\n",
            "     PARENTING       0.62      0.52      0.57       832\n",
            "       PARENTS       0.67      0.04      0.07       377\n",
            "      POLITICS       0.83      0.68      0.75      3259\n",
            "  QUEER VOICES       0.81      0.55      0.66       669\n",
            "      RELIGION       0.63      0.31      0.42       254\n",
            "       SCIENCE       0.77      0.32      0.46       216\n",
            "        SPORTS       0.82      0.49      0.61       478\n",
            "         STYLE       0.65      0.11      0.18       246\n",
            "STYLE & BEAUTY       0.86      0.80      0.83       978\n",
            "         TASTE       0.25      0.01      0.02       230\n",
            "          TECH       0.64      0.33      0.43       193\n",
            " THE WORLDPOST       0.62      0.40      0.49       371\n",
            "        TRAVEL       0.81      0.66      0.72       943\n",
            "      WEDDINGS       0.85      0.74      0.79       371\n",
            "    WEIRD NEWS       0.55      0.08      0.15       273\n",
            "      WELLNESS       0.73      0.62      0.67      1796\n",
            "         WOMEN       0.55      0.13      0.21       359\n",
            "    WORLD NEWS       0.50      0.03      0.06       216\n",
            "     WORLDPOST       0.60      0.21      0.32       238\n",
            "\n",
            "     micro avg       0.75      0.47      0.58     20000\n",
            "     macro avg       0.64      0.32      0.39     20000\n",
            "  weighted avg       0.71      0.47      0.54     20000\n",
            "   samples avg       0.47      0.47      0.47     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.48      0.07      0.13       150\n",
            "ARTS & CULTURE       0.60      0.11      0.18       140\n",
            "  BLACK VOICES       0.56      0.26      0.35       432\n",
            "      BUSINESS       0.59      0.26      0.36       588\n",
            "       COLLEGE       0.50      0.19      0.27       102\n",
            "        COMEDY       0.76      0.27      0.40       526\n",
            "         CRIME       0.67      0.31      0.43       352\n",
            "CULTURE & ARTS       0.60      0.26      0.36        98\n",
            "       DIVORCE       0.92      0.62      0.74       371\n",
            "     EDUCATION       0.63      0.12      0.20       104\n",
            " ENTERTAINMENT       0.75      0.62      0.68      1651\n",
            "   ENVIRONMENT       0.57      0.29      0.39       151\n",
            "         FIFTY       0.42      0.08      0.14       124\n",
            "  FOOD & DRINK       0.69      0.68      0.69       586\n",
            "     GOOD NEWS       0.67      0.04      0.08       136\n",
            "         GREEN       0.59      0.13      0.21       261\n",
            "HEALTHY LIVING       0.56      0.08      0.13       662\n",
            " HOME & LIVING       0.81      0.63      0.71       408\n",
            "        IMPACT       0.63      0.12      0.20       375\n",
            " LATINO VOICES       0.74      0.16      0.26       109\n",
            "         MEDIA       0.62      0.17      0.27       288\n",
            "         MONEY       0.59      0.32      0.42       165\n",
            "     PARENTING       0.62      0.52      0.57       879\n",
            "       PARENTS       0.62      0.03      0.06       392\n",
            "      POLITICS       0.83      0.67      0.74      3202\n",
            "  QUEER VOICES       0.79      0.59      0.68       626\n",
            "      RELIGION       0.65      0.33      0.44       248\n",
            "       SCIENCE       0.80      0.28      0.41       212\n",
            "        SPORTS       0.76      0.47      0.58       514\n",
            "         STYLE       0.83      0.11      0.20       215\n",
            "STYLE & BEAUTY       0.87      0.77      0.82       987\n",
            "         TASTE       0.43      0.01      0.03       210\n",
            "          TECH       0.66      0.31      0.42       207\n",
            " THE WORLDPOST       0.64      0.42      0.50       371\n",
            "        TRAVEL       0.84      0.65      0.73      1010\n",
            "      WEDDINGS       0.86      0.76      0.81       374\n",
            "    WEIRD NEWS       0.62      0.09      0.16       257\n",
            "      WELLNESS       0.74      0.65      0.69      1728\n",
            "         WOMEN       0.43      0.10      0.16       330\n",
            "    WORLD NEWS       0.57      0.02      0.03       224\n",
            "     WORLDPOST       0.54      0.23      0.32       235\n",
            "\n",
            "     micro avg       0.75      0.47      0.58     20000\n",
            "     macro avg       0.66      0.31      0.39     20000\n",
            "  weighted avg       0.72      0.47      0.54     20000\n",
            "   samples avg       0.47      0.47      0.47     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}