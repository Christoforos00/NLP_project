{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_hw1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPU1PY9mxEyIRHx14tfG1Xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christoforos00/NLP_project/blob/main/nlp_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UL3xk0AzGE8",
        "outputId": "5c5ae788-3384-45e5-fca4-7f2ead7b2fd1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "corpus = gutenberg.raw('melville-moby_dick.txt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky5sia2m2_Dj"
      },
      "source": [
        "tokens = word_tokenize(corpus)          #tokenize the whole corpus and find words with under 10 occurancies\n",
        "counts = nltk.FreqDist(tokens)\n",
        "under10 = [w for w in counts if counts[w]<=10]\n",
        "tokens = [token for token in tokens if token not in under10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX5WwjIZ-zwW"
      },
      "source": [
        "sentences = sent_tokenize(corpus)         #tokenize per sentence\n",
        "sentences_tokenized = []\n",
        "\n",
        "for sentance in sentences:\n",
        "    sent_tok = word_tokenize(sentance)  \n",
        "    sentences_tokenized.append(sent_tok)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trbb4alV1hBa"
      },
      "source": [
        "from random import shuffle                    #shuffle and split the corpus\n",
        "shuffle(sentences_tokenized)\n",
        "\n",
        "sentances_num = len(sentences_tokenized)\n",
        "train_sentances = sentences_tokenized[:int(sentances_num*0.6)]\n",
        "dev_sentances = sentences_tokenized[int(sentances_num*0.6):int(sentances_num*0.8)]\n",
        "test_sentances = sentences_tokenized[int(sentances_num*0.8):]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Na53h4CActF"
      },
      "source": [
        "from collections import Counter         #create the bigram and trigram models\n",
        "from nltk.util import ngrams\n",
        "\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sentance in train_sentances:\n",
        "    \n",
        "    sentance = [word for word in sentance if word in tokens]\n",
        "\n",
        "    unigram_counter.update([gram for gram in ngrams(sentance, 1, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sentance, 2, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n",
        "    trigram_counter.update([gram for gram in ngrams(sentance, 3, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<s>',right_pad_symbol='<e>') ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5fe2AOC43Kg"
      },
      "source": [
        "import math          #implement the models (i)\n",
        "\n",
        "\n",
        "def biGramModel(bigram_counter, unigram_counter, sentences_tokenized, alpha, vocab_size):\n",
        "    sum_prob = 0\n",
        "    bigram_cnt = 0\n",
        "    for sent in sentences_tokenized:\n",
        "        sent = ['<s>'] + sent + ['<e>']\n",
        "        for idx in range(1,len(sent)):\n",
        "            if ( sent[idx-1]=='<s>' ):\n",
        "                continue\n",
        "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +alpha) / (unigram_counter[(sent[idx-1],)] + alpha*vocab_size)\n",
        "            sum_prob += math.log2(bigram_prob)\n",
        "            bigram_cnt+=1\n",
        "\n",
        "    CE = -sum_prob / bigram_cnt\n",
        "    perpl = math.pow(2,CE)\n",
        "    return CE, perpl\n",
        "\n",
        "\n",
        "def triGramModel(trigram_counter, bigram_counter, sentences_tokenized, alpha, vocab_size):\n",
        "    sum_prob = 0\n",
        "    trigram_cnt = 0\n",
        "\n",
        "    for sent in sentences_tokenized:\n",
        "        sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n",
        "        for idx in range(2,len(sent)-1):\n",
        "            if (sent[idx-2]=='<s>' or sent[idx-1]=='<s>' ):\n",
        "              continue\n",
        "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size)\n",
        "            sum_prob += math.log2(trigram_prob)\n",
        "            trigram_cnt+=1\n",
        "\n",
        "    CE = -sum_prob / trigram_cnt\n",
        "    perpl = math.pow(2,CE)\n",
        "    return CE, perpl\n",
        "\n",
        "def interpModel(trigram_counter, bigram_counter, unigram_counter, sentences_tokenized, alpha, vocab_size, lamda):\n",
        "    sum_prob = 0\n",
        "    trigram_cnt = 0\n",
        "\n",
        "    for sent in sentences_tokenized:\n",
        "        sent = ['<s>'] + ['<s>'] + sent + ['<e>'] + ['<e>']\n",
        "        for idx in range(2,len(sent)-1):\n",
        "            if (sent[idx-2]=='<s>' or sent[idx-1]=='<s>' ):\n",
        "                continue\n",
        "            bigram_prob = (bigram_counter[(sent[idx-1], sent[idx])] +alpha) / (unigram_counter[(sent[idx-1],)] + alpha*vocab_size)\n",
        "            trigram_prob = (trigram_counter[(sent[idx-2],sent[idx-1], sent[idx])] +alpha) / (bigram_counter[(sent[idx-2],sent[idx-1])] + alpha*vocab_size)\n",
        "            sum_prob += math.log2(lamda*trigram_prob + (1-lamda) * bigram_prob)\n",
        "            trigram_cnt+=1\n",
        "\n",
        "    CE = -sum_prob / trigram_cnt\n",
        "    perpl = math.pow(2,CE)\n",
        "    return CE, perpl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWkKOIrfQB7C"
      },
      "source": [
        "import random\n",
        "\n",
        "dummy_dev_sentances = []\n",
        "for sentance in dev_sentances:\n",
        "  dummy_sentance = [random.sample(tokens,1)[0] for word in sentance]\n",
        "  dummy_dev_sentances.append(dummy_sentance)\n",
        "\n",
        "dummy_test_sentances = []\n",
        "for sentance in test_sentances:\n",
        "  dummy_sentance = [random.sample(tokens,1)[0] for word in sentance]\n",
        "  dummy_test_sentances.append(dummy_sentance)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0R2JpGJ43O-",
        "outputId": "854aa79c-47b6-4b19-ba3c-b6c0cf1113c8"
      },
      "source": [
        "alpha = 0.01             #compare real vs dummy sentances (ii)\n",
        "\n",
        "print(\"Cross Entropy, Perplexity in bigrams\")\n",
        "print (\"dev sentances: \", biGramModel(bigram_counter, unigram_counter, dev_sentances, alpha, len(tokens)) )\n",
        "print (\"dummy sentances: \", biGramModel(bigram_counter, unigram_counter, dummy_dev_sentances, alpha, len(tokens)) )\n",
        "\n",
        "print(\"\\nCross Entropy, Perplexity in trigrams\")\n",
        "print (\"dev sentances: \", triGramModel(trigram_counter, bigram_counter, dev_sentances, alpha, len(tokens)) )\n",
        "print (\"dummy sentances: \", triGramModel(trigram_counter, bigram_counter, dummy_dev_sentances, alpha, len(tokens)) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Entropy, Perplexity in bigrams\n",
            "dev sentances:  (12.139227780585546, 4510.988149074712)\n",
            "dummy sentances:  (13.665288119333885, 12991.569569881716)\n",
            "\n",
            "Cross Entropy, Perplexity in trigrams\n",
            "dev sentances:  (15.956523655111827, 63590.50101597105)\n",
            "dummy sentances:  (17.154884977548086, 145926.7635063462)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7QwvHKYojU4",
        "outputId": "5de720c6-76a5-43f2-c74d-7ec5c6775d4d"
      },
      "source": [
        "        \n",
        "print(\"Cross Entropy, Perplexity\")       #results in the test sentances (iii)\n",
        "print (\"bigram model: \", biGramModel(bigram_counter, unigram_counter, test_sentances, alpha, len(tokens)) )\n",
        "print (\"trigram model: \", triGramModel(trigram_counter, bigram_counter, test_sentances, alpha, len(tokens)) )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross Entropy, Perplexity\n",
            "bigram model:  (12.126698479234742, 4471.981421325683)\n",
            "trigram model:  (15.940354884809375, 62881.79955247013)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymGpuwO9Ux3U",
        "outputId": "2324b429-9c22-41c3-9349-e362f51a7157"
      },
      "source": [
        "for lamda in [0.1,0.3,0.5,0.7,0.9]:            #combinations with interpolation  (iv)\n",
        "    print ( interpModel(trigram_counter, bigram_counter, unigram_counter, dev_sentances, alpha, len(tokens), lamda))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12.003485164312425, 4105.906798788918)\n",
            "(12.129550568523854, 4480.830904793424)\n",
            "(12.315400941372546, 5096.888018293171)\n",
            "(12.608172545829257, 6243.643918077654)\n",
            "(13.228342268113735, 9596.83030741689)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijgb0LloVAK0",
        "outputId": "15efd3ab-9f8b-4d7b-a274-0934fc4d3977"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}