{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_hw5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Christoforos00/NLP_project/blob/main/nlp_hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD4Xay0Hm9F8"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,  Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRj3YEpGrVSo",
        "outputId": "10c239c7-14bd-4c20-9289-66020a5141e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hos9XfTpuP_9",
        "outputId": "2481d63b-8f06-472f-facf-9c98d20efa55"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-16 15:22:04--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  10.8MB/s    in 6m 35s  \n",
            "\n",
            "2021-08-16 15:28:41 (10.9 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGTlueDumK9z",
        "outputId": "e1739903-fec8-439c-93d9-8b5171532296"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!gzip -d cc.en.300.vec.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-16 18:56:59--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 104.22.75.142, 172.67.9.4, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz’\n",
            "\n",
            "cc.en.300.vec.gz    100%[===================>]   1.23G  18.4MB/s    in 66s     \n",
            "\n",
            "2021-08-16 18:58:05 (19.2 MB/s) - ‘cc.en.300.vec.gz’ saved [1325960915/1325960915]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFR2FkDgYcR0"
      },
      "source": [
        "from gensim.models.wrappers import FastText\n",
        "\n",
        "fasttext = FastText.load_fasttext_format('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "a-3sLBhejgfu",
        "outputId": "a8570b67-00d1-4bc3-9b11-feb386de1c7a"
      },
      "source": [
        "i=0\n",
        "vocabulary = {}\n",
        "\n",
        "with open(\"cc.en.300.vec\", 'r', encoding=\"utf-8\", newline='\\n', errors='ignore') as f:\n",
        "    for l in f:\n",
        "        line = l.rstrip().split(' ')\n",
        "        print(i)\n",
        "        if i==0:\n",
        "            vocabulary_size = int(line[0])+2\n",
        "            dim = int(line[1])\n",
        "            vecs = np.zeros(vocabulary_size*dim).reshape(vocabulary_size,dim)\n",
        "            vocabulary[\"__PADDING__\"] = 0\n",
        "            vocabulary[\"__UNK__\"] = 1\n",
        "            i = 2\n",
        "        else:\n",
        "            vocabulary[line[0]] = i\n",
        "            embedding = np.array(line[1:]).astype(np.float)\n",
        "            if (embedding.shape[0]==dim):\n",
        "                vecs[i,:] = embedding\n",
        "                i+=1\n",
        "            else:\n",
        "                print(\"Aaaaaaaaaaaaaaaaa\")\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8e07ae9a7261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cc.en.300.vec\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cc.en.300.vec'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He4itYTyW_m3"
      },
      "source": [
        "\n",
        "# Serialize vocab & embeddings\n",
        "pickle.dump(vocabulary, open(\"/content/drive/My Drive/fasttext_voc.pkl\" ,'wb'))\n",
        "np.save(\"/content/drive/My Drive/fasttext.npy\", vecs)\n",
        "\n",
        "# Free ram\n",
        "vecs = None\n",
        "vocabulary = None\n",
        "embedding = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96CNevkqyjLm"
      },
      "source": [
        "df = pd.read_json('/content/drive/My Drive/News_Category_Dataset_v2.json', lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nYVaQIFSzCVr",
        "outputId": "0bf528a3-3737-4801-a7b8-b8e6d1c35d46"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200848</th>\n",
              "      <td>TECH</td>\n",
              "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
              "      <td>Reuters, Reuters</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
              "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200849</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
              "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200850</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
              "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200851</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
              "      <td>CORRECTION: An earlier version of this story i...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200852</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
              "      <td></td>\n",
              "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
              "      <td>The five-time all-star center tore into his te...</td>\n",
              "      <td>2012-01-28</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200853 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             category  ...       date\n",
              "0               CRIME  ... 2018-05-26\n",
              "1       ENTERTAINMENT  ... 2018-05-26\n",
              "2       ENTERTAINMENT  ... 2018-05-26\n",
              "3       ENTERTAINMENT  ... 2018-05-26\n",
              "4       ENTERTAINMENT  ... 2018-05-26\n",
              "...               ...  ...        ...\n",
              "200848           TECH  ... 2012-01-28\n",
              "200849         SPORTS  ... 2012-01-28\n",
              "200850         SPORTS  ... 2012-01-28\n",
              "200851         SPORTS  ... 2012-01-28\n",
              "200852         SPORTS  ... 2012-01-28\n",
              "\n",
              "[200853 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heZEqGvB6_48"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df[:100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3i_QoR8zeNg"
      },
      "source": [
        "df['text'] = df['headline'] + \" \" + df['short_description']\n",
        "texts = df['text'].tolist() \n",
        "labels = df['category'].tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDvcLmdE5jib"
      },
      "source": [
        "del df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVcr82UrjpQn"
      },
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm',disable=[\"tagger\", \"parser\",\"ner\"])\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer')) \n",
        "\n",
        "def tokenize_samples(samples):\n",
        "  \n",
        "    tokenized_samples = []\n",
        "    for i in range(len(samples)):  # For each sample\n",
        "        doc = nlp(samples[i])  # Tokenize the sample into sentences\n",
        "        tokens = []\n",
        "        for sent in doc.sents:  # For each sentence\n",
        "            for tok in sent:  # Iterate through each token \n",
        "                # Preprocessing: Filter stopwords\n",
        "                if '\\n' in tok.text or \"\\t\" in tok.text or \"--\" in tok.text or \"*\" in tok.text or tok.text.lower() in STOP_WORDS:\n",
        "                    continue\n",
        "                if tok.text.strip():  \n",
        "                    tokens.append(tok.text.replace('\"',\"'\").strip())\n",
        "        tokenized_samples.append(tokens)\n",
        "\n",
        "    return tokenized_samples\n",
        "\n",
        "texts_tokenized = tokenize_samples(texts)\n",
        "text_edited = texts_tokenized\n",
        "texts_tokenized = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbnZEtQym9GC"
      },
      "source": [
        "from sklearn.model_selection import train_test_split  \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_edited, labels, test_size=0.2, random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzvdqYaLVqpv"
      },
      "source": [
        "# Load/deserialize\n",
        "fasttext_embed = np.load(\"/content/drive/My Drive/fasttext.npy\")\n",
        "fasttext_word_to_index = pickle.load(open(\"/content/drive/My Drive/fasttext_voc.pkl\", 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxW6PFUh3ZPL"
      },
      "source": [
        "del text_edited\n",
        "del labels\n",
        "\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av4SQAO9CAwu"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "y_train = lb.fit_transform(y_train)\n",
        "# y_dev = lb.transform(y_dev)\n",
        "y_test = lb.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfonF1FShBcb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "MAX_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 250 \n",
        "EMBEDDING_DIM = fasttext_embed.shape[1]\n",
        "\n",
        "# Init tokenizer\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='__UNK__')\n",
        "# num_words: the maximum number of words to keep, based on word frequency.\n",
        "# oov_token: will be used to replace OOV WORDS\n",
        "\n",
        "# Fit tokenizer\n",
        "tokenizer.fit_on_texts([\" \".join(x) for x in X_train])\n",
        "\n",
        "# Converts text to sequences of IDs\n",
        "train_seqs = tokenizer.texts_to_sequences([\" \".join(x) for x in X_train])\n",
        "test_seqs = tokenizer.texts_to_sequences([\" \".join(x) for x in X_test])\n",
        "\n",
        "# Pads sequences to a fixed value\n",
        "X_train = pad_sequences(sequences=train_seqs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "X_test = pad_sequences(sequences=test_seqs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHfMy9UW3gsM"
      },
      "source": [
        "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.25, random_state=101)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us-xmi81l9Qu",
        "outputId": "cb8f3bc0-a8f7-4cec-e17f-0a09f2858cf0"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens.'.format(len(word_index)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 59449 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oowuMDcOmBce"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQVm2YZomDte",
        "outputId": "15989f87-b9cf-4ec6-82da-b2b789bf21ec"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens.'.format(len(word_index)))\n",
        "\n",
        "embedding_matrix = np.zeros((MAX_WORDS+2, EMBEDDING_DIM))  # +2 (pad, unknown)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_WORDS:\n",
        "            continue\n",
        "    try:\n",
        "        embedding_vector = fasttext_embed[fasttext_word_to_index[word],:]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 59449 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG-cust7BPnQ"
      },
      "source": [
        "y_train_nonbinary = lb.inverse_transform(y_train)\n",
        "y_dev_nonbinary = lb.inverse_transform(y_dev)\n",
        "y_test_nonbinary = lb.inverse_transform(y_test )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLIcOri4-m51"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGp_EO7Am9GF",
        "outputId": "7f345234-fa08-44fa-a60a-ee654d81d5a6"
      },
      "source": [
        "print(\"Results of the majority classifier\")\n",
        "\n",
        "baseline = DummyClassifier(strategy='most_frequent')\n",
        "baseline.fit(X_train, y_train_nonbinary)\n",
        "\n",
        "print(\"Classification report on the training data:\")\n",
        "predictions_train = baseline.predict(X_train)\n",
        "print(classification_report(y_train_nonbinary, predictions_train))\n",
        "\n",
        "print(\"Classification report on the development data:\")\n",
        "predictions_dev = baseline.predict(X_dev)\n",
        "print(classification_report(y_dev_nonbinary, predictions_dev))\n",
        "\n",
        "print(\"Classification report on the test data:\")\n",
        "predictions_test = baseline.predict(X_test)\n",
        "print(classification_report(y_test_nonbinary, predictions_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of the majority classifier\n",
            "Classification report on the training data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.00      0.00      0.00       479\n",
            "ARTS & CULTURE       0.00      0.00      0.00       387\n",
            "  BLACK VOICES       0.00      0.00      0.00      1419\n",
            "      BUSINESS       0.00      0.00      0.00      1791\n",
            "       COLLEGE       0.00      0.00      0.00       344\n",
            "        COMEDY       0.00      0.00      0.00      1488\n",
            "         CRIME       0.00      0.00      0.00      1054\n",
            "CULTURE & ARTS       0.00      0.00      0.00       297\n",
            "       DIVORCE       0.00      0.00      0.00      1025\n",
            "     EDUCATION       0.00      0.00      0.00       285\n",
            " ENTERTAINMENT       0.00      0.00      0.00      4864\n",
            "   ENVIRONMENT       0.00      0.00      0.00       408\n",
            "         FIFTY       0.00      0.00      0.00       420\n",
            "  FOOD & DRINK       0.00      0.00      0.00      1827\n",
            "     GOOD NEWS       0.00      0.00      0.00       381\n",
            "         GREEN       0.00      0.00      0.00       767\n",
            "HEALTHY LIVING       0.00      0.00      0.00      1972\n",
            " HOME & LIVING       0.00      0.00      0.00      1253\n",
            "        IMPACT       0.00      0.00      0.00      1010\n",
            " LATINO VOICES       0.00      0.00      0.00       336\n",
            "         MEDIA       0.00      0.00      0.00       845\n",
            "         MONEY       0.00      0.00      0.00       492\n",
            "     PARENTING       0.00      0.00      0.00      2605\n",
            "       PARENTS       0.00      0.00      0.00      1186\n",
            "      POLITICS       0.16      1.00      0.28      9743\n",
            "  QUEER VOICES       0.00      0.00      0.00      1899\n",
            "      RELIGION       0.00      0.00      0.00       792\n",
            "       SCIENCE       0.00      0.00      0.00       649\n",
            "        SPORTS       0.00      0.00      0.00      1457\n",
            "         STYLE       0.00      0.00      0.00       680\n",
            "STYLE & BEAUTY       0.00      0.00      0.00      2887\n",
            "         TASTE       0.00      0.00      0.00       622\n",
            "          TECH       0.00      0.00      0.00       642\n",
            " THE WORLDPOST       0.00      0.00      0.00      1095\n",
            "        TRAVEL       0.00      0.00      0.00      2932\n",
            "      WEDDINGS       0.00      0.00      0.00      1070\n",
            "    WEIRD NEWS       0.00      0.00      0.00       830\n",
            "      WELLNESS       0.00      0.00      0.00      5342\n",
            "         WOMEN       0.00      0.00      0.00      1006\n",
            "    WORLD NEWS       0.00      0.00      0.00       661\n",
            "     WORLDPOST       0.00      0.00      0.00       758\n",
            "\n",
            "      accuracy                           0.16     60000\n",
            "     macro avg       0.00      0.02      0.01     60000\n",
            "  weighted avg       0.03      0.16      0.05     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.00      0.00      0.00       167\n",
            "ARTS & CULTURE       0.00      0.00      0.00       133\n",
            "  BLACK VOICES       0.00      0.00      0.00       445\n",
            "      BUSINESS       0.00      0.00      0.00       542\n",
            "       COLLEGE       0.00      0.00      0.00       102\n",
            "        COMEDY       0.00      0.00      0.00       565\n",
            "         CRIME       0.00      0.00      0.00       319\n",
            "CULTURE & ARTS       0.00      0.00      0.00       112\n",
            "       DIVORCE       0.00      0.00      0.00       318\n",
            "     EDUCATION       0.00      0.00      0.00       102\n",
            " ENTERTAINMENT       0.00      0.00      0.00      1606\n",
            "   ENVIRONMENT       0.00      0.00      0.00       107\n",
            "         FIFTY       0.00      0.00      0.00       119\n",
            "  FOOD & DRINK       0.00      0.00      0.00       641\n",
            "     GOOD NEWS       0.00      0.00      0.00       136\n",
            "         GREEN       0.00      0.00      0.00       235\n",
            "HEALTHY LIVING       0.00      0.00      0.00       691\n",
            " HOME & LIVING       0.00      0.00      0.00       441\n",
            "        IMPACT       0.00      0.00      0.00       337\n",
            " LATINO VOICES       0.00      0.00      0.00       105\n",
            "         MEDIA       0.00      0.00      0.00       304\n",
            "         MONEY       0.00      0.00      0.00       152\n",
            "     PARENTING       0.00      0.00      0.00       843\n",
            "       PARENTS       0.00      0.00      0.00       406\n",
            "      POLITICS       0.16      1.00      0.28      3288\n",
            "  QUEER VOICES       0.00      0.00      0.00       605\n",
            "      RELIGION       0.00      0.00      0.00       254\n",
            "       SCIENCE       0.00      0.00      0.00       229\n",
            "        SPORTS       0.00      0.00      0.00       503\n",
            "         STYLE       0.00      0.00      0.00       209\n",
            "STYLE & BEAUTY       0.00      0.00      0.00       933\n",
            "         TASTE       0.00      0.00      0.00       247\n",
            "          TECH       0.00      0.00      0.00       237\n",
            " THE WORLDPOST       0.00      0.00      0.00       394\n",
            "        TRAVEL       0.00      0.00      0.00       998\n",
            "      WEDDINGS       0.00      0.00      0.00       340\n",
            "    WEIRD NEWS       0.00      0.00      0.00       247\n",
            "      WELLNESS       0.00      0.00      0.00      1758\n",
            "         WOMEN       0.00      0.00      0.00       337\n",
            "    WORLD NEWS       0.00      0.00      0.00       216\n",
            "     WORLDPOST       0.00      0.00      0.00       277\n",
            "\n",
            "      accuracy                           0.16     20000\n",
            "     macro avg       0.00      0.02      0.01     20000\n",
            "  weighted avg       0.03      0.16      0.05     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.00      0.00      0.00       145\n",
            "ARTS & CULTURE       0.00      0.00      0.00       145\n",
            "  BLACK VOICES       0.00      0.00      0.00       448\n",
            "      BUSINESS       0.00      0.00      0.00       625\n",
            "       COLLEGE       0.00      0.00      0.00       112\n",
            "        COMEDY       0.00      0.00      0.00       508\n",
            "         CRIME       0.00      0.00      0.00       306\n",
            "CULTURE & ARTS       0.00      0.00      0.00        99\n",
            "       DIVORCE       0.00      0.00      0.00       347\n",
            "     EDUCATION       0.00      0.00      0.00       111\n",
            " ENTERTAINMENT       0.00      0.00      0.00      1647\n",
            "   ENVIRONMENT       0.00      0.00      0.00       139\n",
            "         FIFTY       0.00      0.00      0.00       155\n",
            "  FOOD & DRINK       0.00      0.00      0.00       592\n",
            "     GOOD NEWS       0.00      0.00      0.00       136\n",
            "         GREEN       0.00      0.00      0.00       266\n",
            "HEALTHY LIVING       0.00      0.00      0.00       665\n",
            " HOME & LIVING       0.00      0.00      0.00       411\n",
            "        IMPACT       0.00      0.00      0.00       369\n",
            " LATINO VOICES       0.00      0.00      0.00       105\n",
            "         MEDIA       0.00      0.00      0.00       254\n",
            "         MONEY       0.00      0.00      0.00       161\n",
            "     PARENTING       0.00      0.00      0.00       911\n",
            "       PARENTS       0.00      0.00      0.00       448\n",
            "      POLITICS       0.16      1.00      0.28      3258\n",
            "  QUEER VOICES       0.00      0.00      0.00       610\n",
            "      RELIGION       0.00      0.00      0.00       258\n",
            "       SCIENCE       0.00      0.00      0.00       210\n",
            "        SPORTS       0.00      0.00      0.00       482\n",
            "         STYLE       0.00      0.00      0.00       208\n",
            "STYLE & BEAUTY       0.00      0.00      0.00       914\n",
            "         TASTE       0.00      0.00      0.00       205\n",
            "          TECH       0.00      0.00      0.00       200\n",
            " THE WORLDPOST       0.00      0.00      0.00       338\n",
            "        TRAVEL       0.00      0.00      0.00      1004\n",
            "      WEDDINGS       0.00      0.00      0.00       369\n",
            "    WEIRD NEWS       0.00      0.00      0.00       242\n",
            "      WELLNESS       0.00      0.00      0.00      1741\n",
            "         WOMEN       0.00      0.00      0.00       393\n",
            "    WORLD NEWS       0.00      0.00      0.00       213\n",
            "     WORLDPOST       0.00      0.00      0.00       250\n",
            "\n",
            "      accuracy                           0.16     20000\n",
            "     macro avg       0.00      0.02      0.01     20000\n",
            "  weighted avg       0.03      0.16      0.05     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmkcV16zm9GF",
        "outputId": "5bae1415-54c7-456a-e77d-c2786815459c"
      },
      "source": [
        "print(\"Results of logistic regression\")\n",
        "\n",
        "logReg = LogisticRegression(solver=\"liblinear\", C = 10)\n",
        "logReg.fit(X_train, y_train_nonbinary)\n",
        "\n",
        "print(\"Classification report on the training data:\")\n",
        "predictions_train = logReg.predict(X_train)\n",
        "print(classification_report(y_train_nonbinary, predictions_train))\n",
        "\n",
        "print(\"Classification report on the development data:\")\n",
        "predictions_dev = logReg.predict(X_dev)\n",
        "print(classification_report(y_dev_nonbinary, predictions_dev))\n",
        "\n",
        "print(\"Classification report on the test data:\")\n",
        "predictions_test = logReg.predict(X_test)\n",
        "print(classification_report(y_test_nonbinary, predictions_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results of logistic regression\n",
            "Classification report on the training data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.47      0.18      0.26       457\n",
            "ARTS & CULTURE       0.46      0.16      0.23       411\n",
            "  BLACK VOICES       0.49      0.27      0.35      1375\n",
            "      BUSINESS       0.47      0.42      0.45      1756\n",
            "       COLLEGE       0.49      0.33      0.39       368\n",
            "        COMEDY       0.53      0.30      0.38      1576\n",
            "         CRIME       0.56      0.55      0.55      1010\n",
            "CULTURE & ARTS       0.63      0.24      0.35       331\n",
            "       DIVORCE       0.78      0.64      0.70      1003\n",
            "     EDUCATION       0.53      0.33      0.41       302\n",
            " ENTERTAINMENT       0.46      0.70      0.55      4868\n",
            "   ENVIRONMENT       0.72      0.18      0.28       410\n",
            "         FIFTY       0.39      0.07      0.11       399\n",
            "  FOOD & DRINK       0.59      0.71      0.64      1916\n",
            "     GOOD NEWS       0.49      0.18      0.26       439\n",
            "         GREEN       0.45      0.31      0.37       796\n",
            "HEALTHY LIVING       0.36      0.14      0.20      1969\n",
            " HOME & LIVING       0.67      0.65      0.66      1277\n",
            "        IMPACT       0.41      0.20      0.27      1001\n",
            " LATINO VOICES       0.62      0.05      0.10       346\n",
            "         MEDIA       0.51      0.27      0.35       829\n",
            "         MONEY       0.54      0.29      0.38       502\n",
            "     PARENTING       0.50      0.62      0.55      2547\n",
            "       PARENTS       0.50      0.21      0.30      1155\n",
            "      POLITICS       0.62      0.85      0.71      9772\n",
            "  QUEER VOICES       0.71      0.61      0.66      1903\n",
            "      RELIGION       0.61      0.35      0.44       775\n",
            "       SCIENCE       0.58      0.36      0.45       650\n",
            "        SPORTS       0.58      0.52      0.55      1466\n",
            "         STYLE       0.59      0.19      0.29       675\n",
            "STYLE & BEAUTY       0.68      0.77      0.72      2821\n",
            "         TASTE       0.58      0.10      0.17       609\n",
            "          TECH       0.56      0.39      0.46       624\n",
            " THE WORLDPOST       0.50      0.41      0.45      1081\n",
            "        TRAVEL       0.60      0.74      0.66      2942\n",
            "      WEDDINGS       0.79      0.75      0.77      1059\n",
            "    WEIRD NEWS       0.42      0.18      0.26       789\n",
            "      WELLNESS       0.50      0.79      0.61      5377\n",
            "         WOMEN       0.41      0.28      0.33      1008\n",
            "    WORLD NEWS       0.42      0.07      0.12       615\n",
            "     WORLDPOST       0.46      0.19      0.27       791\n",
            "\n",
            "      accuracy                           0.56     60000\n",
            "     macro avg       0.54      0.38      0.42     60000\n",
            "  weighted avg       0.55      0.56      0.53     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.29      0.09      0.14       147\n",
            "ARTS & CULTURE       0.31      0.11      0.16       132\n",
            "  BLACK VOICES       0.53      0.26      0.34       477\n",
            "      BUSINESS       0.44      0.41      0.42       566\n",
            "       COLLEGE       0.33      0.25      0.29       108\n",
            "        COMEDY       0.45      0.24      0.31       502\n",
            "         CRIME       0.53      0.48      0.50       369\n",
            "CULTURE & ARTS       0.35      0.15      0.21       100\n",
            "       DIVORCE       0.76      0.61      0.67       340\n",
            "     EDUCATION       0.30      0.21      0.24        87\n",
            " ENTERTAINMENT       0.46      0.70      0.56      1620\n",
            "   ENVIRONMENT       0.65      0.19      0.29       139\n",
            "         FIFTY       0.14      0.02      0.03       116\n",
            "  FOOD & DRINK       0.55      0.67      0.61       597\n",
            "     GOOD NEWS       0.26      0.09      0.13       152\n",
            "         GREEN       0.39      0.25      0.31       271\n",
            "HEALTHY LIVING       0.29      0.11      0.16       693\n",
            " HOME & LIVING       0.61      0.59      0.60       406\n",
            "        IMPACT       0.35      0.14      0.20       340\n",
            " LATINO VOICES       0.50      0.03      0.06       101\n",
            "         MEDIA       0.46      0.27      0.34       263\n",
            "         MONEY       0.49      0.23      0.32       162\n",
            "     PARENTING       0.48      0.55      0.51       901\n",
            "       PARENTS       0.34      0.16      0.22       380\n",
            "      POLITICS       0.60      0.84      0.70      3264\n",
            "  QUEER VOICES       0.66      0.59      0.62       629\n",
            "      RELIGION       0.53      0.32      0.40       247\n",
            "       SCIENCE       0.48      0.26      0.34       213\n",
            "        SPORTS       0.58      0.51      0.54       490\n",
            "         STYLE       0.48      0.13      0.20       238\n",
            "STYLE & BEAUTY       0.67      0.76      0.71       943\n",
            "         TASTE       0.28      0.04      0.07       210\n",
            "          TECH       0.51      0.29      0.37       224\n",
            " THE WORLDPOST       0.44      0.35      0.39       387\n",
            "        TRAVEL       0.59      0.72      0.65       985\n",
            "      WEDDINGS       0.73      0.71      0.72       350\n",
            "    WEIRD NEWS       0.33      0.13      0.19       262\n",
            "      WELLNESS       0.47      0.77      0.58      1778\n",
            "         WOMEN       0.35      0.24      0.29       341\n",
            "    WORLD NEWS       0.49      0.08      0.13       236\n",
            "     WORLDPOST       0.30      0.14      0.19       234\n",
            "\n",
            "      accuracy                           0.53     20000\n",
            "     macro avg       0.46      0.33      0.36     20000\n",
            "  weighted avg       0.51      0.53      0.49     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.30      0.09      0.14       169\n",
            "ARTS & CULTURE       0.35      0.10      0.16       138\n",
            "  BLACK VOICES       0.43      0.24      0.30       431\n",
            "      BUSINESS       0.43      0.35      0.39       610\n",
            "       COLLEGE       0.46      0.33      0.38       103\n",
            "        COMEDY       0.44      0.26      0.33       476\n",
            "         CRIME       0.49      0.46      0.47       312\n",
            "CULTURE & ARTS       0.50      0.14      0.22       107\n",
            "       DIVORCE       0.78      0.63      0.70       360\n",
            "     EDUCATION       0.45      0.31      0.37        96\n",
            " ENTERTAINMENT       0.41      0.67      0.51      1601\n",
            "   ENVIRONMENT       0.58      0.14      0.23       152\n",
            "         FIFTY       0.30      0.04      0.08       137\n",
            "  FOOD & DRINK       0.52      0.69      0.59       576\n",
            "     GOOD NEWS       0.24      0.06      0.10       127\n",
            "         GREEN       0.37      0.22      0.27       291\n",
            "HEALTHY LIVING       0.26      0.09      0.13       639\n",
            " HOME & LIVING       0.63      0.60      0.61       424\n",
            "        IMPACT       0.41      0.18      0.25       354\n",
            " LATINO VOICES       0.50      0.03      0.06       100\n",
            "         MEDIA       0.52      0.28      0.36       278\n",
            "         MONEY       0.51      0.24      0.32       159\n",
            "     PARENTING       0.46      0.56      0.50       837\n",
            "       PARENTS       0.35      0.17      0.23       376\n",
            "      POLITICS       0.59      0.84      0.69      3168\n",
            "  QUEER VOICES       0.66      0.57      0.61       620\n",
            "      RELIGION       0.56      0.31      0.40       261\n",
            "       SCIENCE       0.51      0.27      0.36       217\n",
            "        SPORTS       0.55      0.48      0.51       505\n",
            "         STYLE       0.51      0.14      0.22       251\n",
            "STYLE & BEAUTY       0.67      0.74      0.70       972\n",
            "         TASTE       0.25      0.02      0.04       250\n",
            "          TECH       0.46      0.29      0.35       203\n",
            " THE WORLDPOST       0.43      0.31      0.36       391\n",
            "        TRAVEL       0.57      0.71      0.64      1030\n",
            "      WEDDINGS       0.75      0.66      0.70       361\n",
            "    WEIRD NEWS       0.32      0.13      0.18       286\n",
            "      WELLNESS       0.48      0.78      0.59      1752\n",
            "         WOMEN       0.39      0.28      0.33       361\n",
            "    WORLD NEWS       0.35      0.05      0.09       240\n",
            "     WORLDPOST       0.40      0.15      0.22       279\n",
            "\n",
            "      accuracy                           0.52     20000\n",
            "     macro avg       0.47      0.33      0.36     20000\n",
            "  weighted avg       0.50      0.52      0.48     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbI3C2_HSJdZ"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxxuV20Pm9GG"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "class Metrics(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, valid_data):\n",
        "        super(Metrics, self).__init__()\n",
        "        self.validation_data = valid_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)\n",
        "        val_targ = self.validation_data[1]\n",
        "        \n",
        "        if len(val_targ.shape) == 2 and val_targ.shape[1] != 1:\n",
        "            val_targ = np.argmax(val_targ, -1)\n",
        "        val_targ = tf.cast(val_targ,dtype=tf.float32)\n",
        "        \n",
        "        _val_f1 = f1_score(val_targ, val_predict,average=\"weighted\")\n",
        "        _val_recall = recall_score(val_targ, val_predict,average=\"weighted\")\n",
        "        _val_precision = precision_score(val_targ, val_predict,average=\"weighted\")\n",
        "\n",
        "        logs['val_f1'] = _val_f1\n",
        "        logs['val_recall'] = _val_recall\n",
        "        logs['val_precision'] = _val_precision\n",
        "        print(\" — val_f1: %f — val_precision: %f — val_recall: %f\" % (_val_f1, _val_precision, _val_recall))\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQjHir8Am9GG",
        "outputId": "37b8137d-51c8-4bd7-83e6-34820c44b33e"
      },
      "source": [
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "\n",
        "with tf.device('/device:GPU:0'):  \n",
        "    model = Sequential()\n",
        "    model.add(Dense(448, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=[\"categorical_crossentropy\"]\n",
        "    )\n",
        "    \n",
        "    if not os.path.exists('./checkpoints'):\n",
        "        os.makedirs('./checkpoints')\n",
        "        \n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'checkpoints/weights.hdf5',\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        verbose=2,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "    \n",
        "    history= model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data= (X_dev, y_dev),\n",
        "        batch_size=256,\n",
        "        epochs=30,\n",
        "        shuffle=True,\n",
        "        callbacks=[Metrics(valid_data=(X_dev, y_dev)), checkpoint]\n",
        "    )\n",
        "\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int)    \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_19 (Dense)             (None, 448)               224448    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 448)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 320)               143680    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 320)               102720    \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 320)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 41)                13161     \n",
            "=================================================================\n",
            "Total params: 484,009\n",
            "Trainable params: 484,009\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "235/235 [==============================] - 7s 26ms/step - loss: 2.8981 - categorical_crossentropy: 2.8981 - val_loss: 2.2267 - val_categorical_crossentropy: 2.2267\n",
            " — val_f1: 0.316848 — val_precision: 0.301038 — val_recall: 0.432450\n",
            "\n",
            "Epoch 00001: categorical_crossentropy improved from -inf to 2.89812, saving model to checkpoints/weights.hdf5\n",
            "Epoch 2/30\n",
            "  7/235 [..............................] - ETA: 5s - loss: 2.3960 - categorical_crossentropy: 2.3960"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "235/235 [==============================] - 6s 26ms/step - loss: 2.2261 - categorical_crossentropy: 2.2261 - val_loss: 1.9780 - val_categorical_crossentropy: 1.9780\n",
            " — val_f1: 0.403841 — val_precision: 0.390308 — val_recall: 0.487300\n",
            "\n",
            "Epoch 00002: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 3/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 2.0492 - categorical_crossentropy: 2.0492 - val_loss: 1.8868 - val_categorical_crossentropy: 1.8868\n",
            " — val_f1: 0.432061 — val_precision: 0.464064 — val_recall: 0.503450\n",
            "\n",
            "Epoch 00003: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 4/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.9530 - categorical_crossentropy: 1.9530 - val_loss: 1.8345 - val_categorical_crossentropy: 1.8345\n",
            " — val_f1: 0.444264 — val_precision: 0.446962 — val_recall: 0.511900\n",
            "\n",
            "Epoch 00004: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 5/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.8857 - categorical_crossentropy: 1.8857 - val_loss: 1.8020 - val_categorical_crossentropy: 1.8020\n",
            " — val_f1: 0.461226 — val_precision: 0.518611 — val_recall: 0.521350\n",
            "\n",
            "Epoch 00005: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 6/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.8321 - categorical_crossentropy: 1.8321 - val_loss: 1.7811 - val_categorical_crossentropy: 1.7811\n",
            " — val_f1: 0.471610 — val_precision: 0.497578 — val_recall: 0.525350\n",
            "\n",
            "Epoch 00006: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 7/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.7843 - categorical_crossentropy: 1.7843 - val_loss: 1.7695 - val_categorical_crossentropy: 1.7695\n",
            " — val_f1: 0.474848 — val_precision: 0.495663 — val_recall: 0.528350\n",
            "\n",
            "Epoch 00007: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 8/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.7445 - categorical_crossentropy: 1.7445 - val_loss: 1.7589 - val_categorical_crossentropy: 1.7589\n",
            " — val_f1: 0.480776 — val_precision: 0.496418 — val_recall: 0.530150\n",
            "\n",
            "Epoch 00008: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 9/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.7092 - categorical_crossentropy: 1.7092 - val_loss: 1.7579 - val_categorical_crossentropy: 1.7579\n",
            " — val_f1: 0.481451 — val_precision: 0.492042 — val_recall: 0.528050\n",
            "\n",
            "Epoch 00009: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 10/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.6774 - categorical_crossentropy: 1.6774 - val_loss: 1.7495 - val_categorical_crossentropy: 1.7495\n",
            " — val_f1: 0.486362 — val_precision: 0.499937 — val_recall: 0.534250\n",
            "\n",
            "Epoch 00010: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 11/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.6436 - categorical_crossentropy: 1.6436 - val_loss: 1.7473 - val_categorical_crossentropy: 1.7473\n",
            " — val_f1: 0.489114 — val_precision: 0.505246 — val_recall: 0.534750\n",
            "\n",
            "Epoch 00011: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 12/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.6126 - categorical_crossentropy: 1.6126 - val_loss: 1.7444 - val_categorical_crossentropy: 1.7444\n",
            " — val_f1: 0.494281 — val_precision: 0.500170 — val_recall: 0.535550\n",
            "\n",
            "Epoch 00012: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 13/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.5835 - categorical_crossentropy: 1.5835 - val_loss: 1.7413 - val_categorical_crossentropy: 1.7413\n",
            " — val_f1: 0.497995 — val_precision: 0.506703 — val_recall: 0.537250\n",
            "\n",
            "Epoch 00013: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 14/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.5566 - categorical_crossentropy: 1.5566 - val_loss: 1.7472 - val_categorical_crossentropy: 1.7472\n",
            " — val_f1: 0.494221 — val_precision: 0.504813 — val_recall: 0.535500\n",
            "\n",
            "Epoch 00014: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 15/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.5280 - categorical_crossentropy: 1.5280 - val_loss: 1.7518 - val_categorical_crossentropy: 1.7518\n",
            " — val_f1: 0.495234 — val_precision: 0.499405 — val_recall: 0.534850\n",
            "\n",
            "Epoch 00015: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 16/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.5034 - categorical_crossentropy: 1.5034 - val_loss: 1.7610 - val_categorical_crossentropy: 1.7610\n",
            " — val_f1: 0.501537 — val_precision: 0.503001 — val_recall: 0.535850\n",
            "\n",
            "Epoch 00016: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 17/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4830 - categorical_crossentropy: 1.4830 - val_loss: 1.7612 - val_categorical_crossentropy: 1.7612\n",
            " — val_f1: 0.497916 — val_precision: 0.500654 — val_recall: 0.535750\n",
            "\n",
            "Epoch 00017: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 18/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4593 - categorical_crossentropy: 1.4593 - val_loss: 1.7627 - val_categorical_crossentropy: 1.7627\n",
            " — val_f1: 0.501351 — val_precision: 0.501631 — val_recall: 0.535650\n",
            "\n",
            "Epoch 00018: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 19/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4410 - categorical_crossentropy: 1.4410 - val_loss: 1.7811 - val_categorical_crossentropy: 1.7811\n",
            " — val_f1: 0.498499 — val_precision: 0.498208 — val_recall: 0.533450\n",
            "\n",
            "Epoch 00019: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 20/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4250 - categorical_crossentropy: 1.4250 - val_loss: 1.7762 - val_categorical_crossentropy: 1.7762\n",
            " — val_f1: 0.500474 — val_precision: 0.506055 — val_recall: 0.533300\n",
            "\n",
            "Epoch 00020: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 21/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.4049 - categorical_crossentropy: 1.4049 - val_loss: 1.7770 - val_categorical_crossentropy: 1.7770\n",
            " — val_f1: 0.503085 — val_precision: 0.501992 — val_recall: 0.535500\n",
            "\n",
            "Epoch 00021: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 22/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.3919 - categorical_crossentropy: 1.3919 - val_loss: 1.7873 - val_categorical_crossentropy: 1.7873\n",
            " — val_f1: 0.501999 — val_precision: 0.499180 — val_recall: 0.535550\n",
            "\n",
            "Epoch 00022: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 23/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3686 - categorical_crossentropy: 1.3686 - val_loss: 1.7889 - val_categorical_crossentropy: 1.7889\n",
            " — val_f1: 0.502478 — val_precision: 0.504906 — val_recall: 0.536000\n",
            "\n",
            "Epoch 00023: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 24/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.3575 - categorical_crossentropy: 1.3575 - val_loss: 1.7978 - val_categorical_crossentropy: 1.7978\n",
            " — val_f1: 0.503681 — val_precision: 0.505884 — val_recall: 0.536150\n",
            "\n",
            "Epoch 00024: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 25/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3495 - categorical_crossentropy: 1.3495 - val_loss: 1.7949 - val_categorical_crossentropy: 1.7949\n",
            " — val_f1: 0.504575 — val_precision: 0.502363 — val_recall: 0.536150\n",
            "\n",
            "Epoch 00025: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 26/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3350 - categorical_crossentropy: 1.3350 - val_loss: 1.8055 - val_categorical_crossentropy: 1.8055\n",
            " — val_f1: 0.506619 — val_precision: 0.514358 — val_recall: 0.536500\n",
            "\n",
            "Epoch 00026: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 27/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3179 - categorical_crossentropy: 1.3179 - val_loss: 1.8161 - val_categorical_crossentropy: 1.8161\n",
            " — val_f1: 0.504297 — val_precision: 0.502617 — val_recall: 0.533850\n",
            "\n",
            "Epoch 00027: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 28/30\n",
            "235/235 [==============================] - 6s 26ms/step - loss: 1.3073 - categorical_crossentropy: 1.3073 - val_loss: 1.8247 - val_categorical_crossentropy: 1.8247\n",
            " — val_f1: 0.503157 — val_precision: 0.501831 — val_recall: 0.533350\n",
            "\n",
            "Epoch 00028: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 29/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.2985 - categorical_crossentropy: 1.2985 - val_loss: 1.8359 - val_categorical_crossentropy: 1.8359\n",
            " — val_f1: 0.505831 — val_precision: 0.504994 — val_recall: 0.535350\n",
            "\n",
            "Epoch 00029: categorical_crossentropy did not improve from 2.89812\n",
            "Epoch 30/30\n",
            "235/235 [==============================] - 6s 25ms/step - loss: 1.2834 - categorical_crossentropy: 1.2834 - val_loss: 1.8353 - val_categorical_crossentropy: 1.8353\n",
            " — val_f1: 0.506289 — val_precision: 0.503671 — val_recall: 0.534400\n",
            "\n",
            "Epoch 00030: categorical_crossentropy did not improve from 2.89812\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.75      0.02      0.04       147\n",
            "ARTS & CULTURE       0.50      0.03      0.06       132\n",
            "  BLACK VOICES       0.72      0.17      0.27       477\n",
            "      BUSINESS       0.59      0.28      0.38       566\n",
            "       COLLEGE       0.45      0.16      0.23       108\n",
            "        COMEDY       0.64      0.20      0.31       502\n",
            "         CRIME       0.58      0.35      0.44       369\n",
            "CULTURE & ARTS       0.45      0.05      0.09       100\n",
            "       DIVORCE       0.83      0.57      0.68       340\n",
            "     EDUCATION       0.41      0.14      0.21        87\n",
            " ENTERTAINMENT       0.67      0.50      0.57      1620\n",
            "   ENVIRONMENT       0.69      0.14      0.24       139\n",
            "         FIFTY       0.00      0.00      0.00       116\n",
            "  FOOD & DRINK       0.63      0.60      0.61       597\n",
            "     GOOD NEWS       0.00      0.00      0.00       152\n",
            "         GREEN       0.45      0.10      0.17       271\n",
            "HEALTHY LIVING       0.56      0.07      0.13       693\n",
            " HOME & LIVING       0.69      0.56      0.62       406\n",
            "        IMPACT       0.57      0.06      0.11       340\n",
            " LATINO VOICES       0.00      0.00      0.00       101\n",
            "         MEDIA       0.54      0.19      0.28       263\n",
            "         MONEY       0.56      0.21      0.30       162\n",
            "     PARENTING       0.56      0.50      0.53       901\n",
            "       PARENTS       0.52      0.09      0.15       380\n",
            "      POLITICS       0.75      0.73      0.74      3264\n",
            "  QUEER VOICES       0.77      0.55      0.64       629\n",
            "      RELIGION       0.64      0.24      0.35       247\n",
            "       SCIENCE       0.54      0.18      0.27       213\n",
            "        SPORTS       0.60      0.47      0.52       490\n",
            "         STYLE       0.63      0.17      0.27       238\n",
            "STYLE & BEAUTY       0.79      0.72      0.75       943\n",
            "         TASTE       0.00      0.00      0.00       210\n",
            "          TECH       0.63      0.25      0.36       224\n",
            " THE WORLDPOST       0.52      0.25      0.34       387\n",
            "        TRAVEL       0.73      0.64      0.68       985\n",
            "      WEDDINGS       0.75      0.69      0.72       350\n",
            "    WEIRD NEWS       0.59      0.04      0.07       262\n",
            "      WELLNESS       0.62      0.61      0.62      1778\n",
            "         WOMEN       0.43      0.13      0.20       341\n",
            "    WORLD NEWS       0.00      0.00      0.00       236\n",
            "     WORLDPOST       0.17      0.00      0.01       234\n",
            "\n",
            "     micro avg       0.68      0.44      0.53     20000\n",
            "     macro avg       0.52      0.26      0.32     20000\n",
            "  weighted avg       0.62      0.44      0.48     20000\n",
            "   samples avg       0.44      0.44      0.44     20000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "NdzCa2RpbCt7",
        "outputId": "0c50fe0f-aaac-4671-a0f0-82071ed987b5"
      },
      "source": [
        "import warnings\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(448, input_dim=X_train.shape[1], activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(320,activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    # Load weights from the pre-trained model\n",
        "    model.load_weights(\"checkpoints/weights.hdf5\")\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy'])\n",
        "    )\n",
        "\n",
        "    print(\"Classification report on the training data:\")\n",
        "    predictions_train = model.predict(X_train)\n",
        "    predictions_train = (predictions_train > 0.5).astype(int)   \n",
        "    print(classification_report(y_train, predictions_train, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the development data:\")\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int) \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the test data:\")\n",
        "    predictions_test = model.predict(X_test)\n",
        "    predictions_test = (predictions_test > 0.5).astype(int)    \n",
        "    print(classification_report(y_test, predictions_test, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-e012c688c25e>\"\u001b[0;36m, line \u001b[0;32m25\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB3DZqOEd7Kp"
      },
      "source": [
        "class DeepSelfAttention(layers.Layer):\n",
        "\n",
        "    def __init__(self, input_dim, dense_dim,**kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.dense1 = Dense(dense_dim)\n",
        "        self.dense2 = Dense(1)\n",
        "\n",
        "    def call(self, input):\n",
        "        output = self.dense1(input)\n",
        "        output = self.dense2(output)\n",
        "        output = tf.nn.softmax(output, axis=-1)\n",
        "        return tf.math.reduce_sum( tf.math.multiply(input,output) , axis=1 )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"input_dim\" : self.input_dim,\n",
        "            \"dense_dim\" : self.dense_dim\n",
        "        })\n",
        "        return config    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTJXB2A6xchb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b01547-7677-4914-d44d-210e82b28e03"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Bidirectional, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    GRU_SIZE = 64\n",
        "    DENSE = 32\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "                    ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(DeepSelfAttention(MAX_SEQUENCE_LENGTH, 100))\n",
        "    model.add(Dropout(0.33))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    if not os.path.exists('/content/gdrive/My Drive/checkpoints'):\n",
        "        os.makedirs('/content/gdrive/My Drive/checkpoints')\n",
        "\n",
        "    checkpoint = ModelCheckpoint('/content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5',\n",
        "                            monitor='val_accuracy', \n",
        "                            mode='max', verbose=2,\n",
        "                            save_best_only=True,\n",
        "                            save_weights_only=True)\n",
        "\n",
        "    history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data = (X_dev,y_dev),\n",
        "                    batch_size=256,\n",
        "                    epochs=30,\n",
        "                    shuffle=True,\n",
        "                    callbacks= [Metrics(valid_data=(X_dev,y_dev)), checkpoint] )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 250, 300)          30000600  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 250, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 250, 128)          140544    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 250, 128)          0         \n",
            "_________________________________________________________________\n",
            "deep_self_attention (DeepSel (None, 128)               13001     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 41)                5289      \n",
            "=================================================================\n",
            "Total params: 30,159,434\n",
            "Trainable params: 158,834\n",
            "Non-trainable params: 30,000,600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "235/235 [==============================] - 46s 139ms/step - loss: 2.4090 - accuracy: 0.4021 - val_loss: 1.8803 - val_accuracy: 0.5217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " — val_f1: 0.465835 — val_precision: 0.488547 — val_recall: 0.521750\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.52175, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 2/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.9453 - accuracy: 0.4937 - val_loss: 1.7536 - val_accuracy: 0.5402\n",
            " — val_f1: 0.492048 — val_precision: 0.514563 — val_recall: 0.540200\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.52175 to 0.54020, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 3/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.8374 - accuracy: 0.5149 - val_loss: 1.6903 - val_accuracy: 0.5531\n",
            " — val_f1: 0.508575 — val_precision: 0.528597 — val_recall: 0.553150\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.54020 to 0.55315, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 4/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.7802 - accuracy: 0.5279 - val_loss: 1.6561 - val_accuracy: 0.5595\n",
            " — val_f1: 0.518680 — val_precision: 0.538523 — val_recall: 0.559550\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.55315 to 0.55955, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 5/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.7363 - accuracy: 0.5370 - val_loss: 1.6316 - val_accuracy: 0.5623\n",
            " — val_f1: 0.521611 — val_precision: 0.544110 — val_recall: 0.562250\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.55955 to 0.56225, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 6/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.7082 - accuracy: 0.5431 - val_loss: 1.6094 - val_accuracy: 0.5719\n",
            " — val_f1: 0.531748 — val_precision: 0.556157 — val_recall: 0.571850\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.56225 to 0.57185, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 7/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.6840 - accuracy: 0.5504 - val_loss: 1.5879 - val_accuracy: 0.5743\n",
            " — val_f1: 0.538785 — val_precision: 0.557700 — val_recall: 0.574350\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.57185 to 0.57435, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 8/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.6639 - accuracy: 0.5531 - val_loss: 1.5728 - val_accuracy: 0.5783\n",
            " — val_f1: 0.544023 — val_precision: 0.563255 — val_recall: 0.578300\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.57435 to 0.57830, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 9/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.6390 - accuracy: 0.5574 - val_loss: 1.5540 - val_accuracy: 0.5815\n",
            " — val_f1: 0.550343 — val_precision: 0.562454 — val_recall: 0.581500\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.57830 to 0.58150, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 10/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.6247 - accuracy: 0.5608 - val_loss: 1.5475 - val_accuracy: 0.5844\n",
            " — val_f1: 0.553667 — val_precision: 0.566246 — val_recall: 0.584400\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.58150 to 0.58440, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 11/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.6078 - accuracy: 0.5634 - val_loss: 1.5320 - val_accuracy: 0.5865\n",
            " — val_f1: 0.559057 — val_precision: 0.566901 — val_recall: 0.586550\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.58440 to 0.58655, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 12/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5937 - accuracy: 0.5670 - val_loss: 1.5261 - val_accuracy: 0.5860\n",
            " — val_f1: 0.553460 — val_precision: 0.573902 — val_recall: 0.586050\n",
            "\n",
            "Epoch 00012: val_accuracy did not improve from 0.58655\n",
            "Epoch 13/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.5738 - accuracy: 0.5707 - val_loss: 1.5180 - val_accuracy: 0.5873\n",
            " — val_f1: 0.555313 — val_precision: 0.576857 — val_recall: 0.587300\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.58655 to 0.58730, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 14/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.5651 - accuracy: 0.5741 - val_loss: 1.5031 - val_accuracy: 0.5917\n",
            " — val_f1: 0.562413 — val_precision: 0.575403 — val_recall: 0.591700\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.58730 to 0.59170, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 15/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.5489 - accuracy: 0.5746 - val_loss: 1.4915 - val_accuracy: 0.5930\n",
            " — val_f1: 0.565796 — val_precision: 0.575438 — val_recall: 0.593000\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.59170 to 0.59300, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 16/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5345 - accuracy: 0.5787 - val_loss: 1.4866 - val_accuracy: 0.5946\n",
            " — val_f1: 0.570094 — val_precision: 0.581390 — val_recall: 0.594600\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.59300 to 0.59460, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 17/30\n",
            "235/235 [==============================] - 29s 122ms/step - loss: 1.5253 - accuracy: 0.5811 - val_loss: 1.4760 - val_accuracy: 0.5960\n",
            " — val_f1: 0.568001 — val_precision: 0.582355 — val_recall: 0.595950\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.59460 to 0.59595, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 18/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5139 - accuracy: 0.5841 - val_loss: 1.4704 - val_accuracy: 0.5984\n",
            " — val_f1: 0.569593 — val_precision: 0.583549 — val_recall: 0.598400\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.59595 to 0.59840, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 19/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.5024 - accuracy: 0.5841 - val_loss: 1.4623 - val_accuracy: 0.5992\n",
            " — val_f1: 0.573875 — val_precision: 0.586309 — val_recall: 0.599200\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.59840 to 0.59920, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 20/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4928 - accuracy: 0.5878 - val_loss: 1.4506 - val_accuracy: 0.6034\n",
            " — val_f1: 0.579036 — val_precision: 0.589338 — val_recall: 0.603400\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.59920 to 0.60340, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 21/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4865 - accuracy: 0.5906 - val_loss: 1.4459 - val_accuracy: 0.6066\n",
            " — val_f1: 0.582243 — val_precision: 0.594324 — val_recall: 0.606650\n",
            "\n",
            "Epoch 00021: val_accuracy improved from 0.60340 to 0.60665, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 22/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4719 - accuracy: 0.5900 - val_loss: 1.4396 - val_accuracy: 0.6037\n",
            " — val_f1: 0.580051 — val_precision: 0.588135 — val_recall: 0.603700\n",
            "\n",
            "Epoch 00022: val_accuracy did not improve from 0.60665\n",
            "Epoch 23/30\n",
            "235/235 [==============================] - 30s 128ms/step - loss: 1.4628 - accuracy: 0.5954 - val_loss: 1.4475 - val_accuracy: 0.5986\n",
            " — val_f1: 0.569546 — val_precision: 0.592334 — val_recall: 0.598550\n",
            "\n",
            "Epoch 00023: val_accuracy did not improve from 0.60665\n",
            "Epoch 24/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4549 - accuracy: 0.5952 - val_loss: 1.4328 - val_accuracy: 0.6044\n",
            " — val_f1: 0.579258 — val_precision: 0.593380 — val_recall: 0.604450\n",
            "\n",
            "Epoch 00024: val_accuracy did not improve from 0.60665\n",
            "Epoch 25/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4488 - accuracy: 0.5947 - val_loss: 1.4240 - val_accuracy: 0.6079\n",
            " — val_f1: 0.584673 — val_precision: 0.594804 — val_recall: 0.607900\n",
            "\n",
            "Epoch 00025: val_accuracy improved from 0.60665 to 0.60790, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 26/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4382 - accuracy: 0.5983 - val_loss: 1.4250 - val_accuracy: 0.6080\n",
            " — val_f1: 0.586376 — val_precision: 0.595832 — val_recall: 0.608050\n",
            "\n",
            "Epoch 00026: val_accuracy improved from 0.60790 to 0.60805, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 27/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4301 - accuracy: 0.6011 - val_loss: 1.4164 - val_accuracy: 0.6114\n",
            " — val_f1: 0.589511 — val_precision: 0.594883 — val_recall: 0.611400\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.60805 to 0.61140, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n",
            "Epoch 28/30\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 1.4216 - accuracy: 0.6026 - val_loss: 1.4226 - val_accuracy: 0.6075\n",
            " — val_f1: 0.588596 — val_precision: 0.599549 — val_recall: 0.607450\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.61140\n",
            "Epoch 29/30\n",
            "235/235 [==============================] - 30s 127ms/step - loss: 1.4122 - accuracy: 0.6031 - val_loss: 1.4153 - val_accuracy: 0.6102\n",
            " — val_f1: 0.585160 — val_precision: 0.600632 — val_recall: 0.610250\n",
            "\n",
            "Epoch 00029: val_accuracy did not improve from 0.61140\n",
            "Epoch 30/30\n",
            "235/235 [==============================] - 28s 115ms/step - loss: 1.4081 - accuracy: 0.6064 - val_loss: 1.4082 - val_accuracy: 0.6119\n",
            " — val_f1: 0.590154 — val_precision: 0.596906 — val_recall: 0.611950\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.61140 to 0.61195, saving model to /content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK2kDnT6Gnc5",
        "outputId": "89a01ece-c520-4aa4-b53e-605c508a5458"
      },
      "source": [
        "import warnings\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "    GRU_SIZE = 64\n",
        "    DENSE = 32\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "                    ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(Bidirectional(GRU(GRU_SIZE, return_sequences=True)))\n",
        "    model.add(Dropout(0.33))\n",
        "\n",
        "    model.add(DeepSelfAttention(MAX_SEQUENCE_LENGTH, 100))\n",
        "    model.add(Dropout(0.33))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(y_train.shape[1],activation='softmax'))\n",
        "\n",
        "    print(model.summary())\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(learning_rate=0.001),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    # Load weights from the pre-trained model\n",
        "    model.load_weights('/content/gdrive/My Drive/checkpoints/BiGRUMLP.hdf5')\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=[\"categorical_crossentropy\"]\n",
        "    )\n",
        "\n",
        "    print(\"Classification report on the training data:\")\n",
        "    predictions_train = model.predict(X_train)\n",
        "    predictions_train = (predictions_train > 0.5).astype(int)   \n",
        "    print(classification_report(y_train, predictions_train, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the development data:\")\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int) \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the test data:\")\n",
        "    predictions_test = model.predict(X_test)\n",
        "    predictions_test = (predictions_test > 0.5).astype(int)    \n",
        "    print(classification_report(y_test, predictions_test, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 250, 300)          30000600  \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 250, 300)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 250, 128)          140544    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 250, 128)          0         \n",
            "_________________________________________________________________\n",
            "deep_self_attention_1 (DeepS (None, 128)               13001     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 41)                5289      \n",
            "=================================================================\n",
            "Total params: 30,159,434\n",
            "Trainable params: 158,834\n",
            "Non-trainable params: 30,000,600\n",
            "_________________________________________________________________\n",
            "None\n",
            "Classification report on the training data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.77      0.15      0.25       458\n",
            "ARTS & CULTURE       0.69      0.12      0.21       380\n",
            "  BLACK VOICES       0.68      0.32      0.44      1420\n",
            "      BUSINESS       0.70      0.32      0.44      1711\n",
            "       COLLEGE       0.57      0.29      0.39       329\n",
            "        COMEDY       0.78      0.33      0.46      1549\n",
            "         CRIME       0.71      0.41      0.52      1016\n",
            "CULTURE & ARTS       0.86      0.33      0.48       323\n",
            "       DIVORCE       0.93      0.64      0.76      1040\n",
            "     EDUCATION       0.64      0.15      0.24       305\n",
            " ENTERTAINMENT       0.77      0.63      0.69      4719\n",
            "   ENVIRONMENT       0.72      0.35      0.47       419\n",
            "         FIFTY       0.73      0.15      0.24       427\n",
            "  FOOD & DRINK       0.74      0.73      0.74      1846\n",
            "     GOOD NEWS       0.75      0.08      0.14       423\n",
            "         GREEN       0.67      0.14      0.24       767\n",
            "HEALTHY LIVING       0.72      0.11      0.19      2022\n",
            " HOME & LIVING       0.89      0.67      0.77      1295\n",
            "        IMPACT       0.72      0.14      0.24      1000\n",
            " LATINO VOICES       0.69      0.21      0.32       340\n",
            "         MEDIA       0.76      0.23      0.35       831\n",
            "         MONEY       0.71      0.35      0.47       544\n",
            "     PARENTING       0.66      0.58      0.62      2556\n",
            "       PARENTS       0.81      0.04      0.08      1168\n",
            "      POLITICS       0.86      0.72      0.78      9739\n",
            "  QUEER VOICES       0.86      0.61      0.71      1903\n",
            "      RELIGION       0.74      0.36      0.49       760\n",
            "       SCIENCE       0.83      0.37      0.52       655\n",
            "        SPORTS       0.83      0.59      0.69      1483\n",
            "         STYLE       0.87      0.16      0.27       650\n",
            "STYLE & BEAUTY       0.89      0.82      0.85      2840\n",
            "         TASTE       0.89      0.03      0.05       612\n",
            "          TECH       0.75      0.37      0.50       617\n",
            " THE WORLDPOST       0.68      0.49      0.57      1072\n",
            "        TRAVEL       0.87      0.72      0.79      2970\n",
            "      WEDDINGS       0.88      0.77      0.82      1094\n",
            "    WEIRD NEWS       0.84      0.13      0.22       794\n",
            "      WELLNESS       0.77      0.65      0.71      5475\n",
            "         WOMEN       0.54      0.12      0.19      1005\n",
            "    WORLD NEWS       0.85      0.04      0.07       656\n",
            "     WORLDPOST       0.67      0.27      0.38       787\n",
            "\n",
            "     micro avg       0.80      0.51      0.62     60000\n",
            "     macro avg       0.76      0.36      0.45     60000\n",
            "  weighted avg       0.79      0.51      0.59     60000\n",
            "   samples avg       0.51      0.51      0.51     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.62      0.10      0.18       145\n",
            "ARTS & CULTURE       0.35      0.05      0.08       145\n",
            "  BLACK VOICES       0.59      0.26      0.36       446\n",
            "      BUSINESS       0.66      0.29      0.40       630\n",
            "       COLLEGE       0.45      0.19      0.27       118\n",
            "        COMEDY       0.72      0.28      0.40       536\n",
            "         CRIME       0.64      0.32      0.42       329\n",
            "CULTURE & ARTS       0.69      0.27      0.39       100\n",
            "       DIVORCE       0.89      0.61      0.73       331\n",
            "     EDUCATION       0.38      0.14      0.21        83\n",
            " ENTERTAINMENT       0.71      0.60      0.65      1629\n",
            "   ENVIRONMENT       0.73      0.36      0.48       135\n",
            "         FIFTY       0.54      0.13      0.21       118\n",
            "  FOOD & DRINK       0.70      0.66      0.68       634\n",
            "     GOOD NEWS       0.71      0.08      0.14       133\n",
            "         GREEN       0.43      0.09      0.14       266\n",
            "HEALTHY LIVING       0.49      0.07      0.12       647\n",
            " HOME & LIVING       0.83      0.64      0.72       406\n",
            "        IMPACT       0.56      0.11      0.19       332\n",
            " LATINO VOICES       0.69      0.15      0.25       117\n",
            "         MEDIA       0.58      0.17      0.27       249\n",
            "         MONEY       0.61      0.34      0.44       172\n",
            "     PARENTING       0.62      0.52      0.57       832\n",
            "       PARENTS       0.67      0.04      0.07       377\n",
            "      POLITICS       0.83      0.68      0.75      3259\n",
            "  QUEER VOICES       0.81      0.55      0.66       669\n",
            "      RELIGION       0.63      0.31      0.42       254\n",
            "       SCIENCE       0.77      0.32      0.46       216\n",
            "        SPORTS       0.82      0.49      0.61       478\n",
            "         STYLE       0.65      0.11      0.18       246\n",
            "STYLE & BEAUTY       0.86      0.80      0.83       978\n",
            "         TASTE       0.25      0.01      0.02       230\n",
            "          TECH       0.64      0.33      0.43       193\n",
            " THE WORLDPOST       0.62      0.40      0.49       371\n",
            "        TRAVEL       0.81      0.66      0.72       943\n",
            "      WEDDINGS       0.85      0.74      0.79       371\n",
            "    WEIRD NEWS       0.55      0.08      0.15       273\n",
            "      WELLNESS       0.73      0.62      0.67      1796\n",
            "         WOMEN       0.55      0.13      0.21       359\n",
            "    WORLD NEWS       0.50      0.03      0.06       216\n",
            "     WORLDPOST       0.60      0.21      0.32       238\n",
            "\n",
            "     micro avg       0.75      0.47      0.58     20000\n",
            "     macro avg       0.64      0.32      0.39     20000\n",
            "  weighted avg       0.71      0.47      0.54     20000\n",
            "   samples avg       0.47      0.47      0.47     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.48      0.07      0.13       150\n",
            "ARTS & CULTURE       0.60      0.11      0.18       140\n",
            "  BLACK VOICES       0.56      0.26      0.35       432\n",
            "      BUSINESS       0.59      0.26      0.36       588\n",
            "       COLLEGE       0.50      0.19      0.27       102\n",
            "        COMEDY       0.76      0.27      0.40       526\n",
            "         CRIME       0.67      0.31      0.43       352\n",
            "CULTURE & ARTS       0.60      0.26      0.36        98\n",
            "       DIVORCE       0.92      0.62      0.74       371\n",
            "     EDUCATION       0.63      0.12      0.20       104\n",
            " ENTERTAINMENT       0.75      0.62      0.68      1651\n",
            "   ENVIRONMENT       0.57      0.29      0.39       151\n",
            "         FIFTY       0.42      0.08      0.14       124\n",
            "  FOOD & DRINK       0.69      0.68      0.69       586\n",
            "     GOOD NEWS       0.67      0.04      0.08       136\n",
            "         GREEN       0.59      0.13      0.21       261\n",
            "HEALTHY LIVING       0.56      0.08      0.13       662\n",
            " HOME & LIVING       0.81      0.63      0.71       408\n",
            "        IMPACT       0.63      0.12      0.20       375\n",
            " LATINO VOICES       0.74      0.16      0.26       109\n",
            "         MEDIA       0.62      0.17      0.27       288\n",
            "         MONEY       0.59      0.32      0.42       165\n",
            "     PARENTING       0.62      0.52      0.57       879\n",
            "       PARENTS       0.62      0.03      0.06       392\n",
            "      POLITICS       0.83      0.67      0.74      3202\n",
            "  QUEER VOICES       0.79      0.59      0.68       626\n",
            "      RELIGION       0.65      0.33      0.44       248\n",
            "       SCIENCE       0.80      0.28      0.41       212\n",
            "        SPORTS       0.76      0.47      0.58       514\n",
            "         STYLE       0.83      0.11      0.20       215\n",
            "STYLE & BEAUTY       0.87      0.77      0.82       987\n",
            "         TASTE       0.43      0.01      0.03       210\n",
            "          TECH       0.66      0.31      0.42       207\n",
            " THE WORLDPOST       0.64      0.42      0.50       371\n",
            "        TRAVEL       0.84      0.65      0.73      1010\n",
            "      WEDDINGS       0.86      0.76      0.81       374\n",
            "    WEIRD NEWS       0.62      0.09      0.16       257\n",
            "      WELLNESS       0.74      0.65      0.69      1728\n",
            "         WOMEN       0.43      0.10      0.16       330\n",
            "    WORLD NEWS       0.57      0.02      0.03       224\n",
            "     WORLDPOST       0.54      0.23      0.32       235\n",
            "\n",
            "     micro avg       0.75      0.47      0.58     20000\n",
            "     macro avg       0.66      0.31      0.39     20000\n",
            "  weighted avg       0.72      0.47      0.54     20000\n",
            "   samples avg       0.47      0.47      0.47     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V98UU5Qy-ZcY"
      },
      "source": [
        "pip install keras-tuner --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVFeNvvITZE8",
        "outputId": "82d37b21-348a-4a69-8f4c-89b69583c908"
      },
      "source": [
        "#hyperparameter tuning\n",
        "\n",
        "def build_model(hp):\n",
        "    MAX_WORDS = 100000\n",
        "    MAX_SEQUENCE_LENGTH = 250 \n",
        "    EMBEDDING_DIM = fasttext_embed.shape[1]\n",
        "\n",
        "    FILTERS = hp.Int('filters num' , min_value=32, max_value=256, step=64) \n",
        "    KERNEL = hp.Int('kernel size' , min_value=2, max_value=5, step=1) \n",
        "    DENSE = hp.Int('dense size' , min_value=32, max_value=256, step=64) \n",
        "    DROPOUT = hp.Float('dropout', min_value=0.2, max_value=0.5, step=0.1)\n",
        "    CONVS = hp.Int('convolutions' , min_value=2, max_value=8, step=2) \n",
        "    \n",
        "    input = Input(shape=(None,))\n",
        "    emb = (Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "            ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))(input)\n",
        "    x = (Dropout(DROPOUT))(emb)\n",
        "\n",
        "    res = x\n",
        "    for i in range(CONVS):\n",
        "        x = Conv1D(FILTERS, KERNEL, activation='relu', padding='same')(x)\n",
        "    res = Conv1D(FILTERS, 1)(res)\n",
        "    x = add([x, res])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    res = x\n",
        "    for i in range(CONVS):\n",
        "        x = Conv1D(FILTERS, KERNEL, activation='relu', padding='same')(x)\n",
        "    res = Conv1D(FILTERS, 1)(res)\n",
        "    x = add([x, res])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = (GlobalMaxPooling1D())(x)\n",
        "    x = (Dropout(DROPOUT))(x)\n",
        "\n",
        "    x = (Dense(DENSE,activation='relu'))(x)\n",
        "    x = (Dropout(DROPOUT))(x)\n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=keras.optimizers.Adam(\n",
        "            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='accuracy',\n",
        "    max_trials=40,\n",
        "    executions_per_trial=2,\n",
        "    overwrite=True )\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 6\n",
            "filters num (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 64, 'sampling': None}\n",
            "kernel size (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 5, 'step': 1, 'sampling': None}\n",
            "dense size (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 64, 'sampling': None}\n",
            "dropout (Float)\n",
            "{'default': 0.2, 'conditions': [], 'min_value': 0.2, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n",
            "convolutions (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 8, 'step': 2, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrfhip7g8VFJ",
        "outputId": "ea585674-2735-4728-9643-5793fba2e514"
      },
      "source": [
        "with tf.device('/device:GPU:0'):  \n",
        "    tuner.search(X_train, y_train,\n",
        "                epochs=20,\n",
        "                batch_size = 256,\n",
        "                validation_data=(X_dev, y_dev))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 4 Complete [00h 36m 48s]\n",
            "accuracy: 0.6722249984741211\n",
            "\n",
            "Best accuracy So Far: 0.6722249984741211\n",
            "Total elapsed time: 02h 26m 54s\n",
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "filters num       |160               |96                \n",
            "kernel size       |3                 |3                 \n",
            "dense size        |224               |224               \n",
            "dropout           |0.2               |0.2               \n",
            "convolutions      |4                 |4                 \n",
            "learning_rate     |0.01              |0.001             \n",
            "\n",
            "Epoch 1/20\n",
            "  6/235 [..............................] - ETA: 50s - loss: 1.1699 - accuracy: 0.6712WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0794s vs `on_train_batch_end` time: 0.1493s). Check your callbacks.\n",
            "235/235 [==============================] - 57s 233ms/step - loss: 1.2907 - accuracy: 0.6445 - val_loss: 1.6338 - val_accuracy: 0.6043\n",
            "Epoch 2/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.3181 - accuracy: 0.6394 - val_loss: 1.6541 - val_accuracy: 0.5979\n",
            "Epoch 3/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3252 - accuracy: 0.6372 - val_loss: 1.6880 - val_accuracy: 0.6036\n",
            "Epoch 4/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.3079 - accuracy: 0.6398 - val_loss: 1.7062 - val_accuracy: 0.6036\n",
            "Epoch 5/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3142 - accuracy: 0.6386 - val_loss: 1.7798 - val_accuracy: 0.6076\n",
            "Epoch 6/20\n",
            "235/235 [==============================] - 55s 232ms/step - loss: 1.3107 - accuracy: 0.6398 - val_loss: 1.6869 - val_accuracy: 0.6035\n",
            "Epoch 7/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3125 - accuracy: 0.6411 - val_loss: 1.6655 - val_accuracy: 0.6051\n",
            "Epoch 8/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3212 - accuracy: 0.6413 - val_loss: 1.6450 - val_accuracy: 0.6039\n",
            "Epoch 9/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3246 - accuracy: 0.6361 - val_loss: 1.6447 - val_accuracy: 0.6026\n",
            "Epoch 10/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3247 - accuracy: 0.6388 - val_loss: 1.6843 - val_accuracy: 0.6043\n",
            "Epoch 11/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3142 - accuracy: 0.6403 - val_loss: 1.6642 - val_accuracy: 0.6023\n",
            "Epoch 12/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3410 - accuracy: 0.6352 - val_loss: 1.7779 - val_accuracy: 0.5952\n",
            "Epoch 13/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3465 - accuracy: 0.6307 - val_loss: 1.7243 - val_accuracy: 0.6033\n",
            "Epoch 14/20\n",
            "235/235 [==============================] - 55s 232ms/step - loss: 1.3240 - accuracy: 0.6378 - val_loss: 1.6284 - val_accuracy: 0.6017\n",
            "Epoch 15/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3090 - accuracy: 0.6416 - val_loss: 1.6591 - val_accuracy: 0.6054\n",
            "Epoch 16/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3343 - accuracy: 0.6376 - val_loss: 1.6979 - val_accuracy: 0.6061\n",
            "Epoch 17/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3162 - accuracy: 0.6395 - val_loss: 1.6537 - val_accuracy: 0.6045\n",
            "Epoch 18/20\n",
            "235/235 [==============================] - 55s 232ms/step - loss: 1.2991 - accuracy: 0.6439 - val_loss: 1.6775 - val_accuracy: 0.6033\n",
            "Epoch 19/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3244 - accuracy: 0.6419 - val_loss: 1.6235 - val_accuracy: 0.6011\n",
            "Epoch 20/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3161 - accuracy: 0.6389 - val_loss: 1.7010 - val_accuracy: 0.6018\n",
            "Epoch 1/20\n",
            "  6/235 [..............................] - ETA: 50s - loss: 1.2130 - accuracy: 0.6595WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0802s vs `on_train_batch_end` time: 0.1459s). Check your callbacks.\n",
            "235/235 [==============================] - 57s 234ms/step - loss: 1.3137 - accuracy: 0.6393 - val_loss: 1.6603 - val_accuracy: 0.6026\n",
            "Epoch 2/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.3083 - accuracy: 0.6411 - val_loss: 1.7043 - val_accuracy: 0.6002\n",
            "Epoch 3/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.3084 - accuracy: 0.6417 - val_loss: 1.6418 - val_accuracy: 0.6033\n",
            "Epoch 4/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.2993 - accuracy: 0.6435 - val_loss: 1.6495 - val_accuracy: 0.6036\n",
            "Epoch 5/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3397 - accuracy: 0.6365 - val_loss: 1.6380 - val_accuracy: 0.6046\n",
            "Epoch 6/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.2911 - accuracy: 0.6438 - val_loss: 1.6669 - val_accuracy: 0.5996\n",
            "Epoch 7/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3050 - accuracy: 0.6416 - val_loss: 1.7583 - val_accuracy: 0.6071\n",
            "Epoch 8/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.2985 - accuracy: 0.6429 - val_loss: 1.7328 - val_accuracy: 0.6018\n",
            "Epoch 9/20\n",
            "235/235 [==============================] - 54s 232ms/step - loss: 1.2782 - accuracy: 0.6472 - val_loss: 1.7652 - val_accuracy: 0.6044\n",
            "Epoch 10/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.2847 - accuracy: 0.6478 - val_loss: 1.6747 - val_accuracy: 0.6043\n",
            "Epoch 11/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.2992 - accuracy: 0.6423 - val_loss: 1.8417 - val_accuracy: 0.6022\n",
            "Epoch 12/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.2928 - accuracy: 0.6453 - val_loss: 1.6972 - val_accuracy: 0.6061\n",
            "Epoch 13/20\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.3841 - accuracy: 0.6335 - val_loss: 1.6949 - val_accuracy: 0.5948\n",
            "Epoch 14/20\n",
            "235/235 [==============================] - ETA: 0s - loss: 1.3216 - accuracy: 0.6392"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IG5A5MtTZLc",
        "outputId": "0614d571-825d-47a0-9d4c-07a92e629f02"
      },
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout,add,BatchNormalization, Embedding, GlobalMaxPooling1D, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "MAX_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 250 \n",
        "EMBEDDING_DIM = fasttext_embed.shape[1]\n",
        "FILTERS = 128 \n",
        "KERNEL = 3 \n",
        "DENSE = 64\n",
        "\n",
        "with tf.device('/device:GPU:0'):  \n",
        "    \n",
        "    input = Input(shape=(None,))\n",
        "    emb = (Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "            ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))(input)\n",
        "    x = (Dropout(0.5))(emb)\n",
        "\n",
        "    res = x\n",
        "    for i in range(4):\n",
        "        x = Conv1D(FILTERS, KERNEL, activation='relu', padding='same')(x)\n",
        "    res = Conv1D(FILTERS, 1)(res)\n",
        "    x = add([x, res])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    res = x\n",
        "    for i in range(4):\n",
        "        x = Conv1D(FILTERS, KERNEL, activation='relu', padding='same')(x)\n",
        "    res = Conv1D(FILTERS, 1)(res)\n",
        "    x = add([x, res])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = (GlobalMaxPooling1D())(x)\n",
        "    x = (Dropout(0.5))(x)\n",
        "\n",
        "    x = (Dense(DENSE,activation='relu'))(x)\n",
        "    x = (Dropout(0.5))(x)\n",
        "\n",
        "    output = (Dense(y_train.shape[1],activation='softmax'))(x)\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "\n",
        "    print(model.summary())\n",
        "    \n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    \n",
        "    if not os.path.exists('./checkpoints'):\n",
        "        os.makedirs('./checkpoints')\n",
        "        \n",
        "    checkpoint = ModelCheckpoint(\n",
        "        'checkpoints/weights.hdf5',\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        verbose=2,\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "    \n",
        "    history= model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_data= (X_dev, y_dev),\n",
        "        batch_size=256,\n",
        "        epochs=50,\n",
        "        shuffle=True,\n",
        "        callbacks=[Metrics(valid_data=(X_dev, y_dev)), checkpoint]\n",
        "    )\n",
        "#max val acc 0.55 , no res no batch reg\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_20 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_25 (Embedding)        (None, None, 300)    30000600    input_20[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_53 (Dropout)            (None, None, 300)    0           embedding_25[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_98 (Conv1D)              (None, None, 128)    115328      dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_99 (Conv1D)              (None, None, 128)    49280       conv1d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_100 (Conv1D)             (None, None, 128)    49280       conv1d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_101 (Conv1D)             (None, None, 128)    49280       conv1d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_102 (Conv1D)             (None, None, 128)    38528       dropout_53[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, None, 128)    0           conv1d_101[0][0]                 \n",
            "                                                                 conv1d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, 128)    512         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_103 (Conv1D)             (None, None, 128)    49280       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_104 (Conv1D)             (None, None, 128)    49280       conv1d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_105 (Conv1D)             (None, None, 128)    49280       conv1d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_106 (Conv1D)             (None, None, 128)    49280       conv1d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_107 (Conv1D)             (None, None, 128)    16512       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, None, 128)    0           conv1d_106[0][0]                 \n",
            "                                                                 conv1d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, 128)    512         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_16 (Global (None, 128)          0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_54 (Dropout)            (None, 128)          0           global_max_pooling1d_16[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_33 (Dense)                (None, 64)           8256        dropout_54[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_55 (Dropout)            (None, 64)           0           dense_33[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_34 (Dense)                (None, 41)           2665        dropout_55[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 30,527,873\n",
            "Trainable params: 526,761\n",
            "Non-trainable params: 30,001,112\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "  6/235 [..............................] - ETA: 50s - loss: 21.9585 - accuracy: 0.0306WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0788s vs `on_train_batch_end` time: 0.1472s). Check your callbacks.\n",
            "235/235 [==============================] - 116s 236ms/step - loss: 4.3729 - accuracy: 0.1153 - val_loss: 3.4105 - val_accuracy: 0.1644\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " — val_f1: 0.046427 — val_precision: 0.027030 — val_recall: 0.164400\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.16440, saving model to checkpoints/weights.hdf5\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 3.4145 - accuracy: 0.1584 - val_loss: 3.2788 - val_accuracy: 0.1645\n",
            " — val_f1: 0.046686 — val_precision: 0.063021 — val_recall: 0.164500\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.16440 to 0.16450, saving model to checkpoints/weights.hdf5\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 3.3042 - accuracy: 0.1535 - val_loss: 3.2385 - val_accuracy: 0.2338\n",
            " — val_f1: 0.130158 — val_precision: 0.099564 — val_recall: 0.233750\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.16450 to 0.23375, saving model to checkpoints/weights.hdf5\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 3.1953 - accuracy: 0.1825 - val_loss: 3.0389 - val_accuracy: 0.2368\n",
            " — val_f1: 0.132477 — val_precision: 0.105540 — val_recall: 0.236850\n",
            "\n",
            "Epoch 00004: val_accuracy improved from 0.23375 to 0.23685, saving model to checkpoints/weights.hdf5\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 3.0806 - accuracy: 0.2087 - val_loss: 2.8434 - val_accuracy: 0.2756\n",
            " — val_f1: 0.156367 — val_precision: 0.136682 — val_recall: 0.275650\n",
            "\n",
            "Epoch 00005: val_accuracy improved from 0.23685 to 0.27565, saving model to checkpoints/weights.hdf5\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 2.9551 - accuracy: 0.2541 - val_loss: 2.7045 - val_accuracy: 0.3451\n",
            " — val_f1: 0.218287 — val_precision: 0.165205 — val_recall: 0.345100\n",
            "\n",
            "Epoch 00006: val_accuracy improved from 0.27565 to 0.34510, saving model to checkpoints/weights.hdf5\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.7836 - accuracy: 0.3168 - val_loss: 2.6889 - val_accuracy: 0.3550\n",
            " — val_f1: 0.228406 — val_precision: 0.194282 — val_recall: 0.354950\n",
            "\n",
            "Epoch 00007: val_accuracy improved from 0.34510 to 0.35495, saving model to checkpoints/weights.hdf5\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.6177 - accuracy: 0.3529 - val_loss: 2.4794 - val_accuracy: 0.3803\n",
            " — val_f1: 0.254715 — val_precision: 0.200180 — val_recall: 0.380350\n",
            "\n",
            "Epoch 00008: val_accuracy improved from 0.35495 to 0.38035, saving model to checkpoints/weights.hdf5\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.5039 - accuracy: 0.3753 - val_loss: 2.2234 - val_accuracy: 0.4198\n",
            " — val_f1: 0.299250 — val_precision: 0.259503 — val_recall: 0.419850\n",
            "\n",
            "Epoch 00009: val_accuracy improved from 0.38035 to 0.41985, saving model to checkpoints/weights.hdf5\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 2.3990 - accuracy: 0.4033 - val_loss: 2.1375 - val_accuracy: 0.4494\n",
            " — val_f1: 0.331967 — val_precision: 0.303858 — val_recall: 0.449450\n",
            "\n",
            "Epoch 00010: val_accuracy improved from 0.41985 to 0.44945, saving model to checkpoints/weights.hdf5\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.3195 - accuracy: 0.4211 - val_loss: 2.0561 - val_accuracy: 0.4650\n",
            " — val_f1: 0.348961 — val_precision: 0.306019 — val_recall: 0.465000\n",
            "\n",
            "Epoch 00011: val_accuracy improved from 0.44945 to 0.46500, saving model to checkpoints/weights.hdf5\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.2540 - accuracy: 0.4362 - val_loss: 1.9803 - val_accuracy: 0.4898\n",
            " — val_f1: 0.396457 — val_precision: 0.358274 — val_recall: 0.489800\n",
            "\n",
            "Epoch 00012: val_accuracy improved from 0.46500 to 0.48980, saving model to checkpoints/weights.hdf5\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 2.1881 - accuracy: 0.4497 - val_loss: 1.9507 - val_accuracy: 0.4942\n",
            " — val_f1: 0.391870 — val_precision: 0.393401 — val_recall: 0.494250\n",
            "\n",
            "Epoch 00013: val_accuracy improved from 0.48980 to 0.49425, saving model to checkpoints/weights.hdf5\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.1336 - accuracy: 0.4629 - val_loss: 1.8985 - val_accuracy: 0.5114\n",
            " — val_f1: 0.433874 — val_precision: 0.406752 — val_recall: 0.511450\n",
            "\n",
            "Epoch 00014: val_accuracy improved from 0.49425 to 0.51145, saving model to checkpoints/weights.hdf5\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.0705 - accuracy: 0.4733 - val_loss: 1.8556 - val_accuracy: 0.5149\n",
            " — val_f1: 0.424050 — val_precision: 0.421140 — val_recall: 0.514900\n",
            "\n",
            "Epoch 00015: val_accuracy improved from 0.51145 to 0.51490, saving model to checkpoints/weights.hdf5\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 2.0188 - accuracy: 0.4844 - val_loss: 1.7758 - val_accuracy: 0.5308\n",
            " — val_f1: 0.446350 — val_precision: 0.441282 — val_recall: 0.530800\n",
            "\n",
            "Epoch 00016: val_accuracy improved from 0.51490 to 0.53080, saving model to checkpoints/weights.hdf5\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.9757 - accuracy: 0.4908 - val_loss: 1.7573 - val_accuracy: 0.5347\n",
            " — val_f1: 0.453778 — val_precision: 0.468995 — val_recall: 0.534700\n",
            "\n",
            "Epoch 00017: val_accuracy improved from 0.53080 to 0.53470, saving model to checkpoints/weights.hdf5\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 55s 235ms/step - loss: 1.9531 - accuracy: 0.4974 - val_loss: 1.7122 - val_accuracy: 0.5406\n",
            " — val_f1: 0.463775 — val_precision: 0.464337 — val_recall: 0.540600\n",
            "\n",
            "Epoch 00018: val_accuracy improved from 0.53470 to 0.54060, saving model to checkpoints/weights.hdf5\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.9061 - accuracy: 0.5039 - val_loss: 1.7407 - val_accuracy: 0.5436\n",
            " — val_f1: 0.469777 — val_precision: 0.472943 — val_recall: 0.543550\n",
            "\n",
            "Epoch 00019: val_accuracy improved from 0.54060 to 0.54355, saving model to checkpoints/weights.hdf5\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.8876 - accuracy: 0.5095 - val_loss: 1.6789 - val_accuracy: 0.5520\n",
            " — val_f1: 0.486673 — val_precision: 0.500094 — val_recall: 0.552000\n",
            "\n",
            "Epoch 00020: val_accuracy improved from 0.54355 to 0.55200, saving model to checkpoints/weights.hdf5\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.8649 - accuracy: 0.5143 - val_loss: 1.6644 - val_accuracy: 0.5504\n",
            " — val_f1: 0.480429 — val_precision: 0.497108 — val_recall: 0.550400\n",
            "\n",
            "Epoch 00021: val_accuracy did not improve from 0.55200\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.8400 - accuracy: 0.5189 - val_loss: 1.6919 - val_accuracy: 0.5555\n",
            " — val_f1: 0.498224 — val_precision: 0.532490 — val_recall: 0.555500\n",
            "\n",
            "Epoch 00022: val_accuracy improved from 0.55200 to 0.55550, saving model to checkpoints/weights.hdf5\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.8257 - accuracy: 0.5190 - val_loss: 1.6632 - val_accuracy: 0.5573\n",
            " — val_f1: 0.486998 — val_precision: 0.523058 — val_recall: 0.557300\n",
            "\n",
            "Epoch 00023: val_accuracy improved from 0.55550 to 0.55730, saving model to checkpoints/weights.hdf5\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.7961 - accuracy: 0.5264 - val_loss: 1.6335 - val_accuracy: 0.5620\n",
            " — val_f1: 0.497130 — val_precision: 0.523566 — val_recall: 0.561950\n",
            "\n",
            "Epoch 00024: val_accuracy improved from 0.55730 to 0.56195, saving model to checkpoints/weights.hdf5\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.7890 - accuracy: 0.5295 - val_loss: 1.6712 - val_accuracy: 0.5600\n",
            " — val_f1: 0.497016 — val_precision: 0.524453 — val_recall: 0.560000\n",
            "\n",
            "Epoch 00025: val_accuracy did not improve from 0.56195\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.7672 - accuracy: 0.5332 - val_loss: 1.6421 - val_accuracy: 0.5612\n",
            " — val_f1: 0.495555 — val_precision: 0.534958 — val_recall: 0.561200\n",
            "\n",
            "Epoch 00026: val_accuracy did not improve from 0.56195\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.7537 - accuracy: 0.5374 - val_loss: 1.6171 - val_accuracy: 0.5701\n",
            " — val_f1: 0.512231 — val_precision: 0.539601 — val_recall: 0.570100\n",
            "\n",
            "Epoch 00027: val_accuracy improved from 0.56195 to 0.57010, saving model to checkpoints/weights.hdf5\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.7412 - accuracy: 0.5395 - val_loss: 1.6005 - val_accuracy: 0.5684\n",
            " — val_f1: 0.509060 — val_precision: 0.548900 — val_recall: 0.568450\n",
            "\n",
            "Epoch 00028: val_accuracy did not improve from 0.57010\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 55s 233ms/step - loss: 1.7345 - accuracy: 0.5425 - val_loss: 1.5930 - val_accuracy: 0.5713\n",
            " — val_f1: 0.511197 — val_precision: 0.541463 — val_recall: 0.571350\n",
            "\n",
            "Epoch 00029: val_accuracy improved from 0.57010 to 0.57135, saving model to checkpoints/weights.hdf5\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 55s 233ms/step - loss: 1.7225 - accuracy: 0.5456 - val_loss: 1.6158 - val_accuracy: 0.5741\n",
            " — val_f1: 0.520915 — val_precision: 0.532504 — val_recall: 0.574050\n",
            "\n",
            "Epoch 00030: val_accuracy improved from 0.57135 to 0.57405, saving model to checkpoints/weights.hdf5\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.7017 - accuracy: 0.5490 - val_loss: 1.6278 - val_accuracy: 0.5764\n",
            " — val_f1: 0.520885 — val_precision: 0.549213 — val_recall: 0.576400\n",
            "\n",
            "Epoch 00031: val_accuracy improved from 0.57405 to 0.57640, saving model to checkpoints/weights.hdf5\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6971 - accuracy: 0.5515 - val_loss: 1.5642 - val_accuracy: 0.5800\n",
            " — val_f1: 0.524338 — val_precision: 0.544198 — val_recall: 0.580000\n",
            "\n",
            "Epoch 00032: val_accuracy improved from 0.57640 to 0.58000, saving model to checkpoints/weights.hdf5\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6698 - accuracy: 0.5541 - val_loss: 1.6289 - val_accuracy: 0.5729\n",
            " — val_f1: 0.515170 — val_precision: 0.543884 — val_recall: 0.572900\n",
            "\n",
            "Epoch 00033: val_accuracy did not improve from 0.58000\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6647 - accuracy: 0.5576 - val_loss: 1.6041 - val_accuracy: 0.5776\n",
            " — val_f1: 0.524561 — val_precision: 0.544567 — val_recall: 0.577600\n",
            "\n",
            "Epoch 00034: val_accuracy did not improve from 0.58000\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6613 - accuracy: 0.5553 - val_loss: 1.5811 - val_accuracy: 0.5827\n",
            " — val_f1: 0.531512 — val_precision: 0.541469 — val_recall: 0.582700\n",
            "\n",
            "Epoch 00035: val_accuracy improved from 0.58000 to 0.58270, saving model to checkpoints/weights.hdf5\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6443 - accuracy: 0.5603 - val_loss: 1.5901 - val_accuracy: 0.5847\n",
            " — val_f1: 0.540486 — val_precision: 0.547575 — val_recall: 0.584750\n",
            "\n",
            "Epoch 00036: val_accuracy improved from 0.58270 to 0.58475, saving model to checkpoints/weights.hdf5\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6365 - accuracy: 0.5625 - val_loss: 1.6009 - val_accuracy: 0.5771\n",
            " — val_f1: 0.525991 — val_precision: 0.543595 — val_recall: 0.577100\n",
            "\n",
            "Epoch 00037: val_accuracy did not improve from 0.58475\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 55s 234ms/step - loss: 1.6332 - accuracy: 0.5642 - val_loss: 1.5703 - val_accuracy: 0.5813\n",
            " — val_f1: 0.528840 — val_precision: 0.547070 — val_recall: 0.581300\n",
            "\n",
            "Epoch 00038: val_accuracy did not improve from 0.58475\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 55s 232ms/step - loss: 1.6235 - accuracy: 0.5653 - val_loss: 1.5799 - val_accuracy: 0.5852\n",
            " — val_f1: 0.537594 — val_precision: 0.550470 — val_recall: 0.585200\n",
            "\n",
            "Epoch 00039: val_accuracy improved from 0.58475 to 0.58520, saving model to checkpoints/weights.hdf5\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 55s 233ms/step - loss: 1.6106 - accuracy: 0.5677 - val_loss: 1.5688 - val_accuracy: 0.5861\n",
            " — val_f1: 0.536713 — val_precision: 0.552519 — val_recall: 0.586150\n",
            "\n",
            "Epoch 00040: val_accuracy improved from 0.58520 to 0.58615, saving model to checkpoints/weights.hdf5\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 55s 232ms/step - loss: 1.6014 - accuracy: 0.5718 - val_loss: 1.5338 - val_accuracy: 0.5857\n",
            " — val_f1: 0.538757 — val_precision: 0.547787 — val_recall: 0.585750\n",
            "\n",
            "Epoch 00041: val_accuracy did not improve from 0.58615\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.5935 - accuracy: 0.5720 - val_loss: 1.5916 - val_accuracy: 0.5893\n",
            " — val_f1: 0.542602 — val_precision: 0.553253 — val_recall: 0.589350\n",
            "\n",
            "Epoch 00042: val_accuracy improved from 0.58615 to 0.58935, saving model to checkpoints/weights.hdf5\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 55s 233ms/step - loss: 1.5909 - accuracy: 0.5722 - val_loss: 1.5812 - val_accuracy: 0.5856\n",
            " — val_f1: 0.534750 — val_precision: 0.546443 — val_recall: 0.585550\n",
            "\n",
            "Epoch 00043: val_accuracy did not improve from 0.58935\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 55s 232ms/step - loss: 1.5778 - accuracy: 0.5769 - val_loss: 1.5751 - val_accuracy: 0.5864\n",
            " — val_f1: 0.540067 — val_precision: 0.557268 — val_recall: 0.586400\n",
            "\n",
            "Epoch 00044: val_accuracy did not improve from 0.58935\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 55s 233ms/step - loss: 1.5732 - accuracy: 0.5777 - val_loss: 1.5574 - val_accuracy: 0.5880\n",
            " — val_f1: 0.541516 — val_precision: 0.556586 — val_recall: 0.588000\n",
            "\n",
            "Epoch 00045: val_accuracy did not improve from 0.58935\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.5661 - accuracy: 0.5782 - val_loss: 1.5380 - val_accuracy: 0.5917\n",
            " — val_f1: 0.546144 — val_precision: 0.561128 — val_recall: 0.591700\n",
            "\n",
            "Epoch 00046: val_accuracy improved from 0.58935 to 0.59170, saving model to checkpoints/weights.hdf5\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.5595 - accuracy: 0.5815 - val_loss: 1.5517 - val_accuracy: 0.5914\n",
            " — val_f1: 0.545229 — val_precision: 0.558993 — val_recall: 0.591350\n",
            "\n",
            "Epoch 00047: val_accuracy did not improve from 0.59170\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.5530 - accuracy: 0.5795 - val_loss: 1.5504 - val_accuracy: 0.5904\n",
            " — val_f1: 0.546130 — val_precision: 0.560432 — val_recall: 0.590450\n",
            "\n",
            "Epoch 00048: val_accuracy did not improve from 0.59170\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.5384 - accuracy: 0.5857 - val_loss: 1.5574 - val_accuracy: 0.5957\n",
            " — val_f1: 0.552295 — val_precision: 0.563279 — val_recall: 0.595700\n",
            "\n",
            "Epoch 00049: val_accuracy improved from 0.59170 to 0.59570, saving model to checkpoints/weights.hdf5\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 54s 231ms/step - loss: 1.5362 - accuracy: 0.5835 - val_loss: 1.5496 - val_accuracy: 0.5946\n",
            " — val_f1: 0.550411 — val_precision: 0.564192 — val_recall: 0.594600\n",
            "\n",
            "Epoch 00050: val_accuracy did not improve from 0.59570\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyQDKDGTTZPf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enb_BtFPTZSE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "yn5UkE-aTZUl",
        "outputId": "ebe7465d-c967-45f2-bb8e-f7b4b064716d"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f25e218db10>]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3xU15338c9PvSKBOpKQAIkiOojiXsG4LG5xryn2JhvHaeuNU9Z51nHyJN7EKU+cTRxvHJc4tuPYDnYwLrgXQKIIJEAghEC99z6a8/xxByOBykjMaDSj3/v1mtdo7j2a+V0QX67OPedcMcaglFLK+/l5ugCllFKuoYGulFI+QgNdKaV8hAa6Ukr5CA10pZTyEQGe+uDY2FiTnp7uqY9XSimvtGPHjjpjTNxg+zwW6Onp6eTm5nrq45VSyiuJyNGh9mmXi1JK+QgNdKWU8hEa6Eop5SOcCnQRWS8ihSJSJCL3D9HmehHZJyIFIvKsa8tUSik1khEvioqIP/AosBYoA3JEZKMxZl+/NpnAd4GzjDGNIhLvroKVUkoNzpkz9FVAkTGm2BjTAzwHXHlSm7uAR40xjQDGmBrXlqmUUmokzgR6MlDa73WZY1t/c4A5IvKxiGwVkfWDvZGI3C0iuSKSW1tbO7aKlVJKDcpV49ADgEzgfCAF+EBEFhljmvo3MsY8BjwGkJ2drev2KqV8jzFQuh1KPoSgcAidBqFTIczxHDoVQqLBz/VjUpwJ9HIgtd/rFMe2/sqAbcaYXuCIiBzECvgcl1SplFITmTFQnQ97X4T8l6D52PDtL30YVv+ry8twJtBzgEwRmYkV5DcCN5/U5hXgJuAJEYnF6oIpdmWhSinlFu31cPRj63FsK/S0gX8Q+Ac6nh1fB4ZB8BQImXLiOSQKWqsh/+9QVwjiD7MvgAu+B3MvBWOHzkboaIDOhhNfp53plkMZMdCNMTYRuQd4A/AH/mSMKRCRB4FcY8xGx751IrIP6APuM8bUu6VipdTk0VoNxz6Bo5/CsU+tYE0/G9LPhRmrITjS+fey9UBbFbRUQNMxK7yPfgK1+639AaGQkg0xs6GvB/p6Tzz3dFjB390MXS3Q3WKF9XEzzoTLfwFZV0F47MDPDZtmvec4EE/dgi47O9voWi5K+bjeTqgthK4m6Go+8ehsgu5W8AuwQjog2DoTDgi2tlXnW2Hb4PhFPzDMCltbD5TvAHuvdTacvNwK+Lh51pl1d+uJR1eL9bktFdBaCe0nDcQIioDU1ZB+FqSdDdOXQUCQc8dljPV5XS1W3RED18qqau7i46I6EqNCmBUXTuKUEETEBX+gICI7jDHZg+3z2OJcSikPMwbqDkJ9ETQcgcYSaDxifd1SAbMvhPPvh6TFzr9nXy9U7ILi9+HI+9bFwb7uU9uJPwRHgN1u7e/rGbg/dCrMOANWfN7qnkhaYgU/QE87lG6Dko/gyIfwyf8Du23ge/fvFomcbgV/5HSYkuR4ng6xc8B/jBEoYv120O83BGMMnxyu55mtR3lzXzV99hMny2FB/syMDWdWXASzYsNZtyCBBdOjxvbZw9BAV8rb2O1jHyFhDFTmQcHL1qOp38J9wVEwLR0SF8LMc62Le384B+ZvgPO/CwlZg79fbSEc3gLF71ln1T1t1r6ERbDqLkhZCeFxEBpt9TmHRFlnx/3PWI2xQt3Wbf2nEDp16GMMCrf+s5l9ofW6uw1aq04EbGDowPceI2MMZY2d7C5tYndpEx09faROCyV1ahip08JInRrKtPAgWrps/H1HGc9sO0pxbTtTwwL50jkz2bBkOs2dvRTXtluPujbySpv4554KkqND3RLo2uWi1ETX3Wqd6R771OpLLs+FmAw46xuw4OqRzzKNgeoCKHjJCvGGYqtbY9YFMP9fIGEhTJtphWj/IOxsgq2/g09/Z4X0gqutM/aIBCu8D2+BonegpcxqH5MBM8+z/jNIPwfCY9z2RzJWxhg6evpo77bR2dtHt81Ol+O5u9dOR4+NQzVt7DrWxO7SRurarN8cggP8CA8OoKF94G8S4UH+2OyGbpud5TOiuXVNGpctSiIk0H/IGrp6+wCGbTOc4bpcNNCVmiiMsfp56w5ZXSG1B6wLd1V7wfSB+FldDykr4cgH1v7oNDjrXlh6i3Vmepy9z/pP4MBrcOCfVleK+Fthu/AamHeFdbHOGR0N8OmjsO33VneH+Fn1BEfBrHNh9kWQcRFEz3DPn8so2e2GXaWNvLmvmtySRlo6e2nrttHWZaOtx4YzkTcrLpylqdEsmzGVZanRzE2MJNDfj7ZuG2WNHZQ2dFLa0EFpYwcA1y5PYWGy68+4B6OBrtRE01ZjXfirLoCa/VaA1x20LhgeFxAKySsg7QyrPzl11Yk+W7sdDr4OHz5inbGHx8Gar1gXBws3QeFm6KizLtjNPBfmXW51nZw8AmM02ush54/WfxYZF0Fy9tj7oF2sq7ePj4vqeGtfNW/vr6aurYcAP2H5jKnERgYRHhRAREgAEcHWIzw4gNBAf4ID/QgO8CfE8Rwc4Ed6TDhRYYGePqQhaaAr5Sl9Nmt8cuUe60z7eIh31J1oE5FgXaD77JFpPaakjNxXbox1cfCjX1pdIGBdDMxcZ4V4xsXWhUEvUNXcxUdFdWwtrqepo5c+ux2b3dDX72GzG+xm4LY+Y6hs6qKzt4+I4ADOnxvH2qwEzp8bT1ToxA3msdJRLkq5ijFWX/SWH0FvhzVaYkqy49nxdXcrVO2xLj5W7zsxyiMgBOLnw9z1Vr91wgKIX3B6fc0iMPMc61GVDx311tm8s8PvPKit28a24no+PFTHR0V1FNVYF1OnhQeROCWEAH/BT4QAP8HfTwgK8CPU8XWAn7XP3/H63Mw4LpwXz5pZMQQFTN7bPGigK+Ws6gJ4/TvWGh0Ji6zxzy0V1kXGkg8HdpeERFvD/VbdZfV7Jy62zrr9xnYhzCmJC9333qehq7ePopo2imraOFTTyqFq6+uS+nbsxrrguGrmNK5bkcLZmbHMT5yCn59rxmxPNhroSo2ksxHe/b+Q87jVfXH5I7DizlPDubvNmsASEAxRqS4ZOjdRdfX2UdXcRUVTJ+VNnVQ2d1HT2kVzp42mjh6aO3tp6uiludN6HBfgJ6THhjM3MZIrFiexelYMK9KmjnnEhxpIA11NDnVF1sXCpmMQnWqNDpmaDlPTrOF6x9nt1uzCjnrrUbkH3v+pFeorPg8X/mDo0SHBERCcOS6H427GGBo7ejlS187R+nZK6js+ey5v7PhsOF9/0WGBTA0LIio0kGnhQcyKDSc6LIhp4UFkxEeQGR9BWkz4pO4ScTcNdOWb7H1QlusY8bHJGkEC1gXD7paBbYOjrKnbnU3WAkr91+gASF0Dlz1sdZ34MFufnU+L69m0t5K39lUPCG0/genRoaTHhHPx/ASSo0NJig5lenQI06NCSYwK0bPsCUADXXkvY6yz6aZSaC6D5lLr0XTMmrHYXmtNoEk7C1Z+yVr9LnqG1dfdeNSaJXn8ub3W6vcOi7GG9oXFWGfiEQnWBUwv6z7p7bOzr6KFHUcb2XGskcqmTtJiwpkZO/ARHOD3WYi/UVBNQ3sPYUH+XDQ/gaWp0aTHhJEeG07K1FCCAzSwJzoNdOVd7H3WMqf5L1mTZk5ecMk/GKJSrAuWcy+HzLXWlPP+QqKsC5ajWaNkArP12Smp7+BgdSt7yprZebSRvLImum3WbxrTo0JInRbG9iMNvLxr4K0MQgL96Oq1Exbkz8XzE7hsURLnz43Ts20vpYGuJobKPGsyTHjMiUWUpiRDWCxgrGnvBS/Dvo3QXgOB4TDnEmvRpahU6xGdak2w8bKzaWfZ7Ybypk6KatoorG6lsMp6FNW20eMI70B/YcH0KG5ZncaKtKksT4smKerEDNLOnj5K6ts5Umc9alq6OGN2rIa4j9BAV57VUmGN6c77KzDIJDe/QGtKe3eLNXNyzjpYcI01cSYobNzLHS9NHT3sLm2isKqVg9VtFNW0cqimjY6evs/aJE4JYW5iJGdnxjInIZJ5iZFkxEcMG8yhQf7MT5rC/CTvmGykRkcDXXlGdxt8/Gtr6VPTZ61HctY3wNYFLZXQWnHiubPRWuxpznprJImPsdsNh2ra2HmskZ2OPu/i2vbP9sdFBjMnIYLrs1PJTIhgTkIkc+IjJ/T0dOUZGuhqfNn7YPez8M6PoK3aOtu++IfWEMLjpkwHVniqQreraelid2kTeWVN5JU2k1fWRGuXtZ731LBAls+YyrXLU1g2I5qspClEh038WZ9qYtBAV+5j77NWDqzMg8rdjuc90NMKKavghr9A6kpPV+lWta3dFFQ0U1DRwt4yK7wrm7sA8PcT5iVGcsXi6VZ/94xoZsaGu+zONmry0UBXrtXbBftfhd3PwLFtYOu0tgeEWlPTl9xgrcM973KfunjZ1m2jpK6dkvp2Dla1kl/RQkFFM9UtJ+7Wkx4TxqqZ01iSEs2S1CgWTI/SC5HKpTTQlWtU5cPOp2DP89bY8KnpkP15SFpqTciJyZgwS62eDmMMB6pa+biojsKqVseIkQ7q2k4Et59ARnwEZ82OJWv6FBYmR5E1fQpTQrTPW7mX9/8LU57TWAJFb1t94uU7rLW352+A5bdbFzHHepu0CaaurZuPDtXxwaFaPjxUR22rFd7xkcGkx4Rz4bw40mPDSY+xHjNjwwkN0jNvNf400JXzWiqtVQWPvG/dMafpmLU9PgvW/wwWX+/8XXAmsKaOHrYfaWDbkQY+PVzPvkprqYCpYYGcnRnHuZmxnJMZR2JUiIcrVWogDXQ1svKd8Mq/Qe1+63VItLX+9pn3WnfDiZ3j1f3h1S1d7DrWyNZiK8QPVLVgHMu6LpsRzX2XzOWczFgWTo/SZV3VhKaBroZXvQ+euQaCImHdQ1aAJyzy2u6UmtYu9pY1s6esmfzyZvaUN3/WhRIS6MeKtKl86+I5rJ4Vw5LUKF2/RHkVDXQ1tPrD8NSV1p127nx14FhxL2KM4YNDdfxmyyF2HG0ETly4PCczlsXJUSxKiWZRcpQu7aq8mga6GlxTKTy5wZrFeftrXhnmxhjeK6zl11sOsbu0ielRIdx/6TxWpE0lK2kK4cH64698i/5Eq1O1VsNTG6x7Y975KsTN9XRFo2KMYcv+Gn7zziH2lDWTHB3KT65exOdWpOgZuPJpGuhqoI4GePoqK9Rvf2XC3dSht89OTkkDW/bX8MHBWlq6erEbK8TtBuzGYOsztHXbSJ0Wys+uXcQ1y1MI9NcgV75PA12d0FQKL9xu9Z3f8jdIXeXpigBobO/hvYM1vL2/hg8Ka2ntthHk78fqWdNYkTYVEcFPwM/xLCIsTI7iyqXTNcjVpKKBPln12aB6L5Ruh9Jt1jT9ljLrDj83/AVmnefR8qqau3ijoIrX8yvZfqQBu7FWHbxsURIXzY/nrIxY7QNX6iT6L2KyMQbe/iFs/yP0dljbpiRD6mpI/RrMvhDi5niktKP17WzOr2JzQRW7jjUBkBkfwVcvyGBtVoKOA1dqBBrok82Hv7DWIV9wNcy7AmassW7Z5iFNHT28mlfBizvLySu1QnxRchT3XTKXSxYkkhHve+ufK+UuTgW6iKwHfg34A48bY3560v47gf8Gjt+w8LfGmMddWKdyhbznrXXIF98AV//BY7M7e/vsfHCwlr/vLOPtfTX09NmZlxjJ9y6bx6ULk0id5rt3IlLKnUYMdBHxBx4F1gJlQI6IbDTG7Dup6fPGmHvcUKNyheL34R9ftRbN2vBbj4R5eVMnT396lBd3lFLX1kNMeBC3rJnBtctTWDB9iq4DrtRpcuYMfRVQZIwpBhCR54ArgZMDXU1U1fvg+VutJWxveAYCxu8OOMYYckoaeeLjI7xRUAXAxfMTuC47lfPnxukoFKVcyJlATwZK+70uA1YP0u5aETkXOAh80xhTOkgbNd5aKuAvn4OgcLj1RQiNHpeP7ert49W8Cv78SQkFFS1EhQZy17mzuP2MdJKjQ0d+A6XUqLnqouirwF+NMd0i8q/Ak8CFJzcSkbuBuwFmzJjhoo9WQ+pqgb9cB13N8PnXx+XiZ3VLF89sPcqz245R397DnIQIfnL1Iq5elqxrhCvlZs4EejmQ2u91CicufgJgjKnv9/Jx4OHB3sgY8xjwGEB2drYZVaXKec1lcPAN6w5CtQfg5hcgabFbP3LXsUae+LiETXsr6TOGi+bFc+eZMzkrI0b7xpUaJ84Eeg6QKSIzsYL8RuDm/g1EJMkYU+l4uQHY79Iq1fDsfVCWY4X4oTehOt/aPjUdrnkMMi5yy8f22Oy8nl/JEx+XsLu0icjgAG4/I507zkwjLSbcLZ+plBraiIFujLGJyD3AG1jDFv9kjCkQkQeBXGPMRuBeEdkA2IAG4E431qz6ay6D/73EmuUp/pB2Jqz9EcxZD7GZbhnNUtPaxbPbjvHstmPUtHYzMzac/9qwgGtXpBChszeV8hgxxjM9H9nZ2SY3N9cjn+1Tnr8VDr0NV/4WMi5260XPXccaefKTEv65t5LePsP5c+O444x0zpsTpzM4lRonIrLDGJM92D49nfJmB9+E/a/CRQ/Aos+57WPySpt4YGMBeaVNRAQHcMvqNG4/I41ZcTqLU6mJRAPdW/V2wuv3WffzPONrbvkIu93w+EfFPLy5kLjIYB68cgHXLNduFaUmKv2X6a0++hU0lsDtG90yUai2tZtv/y2PDw7Wsn5BIj+7djFRYYEu/xyllOtooHuj+sPw0S9h0XVuWeb2w0O1fPP5PFq6ennoqoXcsnqGDj1UygtooHsbY2DTfRAQDOseculb9/bZeeStg/z+/cNkxEXwzJdWMS9xiks/QynlPhro3mbfP+DwFlj/M4hMdNnbFtW08a0XdrOnrJmbVs3ggSuydGanUl5GA92bdLfC5u9C4iJY+SWXvKXdbnh661F+smk/oUH+/M8ty7l0UZJL3lspNb400L3Jez+F1gq4/inwP/2/uqrmLu57MY8PD9Vx/tw4Hr52MfFTQlxQqFLKEzTQvUX5Dtj6P7D8DkhdedpvtzGvgv98JZ8em50fX72Qm1fphU+lvJ0Gujeo2gvPXGvd+/Pi/3Nab9Vjs/P9l/fytx1lLJsRzSPXL2VmrK67opQv0ECf6GoOwFNXQWAY3PkqhE0b81s1d/bylWd28Mnher52YQZfvyiTAL3BhFI+QwN9Iqsrgqc2gJ8/3PGqtXriGJU3dfL5J7ZTXNvOI9cv4ZrlnrsxtFLKPTTQJ6qGI/Dkv1hL4975T4iZPea3yi9v5gt/zqGzp48nv7CKszJiXVioUmqi0ECfiJpK4ckNYOuEO16D+Hljfqt3C2u45y87iQoN5MWvnMncxEgXFqqUmkg00CealkrrzLyrGe7YCIkLx/xWz20/xvdfyWduQiRPfH4lCTokUSmfpoE+0fzzW9BWY4X59KVjegtjDL96+xC/3nKIc+fE8btblusKiUpNAvqvfCIp3wmFm+CCH0DKoOvXj8jWZ+f7L+fzfG4p161I4SfXLCJQR7IoNSlooE8k7/4EQqfC6n8d07d39Ni459ldvHOghq9dmMG31s7RyUJKTSIa6BNF6XYoesuaOBQy+hUO69u6+cKTuewta+KhqxZy65o0l5eolJrYNNAnind/DGGxsPKuUX/rsfoO7nhiOxVNnfz+1hWsW+C6VRiVUt5DA30iKPkYit+DdT+G4NHdp/NgdSs3/3EbNrudZ+9azYq0sc8kVUp5Nw10TzPGOjuPSICVXxzVtx6sbuWmx7bi7ye8+OUzyIjXMeZKTWY6/MHTjrwPRz+Gc74NgaFOf9uh6lZu/qMV5s/dvUbDXCmlge5RxsA7P7ZWUVx+h9Pfdqi6lZv+uBU/Ef569xpmxY2um0Yp5Zs00D2paAuUbXecnTs3i7OoppWb/rgNcYT5bA1zpZSDBrqnGAPvPgRRM2DZbU59S1FNKzc+tg0R+OtdGuZKqYE00D3l4Gao2AXn3QcBQSM2P1LXzo2PbQOsMM+I1zBXSg2kge4pH/7CWt98yU0jNm3q6OELf87BbgzP3b1aw1wpNSgNdE+o2Q9lObDqbvAPHLZpj83Ol5/ZQXljJ4/dtkJHsyilhqTj0D1h1zPgFwCLbxi2mTGGH7yyl63FDfzqhqVkp+ukIaXU0PQMfbz19ULeczD3Uggf/s5Bf/igmBdyy7j3wgyuWpY8TgUqpbyVBvp4O/gGdNSNOLJlc34VP9t8gCsWJ/HNtXPGqTillDfTQB9vu56BiESYfdGQTfaWNfON53exNDWan1+3RJfAVUo5xalAF5H1IlIoIkUicv8w7a4VESMiY7s7g69rrYZDb8KSG8F/8MsXlc2dfPHJHGLCg3nstmxCAv3HuUillLcaMdBFxB94FLgUyAJuEpGsQdpFAl8Htrm6SJ+x5zkwfbDs1iGb/ODlfNq7bfzpzpXERQaPY3FKKW/nzBn6KqDIGFNsjOkBngOuHKTdj4CfAV0urM93GGN1t6SugdjMQZtsK65ny4EavnphBnMTdXiiUmp0nAn0ZKC03+syx7bPiMhyINUY88/h3khE7haRXBHJra2tHXWxXq0sB+oODnl2bozhp5sPkDglhM+fOXOci1NK+YLTvigqIn7AI8C3R2prjHnMGJNtjMmOi4s73Y/2LruehsBwWHDVoLvfKKhi17Emvrk2k9Ag7TdXSo2eM4FeDqT2e53i2HZcJLAQeE9ESoA1wEa9MNpPTzvkvwQLrobgU7tSbH12Ht5cSEZ8BNcuT/FAgUopX+BMoOcAmSIyU0SCgBuBjcd3GmOajTGxxph0Y0w6sBXYYIzJdUvF3mjfRuhpG7K75YXcMorr2vmPS+YS4K8jSZVSYzNiehhjbMA9wBvAfuAFY0yBiDwoIhvcXaBP2PUMTJsNM9acsqujx8av3j5IdtpU1mYleKA4pZSvcGotF2PMJmDTSdseGKLt+adflg+pPwxHP4KLHoBBJgg98XEJNa3d/O6W5TqBSCl1WvT3e3fb/SyI36DL5Da09/D79w6zNitBF95SSp02DXR36u2C3X+BjIthyvRTdv/2nSLae2x8Z/1cDxSnlPI1Guju9MlvoLUSzvzaKbtKGzp4emsJ12en6hrnSimX0EB3l8aj1l2JFlwNM889ZffDbxTi7yd842JdSVEp5Roa6O7y5vetvvN1D526q6CKV/Mq+PJ5s0mMCvFAcUopX6SB7g5FW2D/q3Duv0PUwIlCje09fO/lfLKSpvDVCzI8VKBSyhfpLehczdYDr38Hps2CM+45ZfcDGwto7uzh6S+uIlAnESmlXEgD3dW2/g7qD8EtL0LAwOVvN+2t5NW8Cv593RzmJ03xUIFKKV+lp4iu1FIB7z8Mcy+DzLUDdtW1dfODV/JZnBLFl8+b7aEClVK+TAPdld78T7Db4JKfDNhsjOEHL+fT1mXjF9ct0fValFJuocniKiUfQf6LcPY3YNrA9cw35lWwuaCKb62bQ2aCjjlXSrmHBror9LTDpvsgagac9Y0Bu2paunjgHwUsmxHNXefM8lCBSqnJQC+Knq7i9+HVe6GxBG78KwSFDdj9vZfz6ert4+fXLcHfTxffUkq5j56hj1VXM2y8F57aAOIPd26CeZcNaJJf3szb+6u596JMZsdFeKhQpdRkoWfoY1G4GV77JrRVwZn3wgXfg8DQU5o9/elRQgP9uXV1mgeKVEpNNhroo9HdBq99A/b+DeKz4MZnIHnFoE2bO3r5R145Vy9LJioscJwLVUpNRhroo7Hjz1aYn3c/nPNtCAgasunfdpTS1WvntjXp41aeUmpy00AfjbLtEJ0GF3x32GZ2u+HprUfJTptK1nSdEaqUGh96UXQ0ynIhJXvEZu8fquVofQe3naF950qp8aOB7qyWCmgph5SVIzZ9+tOjxEYEc+nCpHEoTCmlLBrozirLtZ5HCPTShg7eLazh5lWpBAXoH69Savxo4jirLAf8gyBx0bDNntl6FD8RbtahikqpcaaB7qyyXEhcfMqSuP119fbxfG4p67IS9E5ESqlxp4HujD4bVOwasbtlY14FTR293H5G+vjUpZRS/WigO6OmAGydw45wMcbw9KdHmZMQwZpZ08axOKWUsmigO6Msx3oeJtB3lzaxt7yZ29akIaKLcCmlxp8GujPKdkB4nDWpaAhPf3qUiOAArl6eMmQbpZRyJw10Z5TlQHI2DHHm3djew2t7KrlmeTIRwTr5VinlGRroI+lstG76PEx3y8eH6+jps3P1suRxLEwppQbSQB9J+Q7reZgRLtuPNBAW5M+i5KhxKkoppU6lgT6SslxAYPqyIZtsP9LAirSpevNnpZRHaQKNpCwH4udDyOCrJjZ19FBY3cqqdB2qqJTyLKcCXUTWi0ihiBSJyP2D7P+yiOwVkd0i8pGIZLm+VA8wZsQVFnNLGjEGVs7UQFdKedaIgS4i/sCjwKVAFnDTIIH9rDFmkTFmKfAw8IjLK/WE+sPQ1WSNcBlCTkkDQf5+LE2NHsfClFLqVM6coa8CiowxxcaYHuA54Mr+DYwxLf1ehgPGdSV6UPnIKyxuO9LAktQoQgL9x6kopZQanDOBngyU9ntd5tg2gIh8VUQOY52h3zvYG4nI3SKSKyK5tbW1Y6l3fJXlQFAkxM0ddHdHj4388mZWav+5UmoCcNlFUWPMo8aY2cB3gB8M0eYxY0y2MSY7Li7OVR/tPmU5kLwM/AY/+951rAmb3bBK+8+VUhOAM4FeDqT2e53i2DaU54CrTqeoCaGnA6oLRuxu8RNYkTZ1HAtTSqnBORPoOUCmiMwUkSDgRmBj/wYiktnv5eXAIdeV6CGVeWC3jTChqJ6s6VOIDAkcx8KUUmpwIy48Yoyxicg9wBuAP/AnY0yBiDwI5BpjNgL3iMjFQC/QCNzhzqLHxfEVFocY4dJjs7PrWBO36J2JlFIThFMrSRljNgGbTtr2QL+vv+7iujyvPNdaXTFi8L7+veVNdNvsrJqp3S1KqYlBZ4oOpSx3xP5zQEe4KKUmDA30wbRUQEv5sDNEc440MDsunJiIoe8xqpRS40kDfTBlw08o6rMbcksaWTUzZhyLUkqp4eM/ZDsAAAyrSURBVGmgD6Z0G/gHQeKiQXcfqGqhtdvGah1/rpSaQDTQT2bvg4JXIP1sCBi8O2X78f5zDXSl1ASigX6yw+9CSxksv33IJtuPNJAcHUpydOg4FqaUUsPTQD/ZzichLAbmXjbobmMMOSUN2t2ilJpwNND7a6uFwtdh8Y1DdrcU17VT19aj3S1KqQlHA72/Pc+BvReW3zZkk+P957ogl1JqotFAP84Y2Pk0pKyybjk3hJwjDcRGBDErNnwci1NKqZFpoB9Xuh3qCoc9OwdrhujK9GmIyDgVppRSztFAP27nUxAUAQuuGbJJeVMn5U2d2t2ilJqQNNABulqg4CVYeA0ERwzZbFtxPaDrtyilJiYNdLDCvLcDlg099hxgc34V8ZHBZCVNGafClFLKeRroYHW3xM0fdjGutm4b7x2s5bJFSfj5af+5Umri0UCvLoDyHdbM0GEudG7ZX02Pzc5li5LGsTillHKeBvrOp8EvEBbfMGyzTXsriY8MJlvvH6qUmqAmd6Dbuq3JRPOvgPChl8Jt77bxXmEtly5M1O4WpdSENbkD/cBr0NkIy4Yfe77lQA3d2t2ilJrgnLqnqM+w90FtIVTshPKdcHAzRM2AWRcM+22b9ji6W3S4olJqAvP9QDcGPvw5FG2ByjxreCJAUCRMXwpnfxP8hv5Fpb3bxruFNdy4MhV/7W5RSk1gvh/oTcfgnYcgPssayTJ9OUxfBjEZwwb5cdrdopTyFr4f6GU51vPVv4ekJaP+9k17KonT7hallBfw/YuiZbkQEArxC0b9rce7Wy5dmKjdLUqpCW8SBHoOJC8H/9H/MvKOdrcopbyIbwe6rRuq9gw7pX84m/Za3S26GJdSyhv4dqBX7YW+HkgefaB39Gh3i1LKu/h2oB+/IJqyctTf+s6BGrp6tbtFKeU9fD/Qp6TAlNGH8qa9lcRGaHeLUsp7+H6gj6H/vKPHxjsHtLtFKeVdfDfQ22qsSUVj6G5590CtdrcopbyO7wZ6Wa71PMozdGMMT35aQlxksN47VCnlVZwKdBFZLyKFIlIkIvcPsv9bIrJPRPaIyBYRSXN9qaNUlgN+AaOeHfp6fhXbjzTw9YsytbtFKeVVRgx0EfEHHgUuBbKAm0Qk66Rmu4BsY8xi4EXgYVcXOmplOZC4CAJDnf6Wrt4+frJpP/MSI7lp1Qw3FqeUUq7nzBn6KqDIGFNsjOkBngOu7N/AGPOuMcaxjCFbgRTXljlK9j5redxR9p8//mExZY2dPPAvWXp2rpTyOs4EejJQ2u91mWPbUL4IvD7YDhG5W0RyRSS3trbW+SpHq2Y/9LaPakJRVXMXj757mPULEjlzdqz7alNKKTdx6UVREbkVyAb+e7D9xpjHjDHZxpjsuLg4V370QJ9NKHI+0B/efIA+Y/jeZfPdVJRSSrmXMytWlQOp/V6nOLYNICIXA98HzjPGdLumvDEqz4XQaTBtllPNdx5r5KVd5fzb+bOZERPm5uKUUso9nDlDzwEyRWSmiAQBNwIb+zcQkWXAH4ANxpga15c5SmW5Vv+5jNwPbrcb/uvVfcRHBvNvF2SMQ3FKKeUeIwa6McYG3AO8AewHXjDGFIjIgyKywdHsv4EI4G8isltENg7xdu7X2QS1B5y+IPrK7nLySpv4zvp5RAT7/v0+lFK+y6kEM8ZsAjadtO2Bfl9f7OK6xq5ip/WcsmLEpu3dNn76+gGWpEZz9bLhrvMqpdTE53szRctyAYHkkQP9d+8VUdPazQNXZOGnwxSVUl7ONwM9bi6ERA3brLqli8c/PMKVS6ezIm3qOBWnlFLu41uBbozTKyz+9p0i+uyGb6+dOw6FKaWU+/lWoDcUQ2fDiBdESxs6eC7nGNevTNVhikopn+FbgX58hcURZoj+esshRISvXajDFJVSvsPHAj0HAsMhfujZnkU1bby0s4zb1qSRFOX8wl1KKTXR+V6gJy8HP/8hm/zy7YOEBPrzlfNnj2NhSinlft43k8bWA33dYOyOh7Eeti6ozocz7x3yWwsqmvnnnkruuSCD2IjgcSxaKaXcz/sCfevv4O0fDr0/ddWQux558yBTQgK461zn1nhRSilv4n2Bnn4OrHsIxA8Q61n8rHVbgiIgY+2g37bzWCNbDtRw3yVziQoNHN+alVJqHHhfoKesGHRaf0ePjd+/X8yiwnrOyYwlJHBgP/rP3ygkNiKIO89MH6dClVJqfHlfoA/h7zvL+c2WQwCEB/lzwbx4Ll2YxPlz49hd2sQnh+t54IoswnUBLqWUj/KZdNu4u5zM+Aj+84osXs+v4s2CKl7bU0lwgB+RIQEkRYVw82q9T6hSynf5RKCXN3WSU9LIv6+bw7lz4jh3ThwPXbWQnJIGNudX8cHBWr65ds4p3TBKKeVLfCLQX82rAGDDkhNL4Pr7CWtmxbBmVoynylJKqXHlExOL/rG7gmUzonVdFqXUpOb1gX6oupX9lS1sWDLd06UopZRHeX2gb8yrwE/g8sVJni5FKaU8yqsD3RjDxrwKzpwdS3xkiKfLUUopj/LqQM8ra+ZofQcblmp3i1JKeXWg/2N3OUH+flyyINHTpSillMd5baD32Q2v7ankgnlxujaLUkrhxYG+tbie2tbuAWPPlVJqMvPaQN+4u4LwIH8umh/v6VKUUmpC8MpA77b1sSm/kksWJOp0fqWUcvDKQH+vsJbWLpuOblFKqX68MtA35lUwLTyIszJiPV2KUkpNGF4X6G3dNt7eV83li5II9Pe68pVSym28LhHf2ldFt83OldrdopRSA3hdoEcEB7IuK4HlM6Z6uhSllJpQvG499LVZCazNSvB0GUopNeF43Rm6UkqpwTkV6CKyXkQKRaRIRO4fZP+5IrJTRGwi8jnXl6mUUmokIwa6iPgDjwKXAlnATSKSdVKzY8CdwLOuLlAppZRznOlDXwUUGWOKAUTkOeBKYN/xBsaYEsc+uxtqVEop5QRnulySgdJ+r8sc20ZNRO4WkVwRya2trR3LWyillBrCuF4UNcY8ZozJNsZkx8XFjedHK6WUz3Mm0MuB1H6vUxzblFJKTSDOBHoOkCkiM0UkCLgR2OjespRSSo2WGGNGbiRyGfArwB/4kzHmxyLyIJBrjNkoIiuBl4GpQBdQZYxZMMJ71gJHx1h3LFA3xu/1ZpP1uGHyHrse9+TizHGnGWMG7bN2KtAnGhHJNcZke7qO8TZZjxsm77HrcU8up3vcOlNUKaV8hAa6Ukr5CG8N9Mc8XYCHTNbjhsl77Hrck8tpHbdX9qErpZQ6lbeeoSullDqJBrpSSvkIrwv0kZby9RUi8icRqRGR/H7bponIWyJyyPHsc7dtEpFUEXlXRPaJSIGIfN2x3aePXURCRGS7iOQ5jvu/HNtnisg2x8/7847JfT5HRPxFZJeIvOZ47fPHLSIlIrJXRHaLSK5j22n9nHtVoDu5lK+v+DOw/qRt9wNbjDGZwBbHa19jA75tjMkC1gBfdfwd+/qxdwMXGmOWAEuB9SKyBvgZ8EtjTAbQCHzRgzW609eB/f1eT5bjvsAYs7Tf2PPT+jn3qkCn31K+xpge4PhSvj7HGPMB0HDS5iuBJx1fPwlcNa5FjQNjTKUxZqfj61asf+TJ+PixG0ub42Wg42GAC4EXHdt97rgBRCQFuBx43PFamATHPYTT+jn3tkB32VK+XirBGFPp+LoK8Ombq4pIOrAM2MYkOHZHt8NuoAZ4CzgMNBljbI4mvvrz/ivgP4Dj91OIYXIctwHeFJEdInK3Y9tp/Zx73U2ilcUYY0TEZ8ecikgE8HfgG8aYFuukzeKrx26M6QOWikg01tpI8zxcktuJyBVAjTFmh4ic7+l6xtnZxphyEYkH3hKRA/13juXn3NvO0Cf7Ur7VIpIE4Hiu8XA9biEigVhh/hdjzEuOzZPi2AGMMU3Au8AZQLSIHD/x8sWf97OADSJSgtWFeiHwa3z/uDHGlDuea7D+A1/Faf6ce1ugT/alfDcCdzi+vgP4hwdrcQtH/+n/AvuNMY/02+XTxy4icY4zc0QkFFiLdf3gXeD4jdd97riNMd81xqQYY9Kx/j2/Y4y5BR8/bhEJF5HI418D64B8TvPn3Otmig62lK+HS3ILEfkrcD7WcprVwA+BV4AXgBlYSw9fb4w5+cKpVxORs4EPgb2c6FP9HlY/us8eu4gsxroI5o91ovWCMeZBEZmFdeY6DdgF3GqM6fZcpe7j6HL5d2PMFb5+3I7je9nxMgB41rEseQyn8XPudYGulFJqcN7W5aKUUmoIGuhKKeUjNNCVUspHaKArpZSP0EBXSikfoYGulFI+QgNdKaV8xP8H50lBGOF+r0cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIQqCPmXTekI",
        "outputId": "da1193c8-c06e-48dd-d4a1-85824f6bd595"
      },
      "source": [
        "MAX_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 250 \n",
        "EMBEDDING_DIM = fasttext_embed.shape[1]\n",
        "FILTERS = 128 \n",
        "KERNEL = 3 \n",
        "DENSE = 64\n",
        "\n",
        "with tf.device('/device:GPU:0'):  \n",
        "    \n",
        "    input = Input(shape=(None,))\n",
        "    emb = (Embedding(input_dim=MAX_WORDS+2, output_dim=EMBEDDING_DIM, weights=[embedding_matrix]\n",
        "            ,input_length=MAX_SEQUENCE_LENGTH, mask_zero=True, trainable=False))(input)\n",
        "    x = (Dropout(0.5))(emb)\n",
        "\n",
        "    res = x\n",
        "    for i in range(4):\n",
        "        x = Conv1D(FILTERS, KERNEL, activation='relu', padding='same')(x)\n",
        "    res = Conv1D(FILTERS, 1)(res)\n",
        "    x = add([x, res])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    res = x\n",
        "    for i in range(4):\n",
        "        x = Conv1D(FILTERS, KERNEL, activation='relu', padding='same')(x)\n",
        "    res = Conv1D(FILTERS, 1)(res)\n",
        "    x = add([x, res])\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "\n",
        "    x = (GlobalMaxPooling1D())(x)\n",
        "    x = (Dropout(0.5))(x)\n",
        "\n",
        "    x = (Dense(DENSE,activation='relu'))(x)\n",
        "    x = (Dropout(0.5))(x)\n",
        "\n",
        "\n",
        "    output = (Dense(y_train.shape[1],activation='softmax'))(x)\n",
        "\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "\n",
        "    # Load weights from the pre-trained model\n",
        "    model.load_weights(\"checkpoints/weights.hdf5\")\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        metrics=[\"categorical_crossentropy\"]\n",
        "    )\n",
        "\n",
        "    print(\"Classification report on the training data:\")\n",
        "    predictions_train = model.predict(X_train)\n",
        "    predictions_train = (predictions_train > 0.5).astype(int)   \n",
        "    print(classification_report(y_train, predictions_train, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the development data:\")\n",
        "    predictions_dev = model.predict(X_dev)\n",
        "    predictions_dev = (predictions_dev > 0.5).astype(int) \n",
        "    print(classification_report(y_dev, predictions_dev, target_names=lb.classes_))\n",
        "\n",
        "    print(\"Classification report on the test data:\")\n",
        "    predictions_test = model.predict(X_test)\n",
        "    predictions_test = (predictions_test > 0.5).astype(int)    \n",
        "    print(classification_report(y_test, predictions_test, target_names=lb.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification report on the training data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.76      0.05      0.09       463\n",
            "ARTS & CULTURE       0.00      0.00      0.00       408\n",
            "  BLACK VOICES       0.70      0.23      0.35      1358\n",
            "      BUSINESS       0.74      0.31      0.44      1832\n",
            "       COLLEGE       0.00      0.00      0.00       346\n",
            "        COMEDY       0.77      0.32      0.45      1549\n",
            "         CRIME       0.73      0.54      0.62       996\n",
            "CULTURE & ARTS       0.92      0.08      0.14       289\n",
            "       DIVORCE       0.89      0.75      0.81      1022\n",
            "     EDUCATION       0.60      0.01      0.02       282\n",
            " ENTERTAINMENT       0.75      0.76      0.75      4759\n",
            "   ENVIRONMENT       0.98      0.13      0.23       420\n",
            "         FIFTY       0.00      0.00      0.00       383\n",
            "  FOOD & DRINK       0.71      0.86      0.78      1860\n",
            "     GOOD NEWS       0.00      0.00      0.00       393\n",
            "         GREEN       0.66      0.15      0.25       776\n",
            "HEALTHY LIVING       0.82      0.21      0.33      1946\n",
            " HOME & LIVING       0.91      0.81      0.86      1319\n",
            "        IMPACT       0.00      0.00      0.00      1039\n",
            " LATINO VOICES       0.00      0.00      0.00       310\n",
            "         MEDIA       0.88      0.19      0.32       869\n",
            "         MONEY       0.75      0.16      0.26       506\n",
            "     PARENTING       0.68      0.61      0.64      2567\n",
            "       PARENTS       0.68      0.20      0.30      1181\n",
            "      POLITICS       0.82      0.87      0.85      9816\n",
            "  QUEER VOICES       0.90      0.68      0.78      1891\n",
            "      RELIGION       0.83      0.57      0.68       749\n",
            "       SCIENCE       0.93      0.41      0.57       715\n",
            "        SPORTS       0.87      0.76      0.81      1450\n",
            "         STYLE       0.79      0.44      0.56       686\n",
            "STYLE & BEAUTY       0.90      0.87      0.88      2860\n",
            "         TASTE       0.00      0.00      0.00       630\n",
            "          TECH       0.91      0.15      0.26       599\n",
            " THE WORLDPOST       0.70      0.15      0.25      1117\n",
            "        TRAVEL       0.89      0.84      0.86      2988\n",
            "      WEDDINGS       0.91      0.82      0.86      1120\n",
            "    WEIRD NEWS       0.72      0.06      0.11       828\n",
            "      WELLNESS       0.74      0.82      0.77      5283\n",
            "         WOMEN       0.82      0.10      0.18      1008\n",
            "    WORLD NEWS       0.00      0.00      0.00       635\n",
            "     WORLDPOST       0.00      0.00      0.00       752\n",
            "\n",
            "     micro avg       0.80      0.57      0.67     60000\n",
            "     macro avg       0.63      0.34      0.39     60000\n",
            "  weighted avg       0.73      0.57      0.60     60000\n",
            "   samples avg       0.57      0.57      0.57     60000\n",
            "\n",
            "Classification report on the development data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.71      0.03      0.06       155\n",
            "ARTS & CULTURE       0.00      0.00      0.00       135\n",
            "  BLACK VOICES       0.54      0.14      0.23       486\n",
            "      BUSINESS       0.66      0.27      0.38       564\n",
            "       COLLEGE       0.00      0.00      0.00       121\n",
            "        COMEDY       0.67      0.29      0.41       500\n",
            "         CRIME       0.63      0.43      0.51       348\n",
            "CULTURE & ARTS       0.88      0.08      0.15        86\n",
            "       DIVORCE       0.82      0.67      0.74       325\n",
            "     EDUCATION       0.00      0.00      0.00        91\n",
            " ENTERTAINMENT       0.67      0.66      0.67      1594\n",
            "   ENVIRONMENT       0.91      0.16      0.27       124\n",
            "         FIFTY       0.00      0.00      0.00       144\n",
            "  FOOD & DRINK       0.65      0.75      0.70       600\n",
            "     GOOD NEWS       0.00      0.00      0.00       134\n",
            "         GREEN       0.44      0.10      0.16       244\n",
            "HEALTHY LIVING       0.66      0.14      0.22       716\n",
            " HOME & LIVING       0.79      0.70      0.74       409\n",
            "        IMPACT       0.00      0.00      0.00       354\n",
            " LATINO VOICES       0.00      0.00      0.00       127\n",
            "         MEDIA       0.81      0.14      0.24       283\n",
            "         MONEY       0.60      0.14      0.23       151\n",
            "     PARENTING       0.61      0.49      0.54       833\n",
            "       PARENTS       0.59      0.19      0.28       409\n",
            "      POLITICS       0.76      0.80      0.78      3288\n",
            "  QUEER VOICES       0.84      0.65      0.73       628\n",
            "      RELIGION       0.67      0.46      0.54       257\n",
            "       SCIENCE       0.77      0.32      0.45       206\n",
            "        SPORTS       0.76      0.62      0.68       522\n",
            "         STYLE       0.67      0.33      0.44       226\n",
            "STYLE & BEAUTY       0.82      0.79      0.80       946\n",
            "         TASTE       0.00      0.00      0.00       213\n",
            "          TECH       0.86      0.12      0.21       197\n",
            " THE WORLDPOST       0.59      0.11      0.19       370\n",
            "        TRAVEL       0.78      0.71      0.74       952\n",
            "      WEDDINGS       0.85      0.71      0.77       380\n",
            "    WEIRD NEWS       0.57      0.06      0.12       264\n",
            "      WELLNESS       0.66      0.76      0.70      1776\n",
            "         WOMEN       0.74      0.09      0.17       342\n",
            "    WORLD NEWS       0.00      0.00      0.00       247\n",
            "     WORLDPOST       0.00      0.00      0.00       253\n",
            "\n",
            "     micro avg       0.72      0.50      0.59     20000\n",
            "     macro avg       0.54      0.29      0.34     20000\n",
            "  weighted avg       0.64      0.50      0.53     20000\n",
            "   samples avg       0.50      0.50      0.50     20000\n",
            "\n",
            "Classification report on the test data:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "          ARTS       0.67      0.01      0.03       138\n",
            "ARTS & CULTURE       0.00      0.00      0.00       152\n",
            "  BLACK VOICES       0.59      0.18      0.27       443\n",
            "      BUSINESS       0.64      0.23      0.34       591\n",
            "       COLLEGE       0.00      0.00      0.00       115\n",
            "        COMEDY       0.72      0.32      0.45       530\n",
            "         CRIME       0.66      0.44      0.53       353\n",
            "CULTURE & ARTS       1.00      0.07      0.13       104\n",
            "       DIVORCE       0.84      0.65      0.73       374\n",
            "     EDUCATION       1.00      0.01      0.02       108\n",
            " ENTERTAINMENT       0.67      0.65      0.66      1564\n",
            "   ENVIRONMENT       1.00      0.05      0.10       129\n",
            "         FIFTY       0.00      0.00      0.00       131\n",
            "  FOOD & DRINK       0.65      0.74      0.69       626\n",
            "     GOOD NEWS       0.00      0.00      0.00       149\n",
            "         GREEN       0.49      0.10      0.17       260\n",
            "HEALTHY LIVING       0.62      0.13      0.22       640\n",
            " HOME & LIVING       0.77      0.70      0.73       401\n",
            "        IMPACT       0.00      0.00      0.00       349\n",
            " LATINO VOICES       0.00      0.00      0.00       125\n",
            "         MEDIA       0.67      0.11      0.19       265\n",
            "         MONEY       0.62      0.12      0.20       180\n",
            "     PARENTING       0.61      0.52      0.56       870\n",
            "       PARENTS       0.48      0.13      0.21       385\n",
            "      POLITICS       0.76      0.81      0.78      3208\n",
            "  QUEER VOICES       0.84      0.62      0.71       659\n",
            "      RELIGION       0.63      0.43      0.51       244\n",
            "       SCIENCE       0.68      0.25      0.36       208\n",
            "        SPORTS       0.77      0.60      0.67       462\n",
            "         STYLE       0.59      0.30      0.40       220\n",
            "STYLE & BEAUTY       0.82      0.78      0.80       952\n",
            "         TASTE       0.00      0.00      0.00       213\n",
            "          TECH       0.79      0.12      0.21       214\n",
            " THE WORLDPOST       0.73      0.14      0.24       353\n",
            "        TRAVEL       0.81      0.71      0.76      1048\n",
            "      WEDDINGS       0.80      0.70      0.75       362\n",
            "    WEIRD NEWS       0.56      0.04      0.08       229\n",
            "      WELLNESS       0.68      0.76      0.72      1782\n",
            "         WOMEN       0.72      0.08      0.15       348\n",
            "    WORLD NEWS       0.00      0.00      0.00       236\n",
            "     WORLDPOST       0.00      0.00      0.00       280\n",
            "\n",
            "     micro avg       0.72      0.50      0.59     20000\n",
            "     macro avg       0.56      0.28      0.33     20000\n",
            "  weighted avg       0.65      0.50      0.53     20000\n",
            "   samples avg       0.50      0.50      0.50     20000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}